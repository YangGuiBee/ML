
#  11 : ì§€ë„ í•™ìŠµ(Supervised Learning, SL) : íšŒê·€(regression) + ë¶„ë¥˜(classification)

---

	[1] íŒë³„ ë¶„ì„ (Discriminant Analysis)
		[1-1] LDA (Linear Discriminant Analysis)
		[1-2] QDA (Quadratic Discriminant Analysis)
		
	[2] íŠ¸ë¦¬ ê¸°ë°˜ (Tree-based)
		[2-1] ê²°ì • íŠ¸ë¦¬ (Decision Tree)
		[2-2] ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)
		
	[3] ê±°ë¦¬ ê¸°ë°˜ (Distance-based)
		[3-1] k-ìµœê·¼ì ‘ ì´ì›ƒ (k-Nearest Neighbors, K-NN)
		[3-2] ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (Support Vector Machine, SVM)
		
	ì‹ ê²½ë§ ê¸°ë°˜
		MLP, CNN, RNN, LSTM, GRU, Attention Mechanism, Transformer, Autoencoder, VAE, GAN, GCN ...
		
	[4] ì°¨ì› ì¶•ì†Œ (Dimensionality Reduction)
		[4-1] PCR (Principal Component Regression) : PCA(ë¹„ì§€ë„í•™ìŠµì˜ ì°¨ì›ì¶•ì†Œ) + íšŒê·€
		[4-2] PLS (Partial Least Squares)
		[4-3] PLS-DA (Partial Least Squares Discriminant Analysis)
		[4-4] Supervised PCA

---  

# [1-1] LDA (Linear Discriminant Analysis)
â–£ ì •ì˜: ë°ì´í„°ë¥¼ ì§ì„ (ë˜ëŠ” í‰ë©´) í•˜ë‚˜ë¡œ ê¹”ë”í•˜ê²Œ ë‚˜ëˆ„ëŠ” ë°©ë²•ìœ¼ë¡œ<br>
ë°ì´í„°ê°€ ì—¬ëŸ¬ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆì„ ë•Œ, ê·¸ë£¹ ì‚¬ì´ì˜ ì°¨ì´ëŠ” ìµœëŒ€í™”í•˜ë©´ì„œ ê°™ì€ ê·¸ë£¹ ì•ˆì˜ ì°¨ì´ëŠ” ìµœì†Œí™”í•˜ë„ë¡ ë°ì´í„°ë¥¼ ì˜ êµ¬ë¶„í•´ì£¼ëŠ” ì„ (í˜¹ì€ ì´ˆí‰ë©´)ì„ íƒìƒ‰(ê°€ì • : ëª¨ë“  í´ë˜ìŠ¤ì˜ ê³µë¶„ì‚°ì´ ê°™ë‹¤. ëª¨ì–‘ì´ ê°™ì€ 1ì°¨ì‹ ê³¡ì„ )<br>
â–£ í•„ìš”ì„±: í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë¶„ë¥˜ ë¬¸ì œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì : í´ë˜ìŠ¤ ë¶„ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ì—¬ ë¶„ë¥˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìœ¼ë©°, ì„ í˜• ë³€í™˜ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œ<br>
â–£ ë‹¨ì : ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ê²½ìš° ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë©°, í´ë˜ìŠ¤ ê°„ ë¶„í¬ê°€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ë” íš¨ê³¼ì <br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : sklearn.discriminant_analysis.LinearDiscriminantAnalysis<br> 
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/lda_qda.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html<br>

![](./images/LDA_1.png)
<br>

**([1-1] LDA ì˜ˆì œ ì†ŒìŠ¤)**

	# ============================================
	# Iris ë°ì´í„°ì…‹ì— LDA ì ìš©
	#  - ì›ë˜ 4ì°¨ì› íŠ¹ì§•ì„ LDAë¡œ 2ì°¨ì›ìœ¼ë¡œ ì°¨ì›ì¶•ì†Œ
	#  - LDA ê³µê°„(2ì°¨ì›)ì—ì„œ ë‹¤ì‹œ LDA ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµ
	#  - ê·¸ ê²°ê³¼ë¡œ ì–»ì€ ê²°ì •ê²½ê³„(ì„ í˜• íŒë³„ì„ )ë¥¼ í•¨ê»˜ ì‹œê°í™”
	#  - ê° LDA ì„±ë¶„ì˜ ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ê³¼ ì›ë³€ìˆ˜ ê°€ì¤‘ì¹˜ ì¶œë ¥
	# ============================================
	
	from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
	from sklearn.datasets import load_iris
	import matplotlib.pyplot as plt
	import numpy as np
	
	# --------------------------------------------------
	# 1. ë°ì´í„° ë¡œë“œ
	# --------------------------------------------------
	# load_iris() í•¨ìˆ˜ëŠ” ë¶“ê½ƒ(Iris) ë°ì´í„°ì…‹ì„ ë°˜í™˜í•œë‹¤.
	# ë°ì´í„° êµ¬ì¡°:
	#   data.data   : (150, 4) ë°°ì—´, 4ê°œì˜ ì…ë ¥ íŠ¹ì„±
	#   data.target : (150,) ë°°ì—´, 3ê°œ í´ë˜ìŠ¤(0, 1, 2)
	data = load_iris()
	
	# X : ì…ë ¥ íŠ¹ì„± í–‰ë ¬ (150í–‰, 4ì—´)
	#     [sepal length, sepal width, petal length, petal width]
	X = data.data
	
	# y : ì •ë‹µ ë ˆì´ë¸” ë²¡í„° (150ê°œ ìƒ˜í”Œì— ëŒ€í•œ í´ë˜ìŠ¤ ë²ˆí˜¸)
	#     0 = setosa, 1 = versicolor, 2 = virginica
	y = data.target
	
	# --------------------------------------------------
	# 2. LDA ëª¨ë¸ ìƒì„± ë° í•™ìŠµ (4ì°¨ì› -> 2ì°¨ì› íˆ¬ì˜)
	# --------------------------------------------------
	# n_components=2
	#   - LDAê°€ ì°¾ì„ ì¶•(ì„ í˜• íŒë³„ì¶•)ì˜ ê°œìˆ˜
	#   - í´ë˜ìŠ¤ê°€ Cê°œë¼ë©´ ìµœëŒ€ C-1 ì°¨ì›ê¹Œì§€ ê°€ëŠ¥
	#   - ì—¬ê¸°ì„œëŠ” í´ë˜ìŠ¤ê°€ 3ê°œì´ë¯€ë¡œ ìµœëŒ€ 2ì°¨ì›ê¹Œì§€ ì¶•ì†Œ ê°€ëŠ¥
	lda = LinearDiscriminantAnalysis(n_components=2)
	
	# fit_transform(X, y)
	#   1) ì£¼ì–´ì§„ X, yë¡œ LDA ëª¨ë¸ì„ í•™ìŠµ(fit)
	#   2) í•™ìŠµëœ ì„ í˜• íŒë³„ì¶•ìœ¼ë¡œ Xë¥¼ íˆ¬ì˜í•˜ì—¬ ìƒˆë¡œìš´ ì¢Œí‘œë¡œ ë³€í™˜(transform)
	# X_ldaì˜ í¬ê¸° : (150, 2)  -> 2ì°¨ì› LDA ê³µê°„ì˜ ì¢Œí‘œ
	X_lda = lda.fit_transform(X, y)
	
	# --------------------------------------------------
	# 2-1. LDA ì„±ë¶„ ì •ë³´ ì¶œë ¥
	# --------------------------------------------------
	# ê° LDA ì„±ë¶„ì´ í´ë˜ìŠ¤ ë¶„ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë¹„ìœ¨
	print("Explained variance ratio:", lda.explained_variance_ratio_)
	
	# ê° ì›ë³€ìˆ˜(4ê°œ)ê°€ LDA ì¶•ì— ê¸°ì—¬í•˜ëŠ” ê°€ì¤‘ì¹˜(ì„ í˜•ê²°í•© ê³„ìˆ˜)
	# í–‰: ì›ë³€ìˆ˜(sepal length, sepal width, petal length, petal width)
	# ì—´: LDA Component 1, LDA Component 2
	print("Scalings (coefficients for original features):")
	print(lda.scalings_)
	
	# --------------------------------------------------
	# 3. LDA 2ì°¨ì› ê³µê°„ì—ì„œ ë‹¤ì‹œ ë¶„ë¥˜ê¸° í•™ìŠµ
	#    (ì´ ëª¨ë¸ì˜ ê²°ì •ê²½ê³„ë¥¼ 2ì°¨ì› í‰ë©´ì— ê·¸ë¦¼)
	# --------------------------------------------------
	# ì—¬ê¸°ì„œëŠ” X_lda(2ì°¨ì› ì¢Œí‘œ)ì™€ y(í´ë˜ìŠ¤)ë¥¼ ì‚¬ìš©í•˜ì—¬
	# ë‹¤ì‹œ í•œ ë²ˆ LDA ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•œë‹¤.
	# ì´ë ‡ê²Œ í•™ìŠµëœ lda_2dì˜ ê²°ì •ê²½ê³„ë¥¼ 2ì°¨ì› í‰ë©´ì— ê·¸ë¦´ ìˆ˜ ìˆë‹¤.
	lda_2d = LinearDiscriminantAnalysis()
	lda_2d.fit(X_lda, y)
	
	# --------------------------------------------------
	# 4. ê²°ì •ê²½ê³„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ê·¸ë¦¬ë“œ ìƒì„±
	# --------------------------------------------------
	# ì‚°ì ë„ ë²”ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ x, y ë²”ìœ„ë¥¼ ì•½ê°„ í™•ì¥í•˜ì—¬
	# ê·¸ë¦¬ë“œ ìƒì˜ ì ë“¤ì„ ì´˜ì´˜í•˜ê²Œ ìƒì„±í•œë‹¤.
	x_min, x_max = X_lda[:, 0].min() - 1.0, X_lda[:, 0].max() + 1.0
	y_min, y_max = X_lda[:, 1].min() - 1.0, X_lda[:, 1].max() + 1.0
	
	# np.meshgrid:
	#   - xì¶• ë°©í–¥ìœ¼ë¡œ 300ê°œ, yì¶• ë°©í–¥ìœ¼ë¡œ 300ê°œì˜ ì ì„ ë§Œë“¤ê³ 
	#   - ì´ë¥¼ í†µí•´ ì „ì²´ í‰ë©´ì„ ë®ëŠ” ê²©ì ì¢Œí‘œ(xx, yy)ë¥¼ ìƒì„±
	xx, yy = np.meshgrid(
	    np.linspace(x_min, x_max, 300),
	    np.linspace(y_min, y_max, 300)
	)
	
	# ê·¸ë¦¬ë“œ ìœ„ì˜ ëª¨ë“  ì ì„ í•˜ë‚˜ì˜ (N, 2) ë°°ì—´ë¡œ í•©ì¹œë‹¤.
	# ê° í–‰ì€ [xì¢Œí‘œ, yì¢Œí‘œ] í•œ ì ì„ ì˜ë¯¸í•œë‹¤.
	grid_points = np.c_[xx.ravel(), yy.ravel()]
	
	# lda_2d.predict(grid_points):
	#   - ê·¸ë¦¬ë“œ ìƒì˜ ê° ì ì´ ì–´ë–¤ í´ë˜ìŠ¤(0, 1, 2)ì— ì†í•˜ëŠ”ì§€ë¥¼ ì˜ˆì¸¡
	# Zì˜ í¬ê¸° : (300*300,) ì˜ 1ì°¨ì› ë°°ì—´
	Z = lda_2d.predict(grid_points)
	
	# contour, contourfì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´
	# Zë¥¼ xx, yyì™€ ê°™ì€ 2ì°¨ì› í˜•íƒœë¡œ ë‹¤ì‹œ ë³€í˜•í•œë‹¤.
	Z = Z.reshape(xx.shape)
	
	# --------------------------------------------------
	# 5. ê²°ê³¼ ì‹œê°í™” (ì‚°ì ë„ + ê²°ì •ê²½ê³„)
	# --------------------------------------------------
	plt.figure(figsize=(6, 5))
	
	# (1) ë°°ê²½ ì˜ì—­ì„ ì±„ìš°ëŠ” ë¶€ë¶„
	# contourf:
	#   - Z ê°’(í´ë˜ìŠ¤ ë²ˆí˜¸)ì„ ë°”íƒ•ìœ¼ë¡œ í‰ë©´ì„ 3ê°œì˜ ì˜ì—­ìœ¼ë¡œ ìƒ‰ì¹ 
	#   - alpha=0.15 ë¡œ íˆ¬ëª…ë„ë¥¼ ì¤˜ì„œ ë°°ê²½ë§Œ ì˜…ê²Œ í‘œì‹œ
	# levels:
	#   - í´ë˜ìŠ¤ 0,1,2 ì‚¬ì´ì˜ ê²½ê³„ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´
	#     [-0.5, 0.5, 1.5, 2.5] 4ê°œì˜ ê²½ê³„ê°’ ì‚¬ìš©
	plt.contourf(xx, yy, Z, alpha=0.15, levels=[-0.5, 0.5, 1.5, 2.5])
	
	# (2) ê²°ì • ê²½ê³„ì„  ê·¸ë¦¬ê¸°
	# contour:
	#   - levels=[0.5, 1.5] ëŠ”
	#       0ê³¼ 1 ì‚¬ì´ì˜ ê²½ê³„, 1ê³¼ 2 ì‚¬ì´ì˜ ê²½ê³„ë¥¼ ì˜ë¯¸
	#   - colors="k"  : ê²€ì€ìƒ‰ ì„ 
	#   - linestyles="--" : ì ì„  ìŠ¤íƒ€ì¼
	#   - linewidths=1.0  : ì„  ë‘ê»˜
	plt.contour(
	    xx, yy, Z,
	    levels=[0.5, 1.5],
	    colors="k",
	    linestyles="--",
	    linewidths=1.0
	)
	
	# (3) ì‹¤ì œ LDA ë³€í™˜ ë°ì´í„° ì‚°ì ë„
	# c=y ë¡œ í´ë˜ìŠ¤ì— ë”°ë¼ ìƒ‰ì„ ë‹¤ë¥´ê²Œ í‘œì‹œ
	scatter = plt.scatter(
	    X_lda[:, 0],    # xì¶•: LDA Component 1
	    X_lda[:, 1],    # yì¶•: LDA Component 2
	    c=y,            # ìƒ‰ìƒ: í´ë˜ìŠ¤ ë ˆì´ë¸”
	    edgecolor="k"   # ì  í…Œë‘ë¦¬ë¥¼ ê²€ì€ìƒ‰ìœ¼ë¡œ ì„¤ì •
	)
	
	# ì¶• ì´ë¦„ê³¼ ì œëª© ì„¤ì •
	plt.xlabel("LDA Component 1")
	plt.ylabel("LDA Component 2")
	plt.title("LDA on Iris Dataset with Decision Boundaries")
	
	# (4) ë²”ë¡€ ìƒì„±
	# legend_elements() ëŠ” ì‚°ì ë„ì—ì„œ ìë™ìœ¼ë¡œ ë²”ë¡€ìš© í•¸ë“¤/ë¼ë²¨ì„ ì¶”ì¶œ
	handles, _ = scatter.legend_elements()
	
	# ë²”ë¡€ì— ê° í´ë˜ìŠ¤ ì´ë¦„ì„ ëª…ì‹œì ìœ¼ë¡œ ë‹¬ì•„ì¤€ë‹¤.
	plt.legend(
	    handles,
	    ["setosa (0)", "versicolor (1)", "virginica (2)"],
	    loc="best"
	)
	
	# ë ˆì´ì•„ì›ƒì„ ì•½ê°„ ì •ë¦¬í•˜ì—¬ ì—¬ë°± ì¡°ì •
	plt.tight_layout()
	
	# ìµœì¢… ê·¸ë˜í”„ ì¶œë ¥
	plt.show()


**([1-1] LDA ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼)**

	Explained variance ratio: [0.9912126 0.0087874]
	Scalings (coefficients for original features):
	[[ 0.82937764  0.02410215]
 	[ 1.53447307  2.16452123]
 	[-2.20121166 -0.93192121]
 	[-2.81046031  2.83918785]]
 
![](./images/LDA2.png)
<br>


**([1-1] LDA ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼ ë¶„ì„)**

	# --------------------------------------------------
	# [ê·¸ë˜í”„ í•´ì„]
	# --------------------------------------------------
	# ì„¸ ê°€ì§€ í’ˆì¢…(ìƒ‰ìƒë³„ë¡œ êµ¬ë¶„)ì´ LDA ê³µê°„ì—ì„œ ì„œë¡œ ì˜ ë¶„ë¦¬ë˜ì–´ ë‚˜íƒ€ë‚¨
	# LDA Component 1: í´ë˜ìŠ¤ ê°„ ì°¨ì´ë¥¼ ê°€ì¥ ì˜ êµ¬ë¶„í•˜ëŠ” ì£¼ì¶• (ì£¼ë¡œ setosa vs ë‚˜ë¨¸ì§€)
	# LDA Component 2: ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ êµ¬ë¶„ ì¶• (versicolor vs virginica êµ¬ë¶„ ë³´ì¡°)
	# ì›ë˜ 4ì°¨ì› ë°ì´í„°(ê½ƒë°›ì¹¨Â·ê½ƒì ê¸¸ì´/ë„ˆë¹„)ê°€ 2ì°¨ì› ì„ í˜•ê²°í•©(ì¶•ì†Œê³µê°„)ìœ¼ë¡œ íˆ¬ì˜ë˜ì—ˆìŒì—ë„ ì„¸ í’ˆì¢…ì˜ ê²½ê³„ê°€ ëª…í™•íˆ ë“œëŸ¬ë‚¨
	# --------------------------------------------------

	Explained variance ratio: [0.9912126 0.0087874]
		â†’ LDA1: 99.12% ì‚¬ì‹¤ìƒ ì´ ì¶• í•˜ë‚˜ë¡œ 3ê°œ í’ˆì¢… êµ¬ë¶„ ê°€ëŠ¥
		â†’ LDA2: 0.88% â†’ ë¯¸ì„¸í•œ ì°¨ì´ë§Œ ì„¤ëª…
	Scalings (coefficients for original features): íŠ¹ì„±ë³„ ê¸°ì—¬ë„(ë¶€í˜¸ë‘ ë¬´ê´€í•˜ê²Œ ì ˆëŒ€ê°’ìœ¼ë¡œ íŒë‹¨)
					LDA1			LDA2
	sepal length	[[ 0.82937764  0.02410215]
 	sepal width		 [ 1.53447307  2.16452123]
 	petal length	 [-2.20121166 -0.93192121]
 	petal width		 [-2.81046031  2.83918785]]
<br>


![](./images/PCA_LDA.png)
<br>
https://nirpyresearch.com/classification-nir-spectra-linear-discriminant-analysis-python/


**(PCA vs LDA ì˜ˆì œ ì†ŒìŠ¤)**

	"""
	Bream(ë„ë¯¸) vs Smelt(ë¹™ì–´) ì‹¤ë°ì´í„°(ì›ê²© ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë‚´ì¥ í‘œë³¸)ë¡œ PCA vs LDA ë¹„êµ
	- ì‚¬ìš© íŠ¹ì„±: Length2(ì¤‘ê°„ê¸¸ì´), Height
	"""
	
	import io
	import textwrap
	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.preprocessing import StandardScaler
	from sklearn.decomposition import PCA
	from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
	
	# -------------------- 1) ì›ê²© ë¡œë“œ + ì˜¤í”„ë¼ì¸ í´ë°± --------------------
	URLS = [
	    "https://raw.githubusercontent.com/selva86/datasets/master/Fish.csv",
	    "https://raw.githubusercontent.com/holtzy/D3-graph-gallery/master/DATA/Fish.csv",  # ì˜ˆë¹„
	]
	
	# ê³µê°œ Fish Marketì—ì„œ Bream/Smelt í–‰ ì¼ë¶€ë§Œ ë°œì·Œ(Length2, Height ì¤‘ì‹¬)
	FALLBACK_CSV = textwrap.dedent("""\
	Species,Length2,Height
	Bream,23.2,11.52
	Bream,24.0,12.48
	Bream,23.9,12.37
	Bream,26.3,12.73
	Bream,26.5,14.18
	Bream,29.0,14.73
	Bream,29.7,14.88
	Bream,29.9,17.78
	Bream,31.0,16.24
	Bream,31.5,16.64
	Bream,32.0,15.05
	Bream,33.0,15.58
	Bream,33.5,18.26
	Bream,35.0,18.49
	Bream,36.5,18.18
	Bream,36.0,18.67
	Bream,39.0,19.99
	Bream,41.0,21.06
	Smelt,12.9,3.52
	Smelt,14.5,3.52
	Smelt,13.2,4.30
	Smelt,14.3,4.23
	Smelt,15.0,5.14
	Smelt,16.2,5.58
	Smelt,17.4,5.52
	Smelt,17.4,5.22
	Smelt,19.0,5.20
	Smelt,19.0,5.58
	Smelt,20.0,5.69
	Smelt,20.5,5.92
	Smelt,21.0,6.11
	Smelt,22.0,6.63
	""")
	
	def load_fish_df():
	    last_err = None
	    for url in URLS:
	        try:
	            df = pd.read_csv(url)
	            # ì…€ë°”86 ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆ í™•ì¸
	            if {"Species","Length2","Height"}.issubset(df.columns):
	                return df
	        except Exception as e:
	            last_err = e
	            continue
	    # í´ë°±: ë‚´ì¥ í‘œë³¸ ì‚¬ìš©
	    df = pd.read_csv(io.StringIO(FALLBACK_CSV))
	    return df
	
	df = load_fish_df()
	df = df[df["Species"].isin(["Bream","Smelt"])].copy()
	
	# -------------------- 2) íŠ¹ì§• ì„ íƒ/í‘œì¤€í™” --------------------
	features = ["Length2","Height"]
	X = df[features].to_numpy().astype(float)
	y = (df["Species"]=="Bream").astype(int).to_numpy()  # Bream=1, Smelt=0
	
	scaler = StandardScaler()
	Xz = scaler.fit_transform(X)
	
	# -------------------- 3) PCA/LDA ì¶• ê³„ì‚° --------------------
	pca = PCA(n_components=1).fit(Xz)
	w_pca = pca.components_[0]  # (2,)
	
	lda = LDA(n_components=1).fit(Xz, y)
	w_lda = lda.scalings_.ravel()
	w_lda = w_lda / np.linalg.norm(w_lda)
	
	def project_perp(P, w):
	    w = w / np.linalg.norm(w)
	    t = P @ w
	    return np.outer(t, w)
	
	def endpoints(w, span=5.5):
	    w = w/np.linalg.norm(w)
	    return np.vstack([-span*w, span*w])
	
	def plot_panel(ax, X, y, w, title, subtitle):
	    P = X
	    Pr = project_perp(P, w)
	
	    ax.scatter(P[y==1,0], P[y==1,1], c="crimson", s=36, label="Bream")
	    ax.scatter(P[y==0,0], P[y==0,1], c="royalblue", s=36, label="Smelt")
	
	    ab = endpoints(w, 5.5)
	    ax.plot(ab[:,0], ab[:,1], "k-", lw=2)
	    ax.arrow(0,0, w[0]*2.2, w[1]*2.2, head_width=0.15, head_length=0.22, fc="k", ec="k")
	
	    # ìˆ˜ì§íˆ¬ì˜(íšŒìƒ‰ ì ì„ )
	    for p, q in zip(P, Pr):
	        ax.plot([p[0], q[0]], [p[1], q[1]], ls="--", c="gray", lw=1, alpha=0.9)
	
	    ax.scatter(Pr[y==1,0], Pr[y==1,1], c="crimson", s=16)
	    ax.scatter(Pr[y==0,0], Pr[y==0,1], c="royalblue", s=16)
	
	    ax.set_aspect("equal","box")
	    ax.set_xlabel(f"{features[0]} (z-score)")
	    ax.set_ylabel(f"{features[1]} (z-score)")
	    ax.set_title(f"{title}\n{subtitle}", fontsize=11)
	    ax.grid(False)
	    ax.legend(fontsize=9, loc="upper left")
	    ax.set_xlim(-3.5, 3.5); ax.set_ylim(-3.5, 3.5)
	
	# -------------------- 4) í”Œë¡¯ --------------------
	fig, axes = plt.subplots(1,2, figsize=(10,6))
	plot_panel(axes[0], Xz, y, w_pca,
	           "PCA projection:", "Maximising the variance of the whole set")
	plot_panel(axes[1], Xz, y, w_lda,
	           "LDA projection:", "Maximising the distance between groups")
	plt.tight_layout(); plt.show()
	
	# -------------------- 5) ë¶„ë¦¬ë„ ìˆ˜ì¹˜ ë¹„êµ --------------------
	def fisher_score_1d(z, y):
	    m1, m0 = z[y==1].mean(), z[y==0].mean()
	    s1, s0 = z[y==1].var(ddof=1), z[y==0].var(ddof=1)
	    return (m1 - m0)**2 / (s1 + s0)
	
	z_pca = Xz @ (w_pca/np.linalg.norm(w_pca))
	z_lda = Xz @ (w_lda/np.linalg.norm(w_lda))
	print("[Fisher ë¶„ë¦¬ ì ìˆ˜] (ê°’ â†‘ = ë¶„ë¦¬ â†‘)")
	print(f"PCA ì¶• : {fisher_score_1d(z_pca, y):.3f}")
	print(f"LDA ì¶• : {fisher_score_1d(z_lda, y):.3f}")


**(PCA vs LDA ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼)**

	[Fisher ë¶„ë¦¬ ì ìˆ˜] (ê°’ â†‘ = ë¶„ë¦¬ â†‘)
	PCA ì¶• : 8.604
	LDA ì¶• : 21.683

![](./images/PCA_vs_LDA.png)


**(PCA vs LDA ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼ ë¶„ì„)**

	[Fisher ë¶„ë¦¬ì ìˆ˜] (ê°’ â†‘ = ë¶„ë¦¬ â†‘) : ë‘ í´ë˜ìŠ¤ì˜ ì¤‘ì‹¬ì´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆê³ , ê° í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ë¶„ì‚°ì´ ì‘ì„ìˆ˜ë¡ ê°’ì´ ì»¤ì§„ë‹¤.(ì „ì²˜ë¦¬ì— ì‚¬ìš©)
	PCA ì¶• : 8.604    â†’ (ë¹„ì§€ë„) PCAëŠ” ë¶„ì‚° ìµœëŒ€ê°€ ë˜ëŠ” ì¶• : ì „ì²´ë¶„ì‚° ìµœëŒ€í™”
	                     ë¹¨ê°•/íŒŒë‘ì„ êµ¬ë¶„í•˜ì§€ ì•Šê³ , ê°€ì¥ í¼ì ¸ ë³´ì´ëŠ” ë°©í–¥(í™”ì‚´í‘œ)ì„ ì°¾ì€ ë’¤ ê·¸ ì¶• ìœ„ë¡œ ì§ì„  íˆ¬ì˜
	LDA ì¶• : 21.683   â†’ (ì§€ë„) LDAëŠ” ì§‘ë‹¨ ë¶„ë¦¬ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì¶• : ì§‘ë‹¨ê°„ë¶„ì‚°/ì§‘ë‹¨ë‚´ë¶„ì‚° ìµœëŒ€í™”
	                     ê° ì§‘ë‹¨ì´ ì¶• ìœ„ì—ì„œ ë©€ì–´ì§€ë„ë¡ í•˜ë©´ì„œ, ê°™ì€ ì§‘ë‹¨ ë‚´ë¶€ëŠ” ëª¨ì´ë„ë¡(í¼ì§ ìµœì†Œ) í•˜ëŠ” ë°©í–¥ ì°¾ê¸°

<br>

# [1-2] QDA (Quadratic Discriminant Analysis)
â–£ ì •ì˜ : ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì–´ëŠ í´ë˜ìŠ¤(ì§‘ë‹¨)ì— ì†í• ì§€ ì˜ˆì¸¡í•˜ëŠ” ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ<br>
ë°ì´í„°ê°€ ì—¬ëŸ¬ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆì„ ë•Œ, ê° ê·¸ë£¹ì˜ í™•ë¥  ë¶„í¬(íŠ¹íˆ í‰ê· ê³¼ ê³µë¶„ì‚°)ë¥¼ ì´ìš©í•´ì„œ â€œì´ ì ì€ ì–´ë–¤ ê·¸ë£¹ì—ì„œ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì„ê¹Œ?â€ë¥¼ ê³„ì‚°<br>
(ê°€ì • : ê° í´ë˜ìŠ¤ì˜ ê³µë¶„ì‚°ì´ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. ëª¨ì–‘ì´ ë‹¤ë¥¸ 2ì°¨ì‹ ê³¡ì„ )<br>
â–£ ëª©ì  : í´ë˜ìŠ¤ ê°„ì˜ êµ¬ì¡°ê°€ ë” ë³µì¡í•˜ê³  ì„ í˜• ê²½ê³„ë¡œëŠ” ì¶©ë¶„íˆ ë¶„ë¦¬ë˜ì§€ ì•Šì„ ë•Œ, ì¢€ ë” ìœ ì—°í•œ ë¶„ë¦¬ ê²½ê³„ë¥¼ ì œê³µ<br>
â–£ ì¥ì  : ê³µë¶„ì‚°ì´ í´ë˜ìŠ¤ë§ˆë‹¤ ë‹¤ë¥¼ ê²½ìš° LDAë³´ë‹¤ ìœ ì—°í•˜ê²Œ ë¶„ë¥˜ ì„±ëŠ¥ì´ í–¥ìƒ, ë¹„ì„ í˜•(ê³¡ì„ ) ê²½ê³„ë„ í—ˆìš©í•˜ë¯€ë¡œ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ì— ëŒ€ì‘ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : í´ë˜ìŠ¤ë³„ ê³µë¶„ì‚°ì„ ì¶”ì •í•´ì•¼ í•˜ë¯€ë¡œ ìƒ˜í”Œ ìˆ˜ê°€ ì¶©ë¶„ì¹˜ ì•Šê±°ë‚˜ ê³ ì°¨ì› íŠ¹ì„±ì¼ ê²½ìš° ê³¼ì í•© ë° ìˆ˜ì¹˜ë¶ˆì•ˆì • ë¬¸ì œê°€ ë°œìƒ<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis<br> 
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/lda_qda.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html<br>

**(LDAì™€ QDA ì‘ìš©ë¶„ì•¼ ë¹„êµ)**
| êµ¬ë¶„            | **LDA (ì„ í˜•íŒë³„ë¶„ì„)**                                                                                                                 | **QDA (ì´ì°¨íŒë³„ë¶„ì„)**                                                                                                                      |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| **ì ìš© ë°ì´í„° í˜•íƒœ** | ê° í´ë˜ìŠ¤ê°€ ë¹„ìŠ·í•œ ë¶„í¬(íƒ€ì› ëª¨ì–‘), ê²½ê³„ê°€ ì§ì„ ìœ¼ë¡œ ë‚˜ë‰¨                                                                                                | í´ë˜ìŠ¤ë§ˆë‹¤ ë¶„í¬ ëª¨ì–‘ì´ ë‹¤ë¥´ê³ , ê²½ê³„ê°€ ê³¡ì„ í˜•                                                                                                             |
| **ë°ì´í„° íŠ¹ì„±**    | ë‹¨ìˆœí•˜ê³ , í´ë˜ìŠ¤ ê°„ ê²½ê³„ê°€ â€˜ì§ì„ ì â€™                                                                                                            | ë³µì¡í•˜ê³ , ê²½ê³„ê°€ â€˜ë¹„ì„ í˜•ì â€™                                                                                                                      |
| **í•„ìš”í•œ ë°ì´í„°ëŸ‰**  | ì ì€ ë°ì´í„°ì—ë„ ì•ˆì •ì  (ê³µë¶„ì‚°ì„ í•˜ë‚˜ë§Œ ì¶”ì •)                                                                                                       | ë°ì´í„°ê°€ ë§ì•„ì•¼ í•¨ (í´ë˜ìŠ¤ë³„ ê³µë¶„ì‚°ì„ ë”°ë¡œ ì¶”ì •)                                                                                                          |
| **ê³„ì‚° ë³µì¡ë„**    | ë‚®ìŒ (ëª¨ìˆ˜ ì ìŒ)                                                                                                                       | ë†’ìŒ (ëª¨ìˆ˜ ë§ìŒ)                                                                                                                            |
| **ëŒ€í‘œ ì‘ìš© ë¶„ì•¼**  | â€¢ ì–¼êµ´ì¸ì‹ (ì´ˆê¸°í˜• Face Recognition)  <br>â€¢ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Spam vs Ham) <br>â€¢ ì˜ë£Œ ë°ì´í„° ì§„ë‹¨ (ì •ìƒ/ë¹„ì •ìƒ êµ¬ë¶„) <br>â€¢ í’ˆì§ˆê´€ë¦¬, ê²°í•¨íƒì§€ <br>â€¢ ë§ˆì¼€íŒ… ê³ ê° ì„¸ë¶„í™”(ë‹¨ìˆœêµ°ì§‘ ê¸°ë°˜) | â€¢ ìŒì„± ì¸ì‹ (ì„±ë³„, ê°ì • ë¶„ë¥˜ ë“±) <br>â€¢ ìƒë¬¼ì •ë³´í•™(ìœ ì „ì ë°œí˜„ ë°ì´í„°) <br>â€¢ ë³µì¡í•œ ì´ë¯¸ì§€ ë¶„ë¥˜(ë¹„ì„ í˜• ê²½ê³„) <br>â€¢ ë¹„ì •ìƒ íƒì§€(Anomaly Detection) <br>â€¢ ê¸ˆìœµ ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ (í´ë˜ìŠ¤ ë¶„ì‚°ì´ ë‹¤ë¥¼ ë•Œ) |
| **ëª¨ë¸ í˜•íƒœ**     | ì„ í˜• ê²½ê³„ (ì§ì„ Â·í‰ë©´)                                                                                                                    | ê³¡ì„ í˜• ê²½ê³„ (í¬ë¬¼ì„ Â·íƒ€ì›í˜•)                                                                                                                      |
| **ì í•©í•œ ìƒí™©**    | ë³€ìˆ˜ ê°„ ê´€ê³„ê°€ ì„ í˜•, ê³µë¶„ì‚° êµ¬ì¡°ê°€ ìœ ì‚¬                                                                                                          | í´ë˜ìŠ¤ ê°„ ê³µë¶„ì‚° êµ¬ì¡°ê°€ ë‹¤ë¦„, ë¹„ì„ í˜• êµ¬ì¡°                                                                                                              |
| **ì˜ˆì‹œ ë°ì´í„°**    | Iris ë°ì´í„°(ë‘ í´ë˜ìŠ¤ êµ¬ë¶„ ì„ í˜• ê°€ëŠ¥)                                                                                                         | ë³µì¡í•œ íŒ¨í„´ì˜ ìŒì„±Â·ì˜ìƒ ë°ì´í„°                                                                                                                     |
| **ì¥ì **        | ë¹ ë¥´ê³  ë‹¨ìˆœ, í•´ì„ ìš©ì´                                                                                                                    | ìœ ì—°í•˜ê³  ë³µì¡í•œ ê²½ê³„ í‘œí˜„ ê°€ëŠ¥                                                                                                                     |
| **ë‹¨ì **        | ë¹„ì„ í˜• ë°ì´í„°ì— ë¶€ì í•©                                                                                                                     | ê³¼ì í•© ìœ„í—˜, ê³„ì‚°ëŸ‰ í¼                                                                                                                         |

<br>

 
<br>

# [2-1] ê²°ì • íŠ¸ë¦¬ (Decision Tree)
â–£ ì •ì˜ : ë…ë¦½ë³€ìˆ˜ ê³µê°„ì„ ë°˜ë³µì ìœ¼ë¡œ ë¶„í• (split)í•˜ì—¬ ë¦¬í”„ ë…¸ë“œ(leaf)ì—ì„œ ì˜ˆì¸¡ê°’ì„ ì¶œë ¥í•˜ëŠ” íŠ¸ë¦¬êµ¬ì¡°ì˜ ì§€ë„í•™ìŠµ ëª¨ë¸<br> 
â–£ ëª©ì  : ì…ë ¥ ë³€ìˆ˜ì˜ ë¶„í•  ê¸°ì¤€ì„ ì°¾ì•„ ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ê³ , ì§ê´€ì ì¸ ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ëª¨ë¸ì„ ì œê³µ<br>
â–£ ì¥ì  : í•´ì„ì´ ì‰½ê³  íŠ¸ë¦¬ ì‹œê°í™” ë“±ì„ í†µí•´ ì„¤ëª… ê°€ëŠ¥, ë³€ìˆ˜ ë³€í™˜ì´ë‚˜ ìŠ¤ì¼€ì¼ë§ì´ í¬ê²Œ í•„ìš” ì—†ìœ¼ë©°, ë¹„ì„ í˜• ê´€ê³„ë‚˜ ë³€ìˆ˜ ìƒí˜¸ì‘ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜<br>
â–£ ë‹¨ì  : ê³¼ì í•© ìœ„í—˜ì´ í¬ê³ , ì„¸ì„¸í•˜ê²Œ íŠœë‹í•˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±, íŠ¸ë¦¬ê°€ ë„ˆë¬´ ê¹Šê±°ë‚˜ ë¶„í•  ê¸°ì¤€ì´ ë³µì¡í•´ì§€ë©´ í•´ì„ì´ ì–´ë ¤ì›Œì§ˆ ê°€ëŠ¥ì„±<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : ë¶„ë¥˜ìš© sklearn.tree.DecisionTreeClassifier, íšŒê·€ìš© sklearn.tree.DecisionTreeRegressor<br> 
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/tree.html<br>
â–£ API : https://scikit-learn.org/stable/auto_examples/tree/index.html<br>

![](./images/tree.png)

| ì¥ì                              | ë‹¨ì                                               |
|----------------------------------|---------------------------------------------------|
| ì‹œê°í™”ë¥¼ í†µí•œ í•´ì„ì˜ ìš©ì´ì„±(ë‚˜ë¬´ êµ¬ì¡°ë¡œ í‘œí˜„ë˜ì–´ ì´í•´ê°€ ì‰¬ì›€, ìƒˆë¡œìš´ ê°œì²´ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë£¨íŠ¸ ë…¸ë“œë¶€í„° ë ë…¸íŠ¸ê¹Œì§€ ë”°ë¼ê°€ë©´ ë˜ë¯€ë¡œ ë¶„ì„ ìš©ì´) | íœ´ë¦¬ìŠ¤í‹±ì— ê·¼ê±°í•œ ì‹¤ìš©ì  ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµìš© ìë£Œì— ì˜ì¡´í•˜ê¸°ì— ì „ì—­ ìµœì í™”ë¥¼ ì–»ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ(ê²€ì¦ìš© ë°ì´í„°ë¥¼ í™œìš©í•œ êµì°¨ íƒ€ë‹¹ì„± í‰ê°€ë¥¼ ì§„í–‰í•˜ëŠ” ê³¼ì •ì´ í•„ìš”) |
| ë°ì´í„° ì „ì²˜ë¦¬, ê°€ê³µì‘ì—…ì´ ë¶ˆí•„ìš” | ìë£Œì— ë”°ë¼ ë¶ˆì•ˆì •í•¨(ì ì€ ìˆ˜ì˜ ìë£Œë‚˜ í´ë˜ìŠ¤ ìˆ˜ì— ë¹„êµí•˜ì—¬ í•™ìŠµ ë°ì´í„°ê°€ ì ìœ¼ë©´ ë†’ì€ ë¶„ë¥˜ì—ëŸ¬ ë°œìƒ) | 
| ìˆ˜ì¹˜í˜•, ë²”ì£¼í˜• ë°ì´í„° ëª¨ë‘ ì ìš© ê°€ëŠ¥ | ê° ë³€ìˆ˜ì˜ ê³ ìœ í•œ ì˜í–¥ë ¥ì„ í•´ì„í•˜ê¸° ì–´ë ¤ì›€ | 
| ë¹„ëª¨ìˆ˜ì ì¸ ë°©ë²•ìœ¼ë¡œ ì„ í˜•ì„±, ì •ê·œì„± ë“±ì˜ ê°€ì •ì´ í•„ìš”ì—†ê³  ì´ìƒê°’ì— ë¯¼ê°í•˜ì§€ ì•ŠìŒ | ìë£Œê°€ ë³µì¡í•˜ë©´ ì‹¤í–‰ì‹œê°„ì´ ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•¨ | 
| ëŒ€ëŸ‰ì˜ ë°ì´í„° ì²˜ë¦¬ì—ë„ ì í•©í•˜ê³  ëª¨í˜• ë¶„ë¥˜ ì •í™•ë„ê°€ ë†’ìŒ | ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ë¹„ì—°ì†ì  ê°’ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ë¶„ë¦¬ ê²½ê³„ì ì—ì„œëŠ” ì˜ˆì¸¡ì˜¤ë¥˜ê°€ ë§¤ìš° ì»¤ì§€ëŠ” í˜„ìƒ ë°œìƒ | 

![](./images/trees.png)

<br>

## ê²°ì • íŠ¸ë¦¬ íšŒê·€(Decision Tree Regression)
â–£ ì •ì˜ : ë°ì´í„°ì— ë‚´ì¬ë˜ì–´ ìˆëŠ” íŒ¨í„´ì„ ë¹„ìŠ·í•œ ìˆ˜ì¹˜ì˜ ê´€ì¸¡ì¹˜ ë³€ìˆ˜ì˜ ì¡°í•©ìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ ë‚˜ë¬´ í˜•íƒœë¡œ ë§Œë“ ë‹¤.<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/tree.html#regression<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation<br>


**(Decision Tree Regression ì˜ˆì œ ì†ŒìŠ¤)**

	# ============================================
	# ê²°ì •íŠ¸ë¦¬ íšŒê·€(DecisionTreeRegressor) ì˜ˆì œ (ì™„ì „ ì‹¤í–‰í˜•)
	# ë°ì´í„°: sklearn ë‚´ì¥ ë‹¹ë‡¨(íšŒê·€ìš©) ë°ì´í„°ì…‹
	# ì ˆì°¨: ë°ì´í„° ë¡œë“œ â†’ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  â†’ ëª¨ë¸ í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ í‰ê°€
	# í•µì‹¬ í¬ì¸íŠ¸: íŠ¸ë¦¬ê³„ì—´ì€ ìŠ¤ì¼€ì¼ë§ì´ í•„ìˆ˜ëŠ” ì•„ë‹˜(ë¶„í•  ê¸°ì¤€ì´ ìˆœìœ„/ì„ê³„ê°’ ê¸°ë°˜) max_depth ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ê³¼ì í•© ì œì–´
	# ============================================
	from sklearn.datasets import load_diabetes
	from sklearn.model_selection import train_test_split
	from sklearn.tree import DecisionTreeRegressor
	from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
	import numpy as np

	# --------------------------------------------------
	# 1) ë°ì´í„° ë¡œë“œ (íšŒê·€ìš©: ì—°ì†í˜• íƒ€ê¹ƒ y)
	# --------------------------------------------------
	diabetes = load_diabetes()
	X = diabetes.data        # shape (442, 10) â€” 10ê°œì˜ ìˆ˜ì¹˜í˜• íŠ¹ì§•
	y = diabetes.target      # shape (442,)     â€” ì§ˆë³‘ ì§„í–‰ ì •ë„(ì—°ì†ê°’)

	# --------------------------------------------------
	# 2) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
	#    - random_state ê³ ì •: ì¬í˜„ì„± ë³´ì¥
	#    - test_size=0.2: 20%ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ì‚¬ìš©
	# --------------------------------------------------
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

	# --------------------------------------------------
	# 3) ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
	#    - max_depth=5: íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ ì œí•œ(ê³¼ì í•© ë°©ì§€ìš©)
	#    - random_state=42: ë¶„í• /ë™ì‘ì˜ ì¬í˜„ì„±
	# --------------------------------------------------
	tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
	tree_reg.fit(X_train, y_train)

	# --------------------------------------------------
	# 4) ì˜ˆì¸¡
	# --------------------------------------------------
	y_pred = tree_reg.predict(X_test)

	# --------------------------------------------------
	# 5) ì„±ëŠ¥ í‰ê°€
	#    - MSE: í‰ê·  ì œê³± ì˜¤ì°¨ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
	#    - RMSE: ì œê³±ê·¼(í•´ì„ í¸ì˜, y ë‹¨ìœ„ì™€ ë™ì¼)
	#    - MAE: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (ì´ìƒì¹˜ì— ëœ ë¯¼ê°)
	#    - R2 : ê²°ì •ê³„ìˆ˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„¤ëª…ë ¥ ë†’ìŒ)
	# --------------------------------------------------
	mse = mean_squared_error(y_test, y_pred)
	rmse = np.sqrt(mse)
	mae = mean_absolute_error(y_test, y_pred)
	r2 = r2_score(y_test, y_pred)

	print(f"Mean Squared Error (MSE): {mse:.3f}")
	print(f"Root MSE (RMSE)        : {rmse:.3f}")
	print(f"Mean Absolute Error    : {mae:.3f}")
	print(f"R^2 Score              : {r2:.3f}")

	# --------------------------------------------------
	# (ì„ íƒ) íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸: ì–´ë–¤ ë³€ìˆ˜ë¡œ ë¶„í• ì„ ë§ì´ í–ˆëŠ”ì§€
	# --------------------------------------------------
	importances = tree_reg.feature_importances_
	# ì¤‘ìš”ë„ê°€ 0ì´ ì•„ë‹Œ ìƒìœ„ íŠ¹ì§•ë§Œ ë³´ê¸°
	top_idx = np.argsort(importances)[::-1]
	print("\n[Feature Importances]")
	for i in top_idx:
    	if importances[i] > 0:
	        print(f"- {diabetes.feature_names[i]:>6s}: {importances[i]:.3f}")


**(Decision Tree Regression ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼)**

	Mean Squared Error (MSE): 3526.016
	Root MSE (RMSE)        : 59.380
	Mean Absolute Error    : 45.937
	R^2 Score              : 0.334

	[Feature Importances]
	-    bmi: 0.555
	-     s5: 0.189
	-     s1: 0.062
	-     s6: 0.059
	-     s4: 0.040
	-    age: 0.032
	-     s3: 0.023
	-     bp: 0.022
	-     s2: 0.017
	-    sex: 0.002


**(Decision Tree Regression ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼ ë¶„ì„)**

	Mean Squared Error(MSE) : 3526.016 â†’ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ í‰ê·  ì œê³± ì˜¤ì°¨ë¡œ ëª¨ë¸ ì˜¤ì°¨ê°€ ë‹¤ì†Œ í° í¸
	Root MSE(RMSE)          : 59.380 â†’ ì˜ˆì¸¡ ì˜¤ì°¨ì˜ í‰ê·  í¬ê¸°ê°€ ì•½ Â±59 ë‹¨ìœ„ ì •ë„(ë‹¹ë‡¨ ì§„í–‰ ì§€ìˆ˜ëŠ” 0~300 ì •ë„ì´ë¯€ë¡œ ì˜¤ì°¨ê°€ ì¤‘ê°„ ìˆ˜ì¤€)
	Mean Absolute Error(MAE): 45.937 â†’ í‰ê· ì ìœ¼ë¡œ ì•½ 45.9 ì •ë„ ì°¨ì´(MAE < 30 ìš°ìˆ˜, 30 â‰¤ MAE â‰¤ 50 ì¤‘ê°„, MAE > 70 ë¶€ì •í™•)
	R^2 Score(ê²°ì •ê³„ìˆ˜)       : 0.334 â†’ ì „ì²´ ë¶„ì‚°ì˜ ì•½ 33.4%ë§Œ ì„¤ëª…(ì˜ˆì¸¡ë ¥ì´ ì œí•œì ) ê³¼ì í•©ì—†ì´ ê¸°ë³¸íŠ¸ë¦¬ ëª¨ë¸ë¡œëŠ” ì¤‘ê°„ìˆ˜ì¤€ì˜ ì„±ëŠ¥

	[Feature Importances]
	-    bmi: 0.555  â†’ ê°€ì¥ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ê°€ì§. ë¹„ë§Œë„ê°€ ë†’ì„ìˆ˜ë¡ ì¸ìŠë¦° ì €í•­ì„±ê³¼ í˜ˆë‹¹ ìˆ˜ì¹˜ê°€ ì¦ê°€í•˜ë¯€ë¡œ ë‹¹ë‡¨ ì§„í–‰ ì •ë„ ì˜ˆì¸¡ì— ì ˆëŒ€ì  ì˜í–¥ì„ ë¯¸ì¹¨. íŠ¸ë¦¬ì˜ ë£¨íŠ¸ ë¶„í• (ì²« ê¸°ì¤€)ë¡œ ì‚¬ìš©ë˜ì—ˆì„ ê°€ëŠ¥ì„±
	-     s5: 0.189  â†’ í˜ˆì¤‘ ì§€ì§ˆ(íŠ¹íˆ ì¤‘ì„±ì§€ë°©) ëŒ€ì‚¬ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì§€ì§ˆëŒ€ì‚¬ ì´ìƒê³¼ ì¸ìŠë¦° ì €í•­ì„± ê°„ì˜ ì—°ê´€ì„± ë°˜ì˜. í˜ˆì§€ì§ˆ ìˆ˜ì¹˜ê°€ ë†’ì„ìˆ˜ë¡ ë‹¹ë‡¨ ì•…í™” ìœ„í—˜ ì¦ê°€
	-     s1: 0.062  â†’ ì´ì½œë ˆìŠ¤í…Œë¡¤ ìˆ˜ì¹˜ë¡œ, ê³ ì§€í˜ˆì¦Â·í˜ˆê´€ê³„ ë¬¸ì œì™€ ê´€ë ¨. í˜ˆì¤‘ ì½œë ˆìŠ¤í…Œë¡¤ì´ ë†’ì„ìˆ˜ë¡ ë‹¹ë‡¨ í•©ë³‘ì¦ ìœ„í—˜ ìƒìŠ¹
	-     s6: 0.059  â†’ í˜ˆë‹¹ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë³€ìˆ˜. ì‹¤ì œ í˜ˆë‹¹ ë†ë„ ë³€í™”ê°€ ë‹¹ë‡¨ ì§„í–‰ì— ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜ë¨. s5ì™€ í•¨ê»˜ ëŒ€ì‚¬ì„± íŠ¹ì§•ì„ ì„¤ëª…
	-     s4: 0.040  â†’ í˜ˆì²­ ì¸ìŠë¦° ë°˜ì‘ì„±ì„ ë‚˜íƒ€ë‚´ë©°, ì¸ìŠë¦° ë¶„ë¹„ ê¸°ëŠ¥ ì €í•˜ ì—¬ë¶€ë¥¼ ë°˜ì˜. ë‹¹ëŒ€ì‚¬ ë¶ˆê· í˜•ì´ ì‹¬í•œ í™˜ìì—ì„œ ê°’ì´ í¬ê²Œ ì‘ìš©
	-    age: 0.032  â†’ ê³ ë ¹ì¼ìˆ˜ë¡ ë‹¹ë‡¨ ë°œìƒ ë° ì§„í–‰ ìœ„í—˜ì´ ì»¤ì§. ë‹¤ë§Œ ë‹¤ë¥¸ ìƒë¦¬ì  ìš”ì¸(BMI, ì§€ì§ˆ ìˆ˜ì¹˜ ë“±)ì— ë¹„í•´ ì§ì ‘ì ì¸ ì˜í–¥ì€ ìƒëŒ€ì ìœ¼ë¡œ ì‘ìŒ
	-     s3: 0.023  â†’ ì¢‹ì€ ì½œë ˆìŠ¤í…Œë¡¤ë¡œ ë‚®ì„ìˆ˜ë¡ ì‹¬í˜ˆê´€ ì§ˆí™˜ ë° ë‹¹ë‡¨ í•©ë³‘ì¦ ìœ„í—˜ì´ ì¦ê°€. ëª¨ë¸ì—ì„œëŠ” ë³´ì¡°ì  ì§€í‘œë¡œ ì‚¬ìš©
	-     bp: 0.022  â†’ í˜ˆì••ì€ ì¸ìŠë¦° ì €í•­ì„±ê³¼ ì—°ê´€. ê³ í˜ˆì••Â·ëŒ€ì‚¬ì¦í›„êµ° í™˜ìì—ì„œ ë‹¹ë‡¨ ì§„í–‰ ì†ë„ ê°€ì†í™”. ëª¨ë¸ì—ì„œ ë³´ì¡° ì„¤ëª…ë³€ìˆ˜ë¡œ ì‘ìš©
	-     s2: 0.017  â†’ ë‚˜ìœ ì½œë ˆìŠ¤í…Œë¡¤ë¡œ ë†’ì„ìˆ˜ë¡ í˜ˆê´€ì†ìƒ ë° ë‹¹ë‡¨í•©ë³‘ì¦ ìœ ë°œ ê°€ëŠ¥ì„± ë†’ìŒ. ì˜í–¥ë„ëŠ” í¬ì§€ ì•Šì§€ë§Œ ì§€ì§ˆëŒ€ì‚¬ ìš”ì¸ì˜ ì¼ë¶€ë¡œ ë°˜ì˜
	-    sex: 0.002  â†’ ë‚¨ë…€ ê°„ í‰ê· ì  ëŒ€ì‚¬ì°¨ëŠ” ìˆì§€ë§Œ, ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” í° ì˜í–¥ ì—†ìŒ. ëª¨ë¸ì—ì„œ ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
	
<br>

## ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜(Decision Tree Classification)
â–£ ì •ì˜ : ë°ì´í„°ì— ë‚´ì¬ë˜ì–´ ìˆëŠ” íŒ¨í„´ì„ ë¹„ìŠ·í•œ ë²”ì£¼ì˜ ê´€ì¸¡ì¹˜ ë³€ìˆ˜ì˜ ì¡°í•©ìœ¼ë¡œ ë¶„ë¥˜ ëª¨ë¸ì„ ë‚˜ë¬´ í˜•íƒœë¡œ ë§Œë“ ë‹¤.<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/tree.html#classification<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation<br>


**(Decision Tree Classification ì˜ˆì œ ì†ŒìŠ¤)**

	# ============================================
	# ê²°ì •íŠ¸ë¦¬ ë¶„ë¥˜(DecisionTreeClassifier) ì˜ˆì œ (ì™„ì „ ì‹¤í–‰í˜•)
	# ë°ì´í„°: sklearn ë‚´ì¥ ìœ ë°©ì•”(ë¶„ë¥˜ìš©, binary) ë°ì´í„°ì…‹
	# ì ˆì°¨: ë°ì´í„° ë¡œë“œ â†’ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  â†’ ëª¨ë¸ í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ í‰ê°€
	# í•µì‹¬ í¬ì¸íŠ¸: íŠ¸ë¦¬ ê³„ì—´ì€ ìŠ¤ì¼€ì¼ë§ì´ í•„ìˆ˜ ì•„ë‹˜(ì„ê³„ê°’ ê¸°ë°˜ ë¶„í• ). 
	#             max_depth, min_samples_leaf ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ê³¼ì í•© ì œì–´.
	# ============================================
	from sklearn.datasets import load_breast_cancer
	from sklearn.model_selection import train_test_split
	from sklearn.tree import DecisionTreeClassifier, plot_tree
	from sklearn.metrics import (
    	accuracy_score, precision_score, recall_score, f1_score,
    	roc_auc_score, confusion_matrix, classification_report)
	import numpy as np
	import matplotlib.pyplot as plt

	# --------------------------------------------------
	# 1) ë°ì´í„° ë¡œë“œ (ë¶„ë¥˜ìš©: ì´ì§„ íƒ€ê¹ƒ y âˆˆ {0,1})
	# --------------------------------------------------
	data = load_breast_cancer()
	X = data.data                # shape (569, 30) â€” 30ê°œì˜ ìˆ˜ì¹˜í˜• íŠ¹ì§•
	y = data.target              # shape (569,)     â€” 0(ì•…ì„±), 1(ì–‘ì„±)

	# --------------------------------------------------
	# 2) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
	#    - stratify=y: í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€(ë¶„ë¥˜ì—ì„œ ê¶Œì¥)
	#    - random_state ê³ ì •: ì¬í˜„ì„± ë³´ì¥
	# --------------------------------------------------
	X_train, X_test, y_train, y_test = train_test_split(
    	X, y, test_size=0.20, random_state=42, stratify=y)

	# --------------------------------------------------
	# 3) ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
	#    - max_depth=5: íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ ì œí•œ(ê³¼ì í•© ë°©ì§€ìš©)
	#    - random_state=42: ì¬í˜„ì„±
	#    - (ì„ íƒ) class_weight="balanced": í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œ ê°€ì¤‘ì¹˜ ìë™ ë³´ì •
	# --------------------------------------------------
	tree_clf = DecisionTreeClassifier(
    	max_depth=5,
    	random_state=42,
    	# class_weight="balanced")
	tree_clf.fit(X_train, y_train)

	# --------------------------------------------------
	# 4) ì˜ˆì¸¡
	# --------------------------------------------------
	y_pred = tree_clf.predict(X_test)
	y_proba = tree_clf.predict_proba(X_test)[:, 1]  # ì–‘ì„±(1) í™•ë¥ 

	# --------------------------------------------------
	# 5) ì„±ëŠ¥ í‰ê°€
	#    - Accuracy : ì „ì²´ ì •í™•ë„
	#    - Precision: ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨
	#    - Recall   : ì‹¤ì œ ì–‘ì„± ì¤‘ ëª¨ë¸ì´ ì–‘ì„±ìœ¼ë¡œ ì¡ì€ ë¹„ìœ¨(ë¯¼ê°ë„)
	#    - F1-score : Precision/Recall ì¡°í™”í‰ê· 
	#    - ROC-AUC  : ì„ê³„ê°’ ì „ ë²”ìœ„ì—ì„œì˜ ë¶„ë¥˜ ì„±ëŠ¥(1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
	# --------------------------------------------------
	acc  = accuracy_score(y_test, y_pred)
	prec = precision_score(y_test, y_pred, zero_division=0)
	rec  = recall_score(y_test, y_pred, zero_division=0)
	f1   = f1_score(y_test, y_pred, zero_division=0)
	auc  = roc_auc_score(y_test, y_proba)

	print(f"Accuracy     : {acc:.3f}")
	print(f"Precision    : {prec:.3f}")
	print(f"Recall       : {rec:.3f}")
	print(f"F1-score     : {f1:.3f}")
	print(f"ROC-AUC      : {auc:.3f}")

	# í˜¼ë™í–‰ë ¬ & ìƒì„¸ ë¦¬í¬íŠ¸
	cm = confusion_matrix(y_test, y_pred)
	print("\n[Confusion Matrix]")
	print(cm)
	print("\n[Classification Report]")
	print(classification_report(y_test, y_pred, target_names=data.target_names))

	# --------------------------------------------------
	# (ì„ íƒ) íŠ¹ì§• ì¤‘ìš”ë„: ì–´ë–¤ ë³€ìˆ˜ë¡œ ë¶„í• ì„ ë§ì´ í–ˆëŠ”ì§€
	# --------------------------------------------------
	importances = tree_clf.feature_importances_
	top_idx = np.argsort(importances)[::-1]
	print("\n[Feature Importances] (Top-10)")
	for i in top_idx[:10]:
    	print(f"- {data.feature_names[i]:<30s}: {importances[i]:.3f}")

	# --------------------------------------------------
	# (ì„ íƒ) íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™” (ì‘ì€ ê¹Šì´ì¼ ë•Œ ê°€ë…ì„± ì¢‹ìŒ)
	# --------------------------------------------------
	plt.figure(figsize=(14, 8))
	plot_tree(
    	tree_clf,
    	feature_names=data.feature_names,
    	class_names=data.target_names,
    	filled=True,
    	rounded=True,
    	fontsize=8)
	plt.title("Decision Tree (max_depth=5)")
	plt.tight_layout()
	plt.show()



**(Decision Tree Classification ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼)**

	Accuracy     : 0.921
	Precision    : 0.957
	Recall       : 0.917
	F1-score     : 0.936
	ROC-AUC      : 0.916

	[Confusion Matrix]
	[[39  3]
	 [ 6 66]]

	[Classification Report]
				precision    recall  f1-score   support
	malignant    0.87      0.93      0.90        42
	benign       0.96      0.92      0.94        72
	accuracy                            0.92       114
	macro avg       0.91      0.92      0.92       114
	weighted avg    0.92      0.92      0.92       114

	[Feature Importances] (Top-10)
	- worst radius                  : 0.714
	- worst concave points          : 0.119
	- texture error                 : 0.054
	- worst texture                 : 0.031
	- worst concavity               : 0.017
	- worst smoothness              : 0.013
	- area error                    : 0.012
	- mean texture                  : 0.012
	- worst symmetry                : 0.011
	- worst area                    : 0.009

![](./images/dt.png)


**(Decision Tree Classification ì˜ˆì œ ì†ŒìŠ¤ ì‹¤í–‰ ê²°ê³¼ ë¶„ì„)**

	Accuracy     : 0.921  â†’ ì „ì²´ 114ê±´ ì¤‘ 105ê±´(=39+66)ì„ ë§ì¶¤
	Precision    : 0.957  â†’ ëª¨ë¸ì´ benignì´ë¼ ì˜ˆì¸¡í•œ 69ê±´(=66+3) ì¤‘ 66ê±´ì´ ì‹¤ì œ benign(ì˜¤íƒ 3ê±´)
	Recall       : 0.917  â†’ ì‹¤ì œ benign 72ê±´ ì¤‘ 66ê±´ì„ ë§ì¶¤(benignì„ ë†“ì¹œ ê²½ìš°(FN)ëŠ” 6ê±´)
	F1-score     : 0.936  â†’ Precision(0.957)ê³¼ Recall(0.917)ì˜ ì¡°í™” í‰ê· 
	ROC-AUC      : 0.916  â†’ ì„ê³„ê°’ ì „ ë²”ìœ„ë¥¼ í†µí‹€ì–´ ì–‘ì„±ê³¼ ìŒì„±ì„ ì˜ êµ¬ë¶„í•˜ëŠ” ë¶„ë¦¬ë ¥. 0.5(ë¬´ì‘ìœ„) ëŒ€ë¹„ í™•ì‹¤íˆ ìš°ìˆ˜

	[Confusion Matrix] â†’ benign(ì–‘ì„±ì¢…ì–‘, 1), Malignant(ì•…ì„±ì¢…ì–‘, 0)
	                         Malignantì˜ˆì¸¡   Benignì˜ˆì¸¡           
	[[39  3]  â†’  Malignant   TN=39          FP=3
	 [ 6 66]] â†’  Benign      FN=6           TP=66

	[Classification Report]
				precision  recall  f1-score   support
	malignant    0.87      0.93      0.90        42
	benign       0.96      0.92      0.94        72
	accuracy                         0.92       114
	macro avg       0.91   0.92      0.92       114   â†’ macro avg(í´ë˜ìŠ¤ í¬ê¸° ë¬´ì‹œ)
	weighted avg    0.92   0.92      0.92       114   â†’ weighted avg(í‘œë³¸ìˆ˜ ê°€ì¤‘í‰ê· )

	[Feature Importances] (Top-10)
	- worst radius (ìµœì•…ì˜ ë°˜ê²½) : 0.714
	- worst concave points (ìµœì•…ì˜ ì˜¤ëª©ì ) : 0.119
	- texture error (ì§ˆê° ì˜¤ì°¨) : 0.054
	- worst texture (ìµœì•…ì˜ ì§ˆê°) : 0.031
	- worst concavity (ìµœì•…ì˜ ì˜¤ëª©ë„) : 0.017
	- worst smoothness (ìµœì•…ì˜ ë§¤ë„ëŸ¬ì›€) : 0.013
	- area error (ë©´ì  ì˜¤ì°¨) : 0.012
	- mean texture (í‰ê·  ì§ˆê°) : 0.012
	- worst symmetry (ìµœì•…ì˜ ëŒ€ì¹­ë„) : 0.011
	- worst area (ìµœì•…ì˜ ë©´ì ) : 0.009

<!--
â–£ ë¹„ìš©í•¨ìˆ˜(ë¶ˆìˆœë„ ì¸¡ì •) : ë¶ˆìˆœë„(Impurity)ê°€ ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë“¤ì´ ì„ì—¬ ìˆê³ , ë¶ˆìˆœë„ê°€ ë‚®ì„ìˆ˜ë¡ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•œ ë°ì´í„°ê°€ ëª…í™•<br>
(1) ì˜¤ë¶„ë¥˜ìœ¨(Misclassification rate, Error rate) : ë¶„ë¥˜ ëª¨ë¸ì´ ì˜ëª» ë¶„ë¥˜í•œ ìƒ˜í”Œì˜ ë¹„ìœ¨ë¡œ, ì „ì²´ ìƒ˜í”Œ ì¤‘ì—ì„œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì´ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.(0ì—ì„œ 100 ì‚¬ì´ì˜ ê°’, 0%: ëª¨ë¸ì´ ëª¨ë“  ìƒ˜í”Œì„ ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡, 100%: ëª¨ë¸ì´ ëª¨ë“  ìƒ˜í”Œì„ ì˜ëª» ì˜ˆì¸¡)<br><br>
$\frac{FP+FN}{TP+TN+FP+FN}$ 
###### FP(False Positive) : ì‹¤ì œ ê°’ì´ Negativeì¸ë° Positiveë¡œ ì˜ˆì¸¡, FN(False Negative) : ì‹¤ì œ ê°’ì´ Positiveì¸ë° Negativeë¡œ ì˜ˆì¸¡, TP(True Positive) : ì‹¤ì œ ê°’ì´ Positiveì´ê³  Positiveë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡, TN(True Negative) : ì‹¤ì œ ê°’ì´ Negativeì´ê³  Negativeë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡<br>
(2) ì§€ë‹ˆê³„ìˆ˜(Gini Coefficient) : ë°ì´í„°ì…‹ì´ ì–¼ë§ˆë‚˜ í˜¼í•©ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆìˆœë„ì˜ ì¸¡ì •ì¹˜(0ì—ì„œ 0.5 ì‚¬ì´ì˜ ê°’, 0: ë°ì´í„°ê°€ ì™„ë²½í•˜ê²Œ í•œ í´ë˜ìŠ¤ì— ì†í•´ ìˆìŒì„ ì˜ë¯¸í•˜ë©°, ë¶ˆìˆœë„ê°€ ì „í˜€ ì—†ëŠ” ìƒíƒœ, 0.5: ë‘ ê°œì˜ í´ë˜ìŠ¤ê°€ ì™„ë²½í•˜ê²Œ ì„ì—¬ ìˆëŠ” ìƒíƒœ)<br>
$Gini(p)=1-\sum_{i=1}^{n}p_i^2$<br><br>
(3) ì—”íŠ¸ë¡œí”¼(Entropy) : í™•ë¥  ì´ë¡ ì—ì„œ ì˜¨ ê°œë…ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ë˜ëŠ” ì •ë³´ì˜ ë¬´ì§ˆì„œë¥¼ ì¸¡ì •í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ, ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ í˜¼ë€ìŠ¤ëŸ½ê³  ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ì§€ë¥¼ ì¸¡ì •(0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’, 0 : ë°ì´í„°ê°€ ì™„ë²½í•˜ê²Œ í•œ í´ë˜ìŠ¤ì— ì†í•´ ìˆìœ¼ë©°, ë¶ˆí™•ì‹¤ì„±ì´ ì—†ëŠ” ìƒíƒœ, 1 : ë°ì´í„°ê°€ ì™„ì „íˆ ì„ì—¬ ìˆê³ , ê°€ì¥ í° ë¶ˆí™•ì‹¤ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.<br>
$Entropy(p) = -\sum_{i=1}^{n}p_ilog_2p_i$<br> 

â–£ ìœ í˜• :  ID3, CART
 - ID3 : ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ë²”ì£¼í˜• ë°ì´í„°ì¸ ê²½ìš°ì—ë§Œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë‹¤. ì •ë³´íšë“ëŸ‰(Infomation Gain)ì´ ë†’ì€ íŠ¹ì§•ë¶€í„° ë¶„ê¸°í•´ë‚˜ê°€ëŠ”ë° ì •ë³´íšë“ëŸ‰ì€ ë¶„ê¸°ì „ ì—”íŠ¸ë¡œí”¼ì™€ ë¶„ê¸°í›„ ì—”íŠ¸ë¡œí”¼ì˜ ì°¨ì´ë¥¼ ë§í•œë‹¤.(ì—”íŠ¸ë¡œí”¼ ì‚¬ìš©)<br><br>
$IG(S, A) = E(S) - E(S|A)$<br>
 - CART : Classification and Regression Treeì˜ ì•½ìë¡œ, ì´ë¦„ ê·¸ëŒ€ë¡œ ë¶„ë¥˜ì™€ íšŒê·€ê°€ ëª¨ë‘ ê°€ëŠ¥í•œ ê²°ì •íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ yes ë˜ëŠ” no ë‘ ê°€ì§€ë¡œ ë¶„ê¸°í•œë‹¤.(ì§€ë‹ˆê³„ìˆ˜ ì‚¬ìš©)<br><br> 
$f(k,t_k) = \frac{m_{left}}{m}G_{left}+\frac{m_{right}}{m}G_{right}$<br>

<br>

	from sklearn.tree import DecisionTreeClassifier
	from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

 	#ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ ëª¨ë¸ ìƒì„± (ìµœëŒ€ ê¹Šì´ 3ìœ¼ë¡œ ì„¤ì •)
	clf = DecisionTreeClassifier(max_depth=3, random_state=42)
	clf.fit(X_train, y_train)  # í•™ìŠµ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ í•™ìŠµ

	#í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
	y_pred = clf.predict(X_test)

	#ì •í™•ë„ ì¶œë ¥
	accuracy = accuracy_score(y_test, y_pred)  # ì •í™•ë„ ê³„ì‚°
	print(f"Accuracy: {accuracy * 100:.2f}%")  # ì •í™•ë„ ì¶œë ¥
-->

<br>

	(ê°œë³„ íŠ¸ë¦¬ ëª¨ë¸ì˜ ë‹¨ì )	
 	ê³„ì¸µì  êµ¬ì¡°ë¡œ ì¸í•´ ì¤‘ê°„ì— ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì—ëŸ¬ê°€ ê³„ì† ì „íŒŒ
  	í•™ìŠµ ë°ì´í„°ì˜ ë¯¸ì„¸í•œ ë³€ë™ì—ë„ ìµœì¢…ê²°ê³¼ì— í° ì˜í–¥
   	ì ì€ ê°œìˆ˜ì˜ ë…¸ì´ì¦ˆì—ë„ í° ì˜í–¥
	ë‚˜ë¬´ì˜ ìµœì¢… ë…¸ë“œ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ê³¼ì í•© ìœ„í•¨(Low Bias, Large Variance)

	(í•´ê²°ë°©ì•ˆ) ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random forest)


<br>

# [2-2] ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)
â–£ ì •ì˜ : ë§ì€ íŠ¸ë¦¬ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§Œë“¤ì–´ ë‹¤ìˆ˜ê²°ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•<br>
ì—¬ëŸ¬ ê°œì˜ Decision Treeë¥¼ ë°°ê¹…(Bagging, Bootstrap Aggregating) ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ì—¬,<br>
ê·¸ ì˜ˆì¸¡ê°’ì„ í‰ê· (íšŒê·€) ë˜ëŠ” ë‹¤ìˆ˜ê²°(ë¶„ë¥˜)ë¡œ í†µí•©í•˜ëŠ” ì•™ìƒë¸”(Ensemble) í•™ìŠµ ì•Œê³ ë¦¬ì¦˜<br>
ê° íŠ¸ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¶€íŠ¸ìŠ¤íŠ¸ë© í‘œë³¸ê³¼ ì¼ë¶€ íŠ¹ì„±(feature subset)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°„ ìƒê´€ì„ ì¤„ì´ê³ , ê³¼ì í•©(overfitting)ì„ ì™„í™”<br>
â–£ ëª©ì  : ë‹¨ì¼ ê²°ì • íŠ¸ë¦¬ì˜ ë¶ˆì•ˆì •ì„±(variance ë†’ìŒ)ì„ ë³´ì™„í•˜ê³ , ì˜ˆì¸¡ì˜ ì•ˆì •ì„±(stability)ê³¼ ì •í™•ë„(accuracy)ë¥¼ ë†’ì´ê¸°<br>
â–£ ì¥ì  : íŠ¸ë¦¬ ì—¬ëŸ¬ ê°œë¥¼ í‰ê· /íˆ¬í‘œí•¨ìœ¼ë¡œì¨ ë¶„ì‚°(variance)ì„ ë‚®ì¶°ì„œ ê³¼ì í•© ë°©ì§€, ë³€ìˆ˜ ì¤‘ìš”ë„(Feature Importance) ìë™ ì‚°ì¶œ, ë¹„ì„ í˜• ê´€ê³„ ë° ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬ì°©, ë°ì´í„° ìŠ¤ì¼€ì¼ ì¡°ì • ë¶ˆí•„ìš”, ê²°ì¸¡ê°’ì—ë„ ë¹„êµì  ê°•ê±´, ë¶„ë¥˜ì™€ íšŒê·€ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°, ì´ìƒì¹˜(outlier)ì— ë¯¼ê°í•˜ì§€ ì•ŠìŒ<br>
â–£ ë‹¨ì  : ê°œë³„ íŠ¸ë¦¬ ìˆ˜ê°€ ë§ì•„ ëª¨ë¸ í•´ì„ì´ ì–´ë µê³ , ë§ì€ íŠ¸ë¦¬ ìˆ˜ë¡œ í›ˆë ¨ê³¼ ì˜ˆì¸¡ì‹œê°„ì´ ê¸¸ì–´ì§(ë©”ëª¨ë¦¬ ë° ì—°ì‚°ëŸ‰ ì¦ê°€), íŠ¸ë¦¬ ê°„ ìƒê´€ì„± ì™„ì „ ì œê±° ë¶ˆê°€<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : ë¶„ë¥˜ìš© sklearn.ensemble.RandomForestClassifier íšŒê·€ìš© sklearn.ensemble.RandomForestRegressor<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/ensemble.html#random-forests<br>
â–£ ëª¨ë¸ì‹ : $\widehat{y}=\frac{1}{N}\sum_{i=1}^{N}T_i(X)$ ($N$ : ê²°ì •íŠ¸ë¦¬ì˜ ìˆ˜, $T_i(X)$ : ê° ê²°ì •íŠ¸ë¦¬ $i$ê°€ ì…ë ¥ê°’ $X$ì— ëŒ€í•´ ì˜ˆì¸¡í•œ ê°’)

![](./images/Bootstrap.png)
<br>
ì¶œì²˜: https://www.researchgate.net/figure/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method_fig1_309031320<br>


## ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€(Random Forest Regression)  
â–£ ì •ì˜ : ê° íŠ¸ë¦¬ê°€ ì˜ˆì¸¡í•œ ê°’ë“¤ì˜ í‰ê· ì„ í†µí•´ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ëŠ” ëª¨ë¸ë¡œ, ë‹¤ìˆ˜ê²° ëŒ€ì‹  íŠ¸ë¦¬ì—ì„œ ì–»ì€ ì˜ˆì¸¡ê°’ì˜ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ì—°ì†ê°’ ì˜ˆì¸¡<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/ensemble.html#forest<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/ensemble.html#random-forests<br>


	from sklearn.ensemble import RandomForestRegressor
 
 	# íŠ¸ë¦¬ ê°œìˆ˜ë¥¼ ë³€í™”ì‹œí‚¤ë©° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	for iTrees in nTreeList:
    		depth = None  # íŠ¸ë¦¬ ê¹Šì´ ì œí•œ ì—†ìŒ
    		maxFeat = 4  # ì‚¬ìš©í•  ìµœëŒ€ íŠ¹ì§• ìˆ˜
    		# ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    		wineRFModel = ensemble.RandomForestRegressor(n_estimators=iTrees,
			max_depth=depth, max_features=maxFeat,
			oob_score=False, random_state=531)
    		wineRFModel.fit(xTrain, yTrain)  # ëª¨ë¸ í•™ìŠµ
    		# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ê³„ì‚°
    		prediction = wineRFModel.predict(xTest)
    		# MSE ê³„ì‚° ë° ëˆ„ì 
    		mseOos.append(mean_squared_error(yTest, prediction))
     
	# MSE ì¶œë ¥
	print("MSE")
	print(mseOos)


<br>

## ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜(Random Forest Classification)    	  	
â–£ ì •ì˜ : ë‹¤ìˆ˜ì˜ Decision Treesë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•™ìƒë¸” ëª¨ë¸ë¡œ, ê° ë‚˜ë¬´ëŠ” ë…ë¦½ì ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œ í›„ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ í†µí•´ ìµœì¢… í´ë˜ìŠ¤ë¥¼ ê²°ì •<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/ensemble.html#forest<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/ensemble.html#random-forests<br>


	from sklearn.ensemble import RandomForestClassifier
	from sklearn.model_selection import train_test_split
	from sklearn.metrics import accuracy_score
	from sklearn import datasets

	# ë¶“ê½ƒ ë°ì´í„°ì…‹ ë¡œë“œ
	iris = datasets.load_iris()

	# ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ë¶„ë¦¬
	X = iris.data
	y = iris.target

	# í•™ìŠµìš© ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì´ˆê¸°í™”
	model = RandomForestClassifier()

	# ëª¨ë¸ í•™ìŠµ
	model.fit(X_train, y_train)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
	y_pred = model.predict(X_test)

	# ì •í™•ë„ ê³„ì‚°
	accuracy = accuracy_score(y_test, y_pred)
	print("Accuracy:", accuracy)
 

<br>

| êµ¬ë¶„                        | ëœë¤ í¬ë ˆìŠ¤íŠ¸ **íšŒê·€ (Random Forest Regression)**                                                 | ëœë¤ í¬ë ˆìŠ¤íŠ¸ **ë¶„ë¥˜ (Random Forest Classification)**                                              |
| :------------------------ | :---------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------- |
| **ì •ì˜**                    | ì—¬ëŸ¬ ê°œì˜ íšŒê·€ íŠ¸ë¦¬(Regression Tree)ë¥¼ í•™ìŠµì‹œì¼œ ì˜ˆì¸¡ê°’ì„ **í‰ê· **í•˜ì—¬ ì—°ì†í˜• ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸” íšŒê·€ ëª¨ë¸                    | ì—¬ëŸ¬ ê°œì˜ ë¶„ë¥˜ íŠ¸ë¦¬(Classification Tree)ë¥¼ í•™ìŠµì‹œì¼œ ê° íŠ¸ë¦¬ì˜ **íˆ¬í‘œ(Voting)** ê²°ê³¼ë¥¼ ì¢…í•©í•´ ìµœì¢… í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸” ë¶„ë¥˜ ëª¨ë¸ |
| **ì¶œë ¥ê°’ í˜•íƒœ**                | ì‹¤ìˆ˜í˜•(ì—°ì†í˜•) ì˜ˆì¸¡ê°’ (ì˜ˆ: ê°€ê²©, ì˜¨ë„, ë§¤ì¶œ ë“±)                                                            | ë²”ì£¼í˜•(ì´ì‚°í˜•) í´ë˜ìŠ¤ ë¼ë²¨ (ì˜ˆ: ìƒì¡´/ì‚¬ë§, ìŠ¤íŒ¸/ì •ìƒë©”ì¼ ë“±)                                                      |
| **ê²°ê³¼ ê³„ì‚° ë°©ì‹**              | ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’ì„ **í‰ê· (mean)** í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡                                                          | ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ í´ë˜ìŠ¤ ì¤‘ **ê°€ì¥ ë§ì´ ë“±ì¥í•œ í´ë˜ìŠ¤(ë‹¤ìˆ˜ê²°)** ì„ íƒ                                                   |
| **ì†ì‹¤ í•¨ìˆ˜ (Loss Function)** | ì¼ë°˜ì ìœ¼ë¡œ **MSE(Mean Squared Error)** ë˜ëŠ” **MAE(Mean Absolute Error)**                         | ì¼ë°˜ì ìœ¼ë¡œ **Gini ë¶ˆìˆœë„(Gini Impurity)** ë˜ëŠ” **ì—”íŠ¸ë¡œí”¼(Entropy)**                                     |
| **ëª¨ë¸ í‰ê°€ ì§€í‘œ**              | RMSE, MAE, RÂ² ë“± íšŒê·€ ì§€í‘œ ì‚¬ìš©                                                                  | ì •í™•ë„(Accuracy), ì •ë°€ë„(Precision), ì¬í˜„ìœ¨(Recall), F1-score ë“± ë¶„ë¥˜ ì§€í‘œ ì‚¬ìš©                            |
| **íŠ¹ì„± (ì°¨ì´ì )**              | - ì¶œë ¥ê°’ì´ ì—°ì†í˜•ì´ë¼ í‰ê·  ê¸°ë°˜ ì•™ìƒë¸” ì‚¬ìš©<br>- ì´ìƒì¹˜(outlier)ì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ<br>- ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í•´ì„í•˜ê¸° ì‰¬ì›€(ì˜ˆ: ì˜ˆì¸¡ ê³¡ì„ ) | - í´ë˜ìŠ¤ë³„ í™•ë¥  ì˜ˆì¸¡ ê°€ëŠ¥<br>- ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥<br>- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¬¸ì œì—ë„ í™•ì¥ ê°€ëŠ¥                             |
| **ì£¼ ì ìš© ë¶„ì•¼**               | ì£¼íƒ ê°€ê²© ì˜ˆì¸¡, ë§¤ì¶œ/ìˆ˜ìµ ì˜ˆì¸¡, ê¸°ì˜¨/ìˆ˜ìš” ì˜ˆì¸¡, ë³´í—˜ê¸ˆ ì‚°ì¶œ, ë¶€ë™ì‚° ê°€ì¹˜ í‰ê°€ ë“±                                         | íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡, ìŠ¤íŒ¸ë©”ì¼ ë¶„ë¥˜, ì‹ ìš©ì¹´ë“œ ë¶€ì •ê±°ë˜ íƒì§€, ê³ ê° ì´íƒˆ ì˜ˆì¸¡, ì§ˆë³‘ ì§„ë‹¨ ë“±                                      |
| **ì¶œë ¥ ì˜ˆì‹œ**                 | ì˜ˆ: `ì˜ˆì¸¡ ì£¼íƒê°€ê²© = 354,000ë‹¬ëŸ¬`                                                                  | ì˜ˆ: `ì˜ˆì¸¡ í´ë˜ìŠ¤ = ìƒì¡´(1)`                                                                        |


<br>


# [3-1] k-ìµœê·¼ì ‘ ì´ì›ƒ(k-Nearest Neighbors, K-NN)
â–£ ì •ì˜ : ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ìœ ì‚¬ì†ì„±ì— ë”°ë¼ ë¶„ë¥˜í•˜ì—¬ ë°ì´í„°ë¥¼ ê±°ë¦¬ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ë¶„ì„í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ,<br>
ë¹„ì§€ë„í•™ìŠµì¸ êµ°ì§‘í™”(Clustering)ê³¼ ìœ ì‚¬í•œ ê°œë…ì´ë‚˜ ê¸°ì¡´ ê´€ì¸¡ì¹˜ì˜ y ê°’ì´ ì¡´ì¬í•œë‹¤ëŠ” ì ì—ì„œ ì§€ë„í•™ìŠµì— í•´ë‹¹<br>
ìƒˆë¡œìš´ ì…ë ¥ ìƒ˜í”Œì— ëŒ€í•´ í•™ìŠµë°ì´í„° ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ğ‘˜ê°œì˜ ì´ì›ƒì„ ì°¾ì•„, ì´ë“¤ì˜ ë ˆì´ë¸”(ë¶„ë¥˜)ì´ë‚˜ í‰ê· (íšŒê·€)ì„ ì´ìš©í•´ ì˜ˆì¸¡í•˜ëŠ” ë¹„ëª¨ìˆ˜ ê¸°ë°˜ì˜ ì§€ë„í•™ìŠµ ëª¨ë¸<br> 
â–£ ëª©ì  : ë‹¨ìˆœí•˜ë©´ì„œë„ í•™ìŠµëœ ëª¨ë¸ êµ¬ì¡°ê°€ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ ë¹ ë¥´ê²Œ ì ìš© ê°€ëŠ¥í•˜ê³ , ë°ì´í„°ì˜ í˜•íƒœê°€ ë³µì¡í•˜ê±°ë‚˜ ë¹„ì„ í˜•ì¼ ë•Œ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•˜ê³ ì í•  ë•Œ ì‚¬ìš©<br>
â–£ ì¥ì  : í•™ìŠµ ë‹¨ê³„ê°€ ê±°ì˜ ì—†ê³ , êµ¬í˜„ì´ ë§¤ìš° ê°„ë‹¨, ë¹„ì„ í˜• ê²½ê³„ë‚˜ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ëª¨ë¸ë§ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : test ë¹„ìš©ì´ ìƒëŒ€ì ìœ¼ë¡œ í¬ë©°, ê³ ì°¨ì› íŠ¹ì„±ê³µê°„ì—ì„œëŠ” ê±°ë¦¬ ì¸¡ì •ì™œê³¡(ì°¨ì›ì˜ ì €ì£¼)ìœ¼ë¡œ ì„±ëŠ¥ì €í•˜, ì ì ˆí•œ ğ‘˜ì™€ ê±°ë¦¬ ë©”íŠ¸ë¦­ ì„ íƒì´ ì¤‘ìš”í•˜ë©°, ì´ìƒì¹˜ë‚˜ ë…¸ì´ì¦ˆ ë¯¼ê°ì„±<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : ë¶„ë¥˜ìš© sklearn.neighbors.KNeighborsClassifier íšŒê·€ìš© sklearn.neighbors.KNeighborsRegressor<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/neighbors.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html<br>


ë°ì´í„°ë¡œë¶€í„° ê±°ë¦¬ê°€ ê°€ê¹Œìš´ 'K'ê°œì˜ ë‹¤ë¥¸ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ ì°¸ì¡°í•˜ì—¬ ë¶„ë¥˜í• ë•Œ ê±°ë¦¬ì¸¡ì •ì€ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê³„ì‚°ë²•ì„ ì‚¬ìš©<br>
![](./images/distance.PNG)

K-NN ëª¨ë¸ì€ ê° ë³€ìˆ˜ë“¤ì˜ ë²”ìœ„ë¥¼ ì¬ì¡°ì •(í‘œì¤€í™”, ì •ê·œí™”)í•˜ì—¬ ê±°ë¦¬í•¨ìˆ˜ì˜ ì˜í–¥ì„ ì¤„ì—¬ì•¼ í•œë‹¤.<br>
(1) ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”(min-max normalization) : ë³€ìˆ˜ Xì˜ ë²”ìœ„ë¥¼ 0(0%)ì—ì„œ 1(100%)ì‚¬ì´ë¡œ ë‚˜íƒ€ëƒ„<br><br>
$X_{new} = \frac{X-min(X)}{max(X)-min(X)}$<br>

(2) z-ì ìˆ˜ í‘œì¤€í™”(z-score standardization) : ë³€ìˆ˜ Xì˜ ë²”ìœ„ë¥¼ í‰ê· ì˜ ìœ„ë˜ëŠ” ì•„ë˜ë¡œ í‘œì¤€í¸ì°¨ë§Œí¼ ë–¨ì–´ì ¸ ìˆëŠ” ì§€ì ìœ¼ë¡œ í™•ëŒ€ ë˜ëŠ” ì¶•ì†Œ(ë°ì´í„°ë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜)í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ë°ì´í„°ì˜ ì¤‘ì‹¬ì„ 0ìœ¼ë¡œ ë§ì¶”ê³ , ë°ì´í„°ë¥¼ ë‹¨ìœ„ í‘œì¤€ í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ ê°’ì„ ì¬ì¡°ì •<br><br>
$X_{new} = \frac{X-\mu}{\sigma}= \frac{X-min(X)}{StdDev(X)}$

<br>

## k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€(k-Nearest Neighbors Regression)
â–£ ì •ì˜ : ìƒˆë¡œìš´ ì…ë ¥ê°’ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì´ì›ƒ ë°ì´í„°ì˜ íƒ€ê¹ƒê°’ í‰ê· (ë˜ëŠ” ê°€ì¤‘í‰ê· ) ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë¹„ëª¨ìˆ˜ì  íšŒê·€ ì•Œê³ ë¦¬ì¦˜<br>
ì¦‰, ì…ë ¥ ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ë°ì´í„°ë“¤ì´ ë¹„ìŠ·í•œ ì¶œë ¥ê°’ì„ ê°€ì§„ë‹¤ëŠ” ê°€ì •ì— ê¸°ë°˜<br>
â–£ ê°€ì´ë“œ :â€¨https://scikit-learn.org/stable/modules/neighbors.html#regression<br>
â–£ API :â€¨https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html<br>
â–£ ì˜ˆì œ :â€¨https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html<br>
â–£ ëª¨ë¸ì‹ :â€¨https://scikit-learn.org/stable/modules/neighbors.html#id6<br>


	class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', 
	leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
	 
	# n_neighbors : int
	# ì´ì›ƒì˜ ìˆ˜ì¸ Kë¥¼ ê²°ì •í•œë‹¤. defaultëŠ” 5ë‹¤.Â 
	Â 
  	# weights : {'uniform', 'distance'} or callable
	# ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ ë°©ë²•ì„ ê²°ì •í•œë‹¤. defaultëŠ” uniformì´ë‹¤.Â 
	# 'uniform' : ê°ê°ì˜ ì´ì›ƒì´ ëª¨ë‘ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤.Â 
	# 'distance' : ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ ë” í° ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤.
	# callable : ì‚¬ìš©ìê°€ ì •ì˜í•œ í•¨ìˆ˜(ê±°ë¦¬ê°€ ì €ì¥ëœ ë°°ì—´ì„ ì…ë ¥ë°›ê³  ê°€ì¤‘ì¹˜ê°€ ì €ì¥ëœ ë°°ì—´ì„ ë°˜í™˜)
 	
	# algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}Â 
	# ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒë“¤ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê²°ì •í•œë‹¤. defaultëŠ” autoì´ë‹¤.Â 
	# 'auto' : ì…ë ¥ëœ í›ˆë ¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œë‹¤.Â 
	# 'ball_tree' : Ball-Tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.(https://nobilitycat.tistory.com/entry/ball-tree)
	# 'kd_tree' : KD-Tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.
	# 'brute' : Brute-Force íƒìƒ‰ì„ ì‚¬ìš©í•œë‹¤.Â Â 	
 	
	# leaf_size : int
	# Ball-Treeë‚˜ KD-Treeì˜ leaf sizeë¥¼ ê²°ì •í•œë‹¤. defaultê°’ì€ 30ì´ë‹¤.
	# íŠ¸ë¦¬ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ë¿ë§Œ ì•„ë‹ˆë¼, íŠ¸ë¦¬ì˜ êµ¬ì„±ê³¼ ì¿¼ë¦¬ ì²˜ë¦¬ì˜ ì†ë„ì—ë„ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.Â 
 	
	# p : int
	# ë¯¼ì½”í”„ìŠ¤í‚¤ ë¯¸í„°ë²•(Minkowski)ì˜ ì°¨ìˆ˜ë¥¼ ê²°ì •í•œë‹¤. 
	# p = 1ì´ë©´ ë§¨í•´íŠ¼ ê±°ë¦¬(Manhatten distance)
	# p = 2ì´ë©´ ìœ í´ë¦¬ë“œ ê±°ë¦¬(Euclidean distance)ì´ë‹¤.Â 

<br>

## k-ìµœê·¼ì ‘ ì´ì›ƒ ë¶„ë¥˜(k-Nearest Neighbors Classification)
â–£ ì •ì˜ : ìƒˆë¡œìš´ ìƒ˜í”Œì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì´ì›ƒ ì¤‘ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¡œ í´ë˜ìŠ¤ ë¼ë²¨ì„ ê²°ì •í•˜ëŠ” ê±°ë¦¬ ê¸°ë°˜ ë¹„ëª¨ìˆ˜ì  ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜<br>
ì¦‰, ê·¼ì ‘í•œ ë°ì´í„°ë“¤ì´ ê°™ì€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ê°€ì •ì— ë”°ë¦„<br>
â–£ ê°€ì´ë“œ :â€¨https://scikit-learn.org/stable/modules/neighbors.html#classification<br>
â–£ API :â€¨https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html<br>
â–£ ì˜ˆì œ :â€¨https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html<br>
â–£ ëª¨ë¸ì‹ :â€¨https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification<br>


	from sklearn.neighbors import KNeighborsClassifier
	kn = KNeighborsClassifier()

	#í›ˆë ¨
	kn.fit(train_input, train_target)
	#í‰ê°€
	print(kn.score(test_input, test_target))

<br>

<br>

# [3-2] ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (Support Vector Machine, SVM)
â–£ ì •ì˜ : í´ë˜ìŠ¤ ê°„ ë§ˆì§„(ì—¬ìœ í­)ì„ ìµœëŒ€í™”í•˜ëŠ” ì´ˆí‰ë©´(hyperplane)ì„ ì°¾ì•„ ë¶„ë¥˜ í˜¹ì€ íšŒê·€í•˜ëŠ” ì§€ë„í•™ìŠµ ê¸°ë²•<br> 
Nì°¨ì› ê³µê°„ì„ (N-1)ì°¨ì›ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë¶„ë¥˜ ê¸°ë²•ìœ¼ë¡œ 2ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ìµœì ì˜ ê²½ê³„ë¥¼ íƒìƒ‰<br>
â–£ ëª©ì  : íŠ¹íˆ ê²½ê³„ê°€ ì„ í˜•ì´ ì•„ë‹ˆê±°ë‚˜, ê³ ì°¨ì› ê³µê°„ì—ì„œ ë§ˆì§„ì´ ì¤‘ìš”í•œ ë¬¸ì œì— ëŒ€í•´ ê°•ê±´í•œ ë¶„ë¥˜/íšŒê·€ ëª¨ë¸ì„ êµ¬ì¶•<br>
â–£ ì¥ì  : ë§ˆì§„ ìµœëŒ€í™”ë¼ëŠ” ê²¬ê³ í•œ ì´ë¡  ê¸°ë°˜, ì»¤ë„ì„ ì‚¬ìš©í•´ ë¹„ì„ í˜• ë°ì´í„°ë„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬, ê³ ì°¨ì› íŠ¹ì„± ê³µê°„ì—ì„œ ë¹„êµì  ì˜ ì‘ë™<br>
â–£ ë‹¨ì  : í›ˆë ¨ ë° ì˜ˆì¸¡ ì‹œê°„ì´ ìƒ˜í”Œ ìˆ˜ ë° íŠ¹ì„± ìˆ˜ì— ë”°ë¼ ê¸‰ê²©íˆ ì¦ê°€, ì»¤ë„ ì„ ì •Â·í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ ê²°ê³¼ í•´ì„ì´ ê¹Œë‹¤ë¡­ë‹¤<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : ë¶„ë¥˜ìš© sklearn.svm.SVC íšŒê·€ìš© sklearn.svm.SVR<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/svm.html<br>
â–£ API : https://scikit-learn.org/stable/auto_examples/svm/index.html<br>

![](./images/margin.png)

**ìµœì ì˜ ê²½ê³„ :** ê° í´ë˜ìŠ¤ì˜ ë§ë‹¨ì— ìœ„ì¹˜í•œ ë°ì´í„°ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” ê²½ê³„<br>
**ì´ˆí‰ë©´(hyper plane) :** ê³ ì°¨ì›(Nì°¨ì›)ì—ì„œ ë°ì´í„°ë¥¼ ë‘ ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²°ì • ê²½ê³„<br>
**Support Vector :** ë°ì´í„°ë“¤ ì¤‘ì—ì„œ ê²°ì • ê²½ê³„ì— ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë“¤<br>
**ë§ˆì§„(Margin) :** ê²°ì • ê²½ê³„ì™€ support vectorì‚¬ì´ì˜ ê±°ë¦¬<br>
**ë¹„ìš©(Cost) :** ë§ˆì§„(Margin) í¬ê¸°ì˜ ë°˜ë¹„ë¡€<br>
**ê°ë§ˆ(Gamma) :** train data í•˜ë‚˜ ë‹¹ ê²°ì • ê²½ê³„ì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ë²”ìœ„ë¥¼ ì¡°ì ˆí•˜ëŠ” ë³€ìˆ˜(í¬ë©´ ì˜¤ë²„í”¼íŒ…, ì‘ìœ¼ë©´ ì–¸ë”í”¼íŒ…)<br>


| ì¥ì                              | ë‹¨ì                                               |
|----------------------------------|---------------------------------------------------|
| ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆë‹¤ | ì»¤ë„í•¨ìˆ˜ ì„ íƒì´ ëª…í™•í•˜ì§€ ì•Šë‹¤ |
| ë¶„ë¥˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ | íŒŒë¼ë¯¸í„° ì¡°ì ˆì„ ì ì ˆíˆ ìˆ˜í–‰í•´ì•¼ë§Œ ìµœì ì˜ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ |
| ì €ì°¨ì›, ê³ ì°¨ì› ê³µê°„ì˜ ì ì€ ë°ì´í„°ì— ëŒ€í•´ì„œ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ìš°ìˆ˜ | ê³„ì‚°ëŸ‰ ë¶€ë‹´ì´ ìˆë‹¤ |
| ì¡ìŒì— ê°•í•˜ë‹¤ | ë°ì´í„° íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ë§ì— ë¯¼ê°í•˜ë‹¤|
| ë°ì´í„° íŠ¹ì„±ì´ ì ì–´ë„ ì¢‹ì€ ì„±ëŠ¥ | | 

â–£ ìœ í˜• : ì„ í˜•SVM(í•˜ë“œë§ˆì§„, ì†Œí”„íŠ¸ë§ˆì§„), ë¹„ì„ í˜•SVM<br>
- í•˜ë“œë§ˆì§„ : ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ë§ˆì§„ì˜ ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ, ëª¨ë“  í›ˆë ¨ë°ì´í„°ëŠ” ë§ˆì§„ì˜ ë°”ê¹¥ì¡±ì— ìœ„ì¹˜í•˜ê²Œ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ í•˜ë‚˜ì˜ ì˜¤ì°¨ë„ í—ˆìš©í•˜ë©´ ì•ˆëœë‹¤. ëª¨ë“  ë°ì´í„°ë¥¼ ì„ í˜•ìœ¼ë¡œ ì˜¤ì°¨ì—†ì´ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê²°ì •ê²½ê³„ë¥¼ ì°¾ëŠ” ê²ƒì€ ì‚¬ì‹¤ìƒ ì–´ë µë‹¤.<br><br>
$\displaystyle \min_{w}\frac{1}{2}\left\|\left\|w\right\|\right\|^2$     ì œì•½ ì¡°ê±´ì€ ëª¨ë“  iì— ëŒ€í•´ $ğ‘¦_ğ‘–(ğ‘¤â‹…ğ‘¥_ğ‘–+ğ‘)â‰¥1$ <br>

![](./images/hmargin.png)

- ì†Œí”„íŠ¸ë§ˆì§„ :  í•˜ë“œë§ˆì§„ì´ ê°€ì§„ í•œê³„ë¥¼ ê°œì„ í•˜ê³ ì ë‚˜ì˜¨ ê°œë…ìœ¼ë¡œ, ì™„ë²½í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–´ëŠ ì •ë„ì˜ ì˜¤ë¶„ë¥˜ë¥¼ í—ˆìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì†Œí”„íŠ¸ë§ˆì§„ì—ì„œëŠ” ì˜¤ë¶„ë¥˜ë¥¼ í—ˆìš©í•˜ê³  ì´ë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ slack variableì„ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ê²°ì •ê²½ê³„ë¡œë¶€í„° ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„°ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•œë‹¤.<br><br>
$\displaystyle \min_{w}\frac{1}{2}\left\|\left\|w\right\|\right\|^2 + C\sum_{i=1}^{n}\xi_i$

![](./images/smargin.png)

- ë¹„ì„ í˜•ë¶„ë¥˜ : ì„ í˜•ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•œ ì…ë ¥ê³µê°„ì„ ì„ í˜•ë¶„ë¦¬ê°€ ê°€ëŠ¥í•œ ê³ ì°¨ì› íŠ¹ì„±ê³µê°„ìœ¼ë¡œ ë³´ë‚´ ì„ í˜•ë¶„ë¦¬ë¥¼ ì§„í–‰í•˜ê³  ê·¸ í›„ ë‹¤ì‹œ ê¸°ì¡´ì˜ ì…ë ¥ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ë©´ ë¹„ì„ í˜• ë¶„ë¦¬ë¥¼ í•˜ê²Œ ëœë‹¤.<br><br>
![](./images/nlsvm.png)

<br>

	from sklearn.datasets import make_moons
	from sklearn.pipeline import Pipeline
	from sklearn.preprocessing import PolynomialFeatures

	polynomial_svm_clf = 
	Pipeline([("poly_features", PolynomialFeatures(degree=3)),("scaler", StandardScaler()),
			("svm_clf", LinearSVC(C=10, loss="hinge", max_iter=2000, random_state=42))])
	polynomial_svm_clf.fit(X, y)

<br>

ì…ë ¥ê³µê°„ì„ íŠ¹ì„±ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œ mapping functionì„ ì‚¬ìš©í•œë‹¤<br>
$\Phi(x) = Ax$<br><br>
ê³ ì°¨ì›ì˜ íŠ¹ì„±ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ëª©ì í•¨ìˆ˜ì— ëŒ€í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì´ ê°„ë‹¨í•œ ì°¨ì›ì—ì„œëŠ” ê°€ëŠ¥í•˜ë‚˜ ê·¸ ì°¨ìˆ˜ê°€ ì»¤ì§ˆìˆ˜ë¡ ê³„ì‚°ëŸ‰ì˜ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë‹¤ì‹œ í•´ê²°í•˜ê³ ì ë‚˜ì˜¤ëŠ” ê°œë…ì´ ì»¤ë„íŠ¸ë¦­(Kernel trick) : ë¹„ì„ í˜• ë¶„ë¥˜ë¥¼ í•˜ê¸° ìœ„í•´ ì°¨ì›ì„ ë†’ì—¬ì¤„ ë•Œë§ˆë‹¤ í•„ìš”í•œ ì—„ì²­ë‚œ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì»¤ë„ íŠ¸ë¦­ì€ ì‹¤ì œë¡œëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ì„ í™•ì¥í•˜ì§€ ì•Šìœ¼ë©´ì„œ íŠ¹ì„±ì„ í™•ì¥í•œ ê²ƒê³¼ ë™ì¼í•œ íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê¸°ë²•<br>
$k(x_i, x_j) =\Phi(x_i)^T\Phi(x_j)$<br><br>
í™•ì¥ëœ íŠ¹ì„±ê³µê°„ì˜ ë‘ ë²¡í„°ì˜ ë‚´ì ë§Œì„ ê³„ì‚°í•˜ì—¬ ê³ ì°¨ì›ì˜ ë³µì¡í•œ ê³„ì‚° ì—†ì´ ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ê°„ë‹¨í•˜ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ê³  ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ê°€ìš°ì‹œì•ˆ RBF(Radial basis function)ìœ¼ë¡œ ë‘ ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë¹„ì„ í˜• ë°©ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³ ì°¨ì› íŠ¹ì§• ê³µê°„ì—ì„œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì‚¬ìš©.<br><br>
$k(x,y) = e^{-\frac{-\left\|x_i-x_j\right\|^2}{2\sigma^2}}$<br><br>
ì§ì ‘ ì°¨ìˆ˜ë¥¼ ì •í•˜ëŠ” ë°©ì‹(Polynomial) : $k(x,y) = (1+x^Ty)^p$<br>
ì‹ ê²½ë§ í•™ìŠµ(Signomail) : $k(x,y) = tanh(kx_ix_j-\delta)$<br>

![](./images/hnlsvm.png)

<br>  

## ì„œí¬íŠ¸ ë²¡í„° íšŒê·€(Support Vector Regression, SVR)
â–£ ì •ì˜ : ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ì´ˆí‰ë©´ ê·¼ì²˜ì— ë°°ì¹˜í•˜ë©´ì„œ, í—ˆìš© ì˜¤ì°¨ $Ïµ$ ë‚´ì—ì„œ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ.<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/svm.html#regression<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/svm.html#svr<br>


	from sklearn.svm import SVR
 
 	svr = SVR(kernel='rbf', gamma='auto')
	svr.fit(xtrain, ytrain)

	score = svr.score(xtest, ytest)
	print("R-squared: ", score)

<br> 

## ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜(Support Vector Classification, SVC)
â–£ ì •ì˜ : ë‘ í´ë˜ìŠ¤(ë˜ëŠ” ë‹¤ìˆ˜ì˜ í´ë˜ìŠ¤)ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ìµœëŒ€ ë§ˆì§„ì„ ê°€ì§€ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ê²ƒ.<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/svm.html#classification<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/svm.html#svc<br>


	import sklearn.svm as svm

 	# ì„ í˜•ì¼ ê²½ìš°
	svm_clf =svm.SVC(kernel = 'linear')
 	# ë¹„ì„ í˜•ì¼ ê²½ìš°
 	svm_clf =svm.SVC(kernel = 'rbf')

	# êµì°¨ê²€ì¦
	scores = cross_val_score(svm_clf, X, y, cv = 5)
 	scores.mean()

<br>



| ëª¨ë¸                                                         | ìˆ˜ì‹                                                                                                                                                                                                                                                      | ì£¼ìš” ì ìš© ë¶„ì•¼                           |
| :------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------- |
| **[1-1] LDA (Linear Discriminant Analysis)**                   | ![lda](https://latex.codecogs.com/svg.image?%5Cdelta_k%28x%29%3Dx%5E%5Ctop%20%5CSigma%5E%7B-1%7D%5Cmu_k-%5Ctfrac12%20%5Cmu_k%5E%5Ctop%20%5CSigma%5E%7B-1%7D%5Cmu_k%2B%5Clog%20%5Cpi_k)                                                                     | ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜, ì–¼êµ´ ì¸ì‹, ë¬¸ì„œ ë¶„ë¥˜, ì˜ë£Œ ë°ì´í„°(ë‹¹ë‡¨í™˜ìì˜ í˜ˆì•¡ë°ì´í„° ë¶„ë¥˜)    |
| **[1-2] QDA (Quadratic Discriminant Analysis)**                | ![qda](https://latex.codecogs.com/svg.image?%5Cdelta_k%28x%29%3D-%5Ctfrac12%20%5Clog%7C%5CSigma_k%7C-%5Ctfrac12%20%28x-%5Cmu_k%29%5E%5Ctop%20%5CSigma_k%5E%7B-1%7D%20%28x-%5Cmu_k%29%2B%5Clog%20%5Cpi_k)                                                   | ê³µë¶„ì‚°ì´ í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ë¶„ë¥˜, ìƒë¬¼ì •ë³´, ê¸ˆìœµ ë¦¬ìŠ¤í¬(ì‹ ìš©ì¹´ë“œ ë¶€ì •ê±°ë˜ íƒì§€)     |
| **[2-1] ê²°ì • íŠ¸ë¦¬ (Decision Tree)**                                | ![tree](https://latex.codecogs.com/svg.image?I%28t%29%3D-%5Csum_i%20p_i%28t%29%5Clog%20p_i%28t%29)                                                                                                                                                         | ë¶„ë¥˜Â·íšŒê·€, ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„(ê³ ê° ì´íƒˆ ì˜ˆì¸¡)            |
| **[2-2] ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)**                              | ![rf](https://latex.codecogs.com/svg.image?%5Chat{y}%28x%29%3D%5Ctfrac1B%20%5Csum_%7Bb%3D1%7D%5EB%20h_b%28x%29)                                                                                                                                            | ëŒ€ê·œëª¨ ë¶„ë¥˜Â·íšŒê·€, ë³€ìˆ˜ ì¤‘ìš”ë„, ì´ìƒ íƒì§€(ì‹ ìš© í‰ê°€, ì„¼ì„œ ë°ì´í„° ì´ìƒíƒì§€)           |
| **[3-1] K-NN (k-Nearest Neighbors)**                           | ë¶„ë¥˜: ![knn1](https://latex.codecogs.com/svg.image?%5Chat{y}%3D%5Coperatorname%7Bmode%7D%5C%7By_i%3A%20x_i%5Cin%20N_k%28x%29%5C%7D) <br> íšŒê·€: ![knn2](https://latex.codecogs.com/svg.image?%5Chat{y}%3D%5Ctfrac1k%20%5Csum_%7Bx_i%5Cin%20N_k%28x%29%7D%20y_i) | íŒ¨í„´ ì¸ì‹, ì¶”ì²œ ì‹œìŠ¤í…œ, ë¹„ëª¨ìˆ˜ ê·¼ì ‘ ì˜ˆì¸¡(ì‚¬ìš©ì ì·¨í–¥ê¸°ë°˜ ì˜í™” ì¶”ì²œ)           |
| **[3-2] SVM (Support Vector Machine)**                         | ![svm](https://latex.codecogs.com/svg.image?%5Cmin_%7Bw%2Cb%7D%20%5Ctfrac12%20%5C%7Cw%5C%7C%5E2%20%5Ctext%7Bs.t.%7D%20%20y_i%28w%5E%5Ctop%20x_i%2Bb%29%5Cge%201)                                                                                           | ì´ì§„/ë‹¤ì¤‘ ë¶„ë¥˜, ê³ ì°¨ì› í…ìŠ¤íŠ¸/ì´ë¯¸ì§€, ìƒì²´ ì‹ í˜¸(ì–¼êµ´ ê°ì • ì¸ì‹)      |


---

**ì§€ë„ í•™ìŠµì—ì„œ ì°¨ì›ì¶•ì†Œë¥¼ í•˜ëŠ” ì´ìœ **

	(1) ë‹¤ì¤‘ê³µì„ ì„±(Multicollinearity) ì œê±°
	Xì˜ íŠ¹ì„±ë“¤ì´ ì„œë¡œ ê°•í•˜ê²Œ ìƒê´€ë˜ì–´ ìˆìœ¼ë©´ íšŒê·€ ê³„ìˆ˜(ë² íƒ€)ê°€ ë¶ˆì•ˆì •í•´ì§
	MSEê°€ ë†’ì•„ì§€ê³ , ê³„ìˆ˜ì˜ ë¶€í˜¸ê°€ ë’¤ì§‘íˆê±°ë‚˜ ë¶„ì‚°ì´ í° ê°’ì´ ë¨
	PCAì™€ PLSë¥¼ í†µí•´ ì„œë¡œ ì§êµí•˜ëŠ” ìƒˆë¡œìš´ ì¶•ì„ ë§Œë“¦ìœ¼ë¡œì¨ ê³µì„ ì„± ì œê±°
	ê·¸ ê²°ê³¼ íšŒê·€ ê³„ìˆ˜ê°€ ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ì„±ëŠ¥ì´ í–¥ìƒ

	(2) ì°¨ì›ì˜ ì €ì£¼(Curse of Dimensionality) í•´ê²°
	íŠ¹íˆ K-NN, SVM ë“± ê±°ë¦¬ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ì¤‘ìš”
	ê³ ì°¨ì›ì—ì„œëŠ” ê±°ë¦¬ì™€ ë°€ë„ ê°œë…ì´ ë¬´ë„ˆì ¸ ëª¨ë¸ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë¨
	ì°¨ì›ì¶•ì†Œë¥¼ í†µí•´ ì˜ë¯¸ ìˆëŠ” ê±°ë¦¬ ê³µê°„ì„ ë‹¤ì‹œ êµ¬ì„±í•˜ë©´ ì„±ëŠ¥ê³¼ ì†ë„ê°€ ê°œì„ ë¨
	êµ°ì§‘ ë¶„ì„ì—ì„œ t-SNE, UMAPì´ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ê³¼ ë™ì¼í•œ ì›ë¦¬

	(3) ê³¼ì í•©(Overfitting) ë°©ì§€
	ê³ ì°¨ì›ì¼ìˆ˜ë¡í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ëŠ” ê²½í–¥ì´ ìˆìŒ
	íŠ¹íˆ í‘œë³¸ ìˆ˜ë³´ë‹¤ ë³€ìˆ˜ ìˆ˜ê°€ ë§ì€ ê²½ìš°(n < p)ì—ëŠ” íšŒê·€ê°€ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ê³„ì‚°ì¡°ì°¨ ë˜ì§€ ì•ŠìŒ
	ì°¨ì›ì¶•ì†ŒëŠ” ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¡ìŒ(Noise)ì¶•ì„ ì œê±°í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì˜¬ë ¤ ì¤Œ
	PLS, LDA, Supervised PCA ë“±ì€ ì¡ìŒ ì œê±°ì™€ ë™ì‹œì— ì˜ˆì¸¡ì— ë„ì›€ì´ ë˜ëŠ” ì¶•ì„ ê°•ì¡°í•˜ëŠ” ë°©ì‹

	(4) ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ
	ë‹¨ìˆœíˆ ë³€ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ëª©ì ì´ ì•„ë‹˜
	y(ë¼ë²¨)ë¥¼ ê°€ì¥ ì˜ ì˜ˆì¸¡í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” ìƒˆë¡œìš´ ì¶•ì„ ì°¾ëŠ” ê³¼ì •
	LDAëŠ” í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ê³  í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ì¶•ì„ ì°¾ìŒ
	PLS-DAëŠ” yì™€ ê³µë¶„ì‚°ì´ ê°€ì¥ í° ì¶•ì„ ì°¾ìŒ
	Supervised PCAëŠ” ë¶„ë¥˜ ë˜ëŠ” íšŒê·€ ì„±ëŠ¥ì— ë„ì›€ë˜ëŠ” ì„±ë¶„ë§Œ ì„ íƒí•¨

	(5) ê³„ì‚°ëŸ‰ ê°ì†Œ
	ê³ ì°¨ì› ë°ì´í„°ì¼ìˆ˜ë¡ í•™ìŠµ ì‹œê°„ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€
	íŠ¹íˆ SVMì€ ê³„ì‚° ë³µì¡ë„ê°€ ë§¤ìš° ë†’ì•„ ê³ ì°¨ì› ë°ì´í„°ì—ì„œëŠ” í•™ìŠµì´ ë§¤ìš° ëŠë¦¼
	PCA, PLS ë“±ì„ í†µí•´ ì°¨ì›ì„ ì¤„ì„ìœ¼ë¡œì¨ í•™ìŠµ ì†ë„ ê°œì„ 

	(6) ì‹œê°í™” ë° í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ
	nì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›ìœ¼ë¡œ í‘œí˜„ìœ¼ë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ ìš©ì´í•œ ë°ì´í„° êµ¬ì¡° íŒŒì•…
	ë¶„ë¥˜ ê²½ê³„ë‚˜ êµ°ì§‘ êµ¬ì¡°, ì´ìƒì¹˜ ë“±ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ìƒˆê±´
	íŠ¹íˆ LDA, PLS-DAëŠ” í´ë˜ìŠ¤ ê°„ êµ¬ì¡°ê°€ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚˜ëŠ” ì¥ì 
	ì—°êµ¬, ë³´ê³ ì„œ, í”„ë ˆì  í…Œì´ì…˜ì—ì„œ ë§¤ìš° ìœ ìš©í•˜ê²Œ ì‚¬ìš©

	

# [4-1] PCR (Principal Component Regression)
<br>
â–£ ì •ì˜ : ë¨¼ì € ë…ë¦½ë³€ìˆ˜ ğ‘‹ì— ëŒ€í•´ Principal Component Analysis(PCA)ë¥¼ ì ìš©í•˜ì—¬ ë¹„ì§€ë„í•™ìŠµì˜ ì°¨ì›ì¶•ì†Œ(ì£¼ì„±ë¶„)ë¥¼ ìˆ˜í–‰í•˜ê³ ,<br> 
ê·¸ ë‹¤ìŒ ì£¼ì„±ë¶„ì„ ë…ë¦½ë³€ìˆ˜ë¡œ í•˜ì—¬ ì„ í˜•íšŒê·€(OLS ë“±)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì´ì¤‘ ë‹¨ê³„ ë°©ì‹ì˜ íšŒê·€ê¸°ë²•<br> 
â–£ ëª©ì  : ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) ë¬¸ì œê°€ í¬ê±°ë‚˜, ë³€ìˆ˜ì°¨ì›ì´ ë§¤ìš° í° ê²½ìš°ì— ì°¨ì›ì„ ì¶•ì†Œí•¨ìœ¼ë¡œì¨ íšŒê·€ ì•ˆì •ì„±ì„ í™•ë³´í•˜ê³  ê³¼ì í•©ì„ ì™„í™”<br> 
â–£ ì¥ì  : ê³µì„ ì„±ì´ ì‹¬í•œ ë°ì´í„°ë‚˜ ë³€ìˆ˜ìˆ˜ê°€ ë§¤ìš° ë§ì€ ìƒí™©ì—ì„œ ìœ ìš©, ì°¨ì›ì¶•ì†Œâ†’íšŒê·€ ë‹¨ê³„ë¥¼ í†µí•´ ëª¨ë¸ ë‹¨ìˆœí™” ë° í•´ì„ ê°€ëŠ¥ì„± ì œê³ <br>
â–£ ë‹¨ì  : ì£¼ì„±ë¶„ ì„ íƒ ì‹œ â€˜ë³€ë™ì„±(variance)â€™ í° ì£¼ì„±ë¶„ì´ ë°˜ë“œì‹œ ì˜ˆì¸¡ë ¥(ì¢…ì†ë³€ìˆ˜ ì„¤ëª…ë ¥)ì´ ë†’ì€ ê²ƒì€ ì•„ë‹ˆë¼ëŠ” ì ì—ì„œ, ì¤‘ìš”í•œ ì •ë³´ê°€ ì‚¬ë¼ì§ˆ ê°€ëŠ¥ì„±<br> 
ë¹„ì§€ë„ ë°©ì‹ì˜ PCAë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ë¯€ë¡œ, ì¢…ì†ë³€ìˆ˜ ğ‘¦ì •ë³´ê°€ ì£¼ì„±ë¶„ ì„ ì •ì— ë°˜ì˜ë˜ì§€ ì•Šì•„ ì˜ˆì¸¡ë ¥ì´ ë–¨ì–´ì§ˆ ê°€ëŠ¥ì„±<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : (íŒŒì´í”„ë¼ì¸) sklearn.decomposition.PCA + sklearn.linear_model.LinearRegression<br> 
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html<br>
 
<br>

# [4-2] PLS (Partial Least Squares)

â–£ ì •ì˜ : ë…ë¦½ë³€ìˆ˜ ğ‘‹ì™€ ì¢…ì†ë³€ìˆ˜ y ì–‘ìª½ì„ ê³ ë ¤í•˜ì—¬ ìƒˆë¡œìš´ ì ì¬ë³€ìˆ˜(ì„±ë¶„)ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ ë³€ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íšŒê·€ëª¨í˜•ì„ ì í•©í•˜ëŠ” ì°¨ì›ì¶•ì†Œ íšŒê·€ê¸°ë²•<br> 
â–£ ëª©ì  : ë…ë¦½ë³€ìˆ˜ê°€ ë§ê³  ë‹¤ì¤‘ê³µì„ ì„±ì´ ì‹¬í•˜ê±°ë‚˜, ê´€ì¸¡ì¹˜<ë³€ìˆ˜ì¸ ê³ ì°¨ì› ìƒí™©ì—ì„œ ğ‘‹ì™€ y ê°„ì˜ ê³µë³€ëŸ‰ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ë°˜ì˜í•˜ë©´ì„œ íšŒê·€ëª¨í˜•ì„ êµ¬ì¶•<br> 
â–£ ì¥ì  : ğ‘‹ì™€ y ê°„ì˜ ìƒê´€/ê³µë³€ëŸ‰ì„ ê³ ë ¤í•˜ë¯€ë¡œ, PCRë³´ë‹¤ ì¢…ì†ë³€ìˆ˜ ì„¤ëª…ë ¥ì´ ë†’ê³ , ì°¨ì›ì¶•ì†Œì™€ íšŒê·€ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•˜ì—¬ ê³ ì°¨ì›/ê³µì„ ì„± ë°ì´í„°ì—ì„œ ì•ˆì •ì <br>
â–£ ë‹¨ì  : í•´ì„ì´ ë‹¤ì†Œ ë³µì¡í•˜ê³ , ì ì¬ë³€ìˆ˜ êµ¬ì„± ë°©ì‹ì´ ëœ ì§ê´€ì ì¼ ê°€ëŠ¥ì„±, êµ¬ì„± ì„±ë¶„ ìˆ˜(n_components)ê°€ ê³¼ë‹¤í•˜ê²Œ ì„ íƒí•˜ë©´ ê³¼ì í•© ìœ„í—˜ë„ ì¡´ì¬<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : sklearn.cross_decomposition.PLSRegression<br> 
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/cross_decomposition.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html<br>
 
<br>

# [4-3] PLSâ€‘DA (Partial Least Squares Discriminant Analysis)
â–£ ì •ì˜ : PLS ê¸°ë²•ì„ ë³€í˜•í•˜ì—¬ **ì¢…ì†ë³€ìˆ˜ê°€ ë²”ì£¼í˜•(yê°€ í´ë˜ìŠ¤ ë ˆì´ë¸”)**ì¸ ê²½ìš°ì— ì ìš©í•˜ëŠ” íŒë³„ë¶„ì„ í˜•íƒœì˜ ê¸°ë²•<br> 
â–£ ëª©ì  : PLSì˜ ì ì¬ë³€ìˆ˜ ì¶”ì¶œ ë°©ì‹ê³¼ íŒë³„ë¶„ì„ ë°°ì¹˜ë¥¼ ê²°í•©í•´, ê³ ì°¨ì›/ê³µì„ ì„± ìˆëŠ” ë°ì´í„°ì—ì„œ ë¶„ë¥˜ëª¨ë¸ì„ êµ¬ì¶•<br>
â–£ ì¥ì  : ì „í†µì ì¸ íŒë³„ëª¨ë¸(LDA/QDA)ë³´ë‹¤ ë³€ìˆ˜ ìˆ˜ê°€ ë§ê±°ë‚˜ íŠ¹ì„± ê°„ ìƒê´€ì´ ë†’ì„ ë•Œ ìœ ë¦¬<br>
â–£ ë‹¨ì  : scikit-learnì—ì„œ ê³µì‹ì ìœ¼ë¡œ ë…ë¦½ëœ â€œPLS-DAâ€ í´ë˜ìŠ¤ê°€ ì œê³µë˜ì§€ ì•ŠìŒ. ì ì¬ë³€ìˆ˜ í•´ì„ì´ ì–´ë µê³ , íŠœë‹ì´ ë³µì¡í•  ê°€ëŠ¥ì„±<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : ê³µì‹ í´ë˜ìŠ¤ ì—†ìŒ â†’ ì¼ë°˜ì ìœ¼ë¡œ PLSRegression + ë²”ì£¼í˜• y â†’ í›„ì²˜ë¦¬ íŒë³„ë¶„ì„ í˜•íƒœë¡œ êµ¬í˜„<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/cross_decomposition.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html<br>

<br>

# [4-4] Supervised PCA
â–£ ì •ì˜ : ì¼ë°˜ PCAê°€ ë…ë¦½ë³€ìˆ˜ ğ‘‹ë§Œì„ ê³ ë ¤í•´ ì£¼ì„±ë¶„ì„ ì¶”ì¶œí•˜ëŠ” ë° ë°˜í•´, ì¢…ì†ë³€ìˆ˜ ğ‘¦ ì •ë³´ê¹Œì§€ ì´ìš©í•´ ì°¨ì›ì¶•ì†Œë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹(ì¦‰, ì§€ë„í˜• ì°¨ì›ì¶•ì†Œ)<br>
â–£ ëª©ì  : ì°¨ì›ì¶•ì†Œí•˜ë©´ì„œë„ ğ‘¦ì™€ì˜ ê´€ê³„(ì˜ˆì¸¡ë ¥)ë¥¼ ë³´ì¡´í•˜ë ¤ëŠ” ëª©ì <br>
â–£ ì¥ì  : ë‹¨ìˆœ PCAë³´ë‹¤ ì˜ˆì¸¡ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒ, ë³€ìˆ˜ ìˆ˜ê°€ ë§ê³  ì˜ˆì¸¡ë³€ìˆ˜â†’ì¢…ì†ë³€ìˆ˜ ê°„ ê´€ê³„ê°€ ë³µì¡í•  ë•Œ ìœ ë¦¬<br>
â–£ ë‹¨ì  : scikit-learnì—ì„œ í•˜ë‚˜ì˜ í‘œì¤€ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì œê³µë˜ì§€ëŠ” ì•Šì•„ êµ¬í˜„ì— ìœ ì—°ì„±ì´ í•„ìš”, í•´ì„ì´ ë‹¤ì†Œ ì–´ë µê³ , ê³¼ì í•© ê°€ëŠ¥ì„±<br>
â–£ Scikit-learn í´ë˜ìŠ¤ëª… : ê³µì‹ ì œê³µ ì—†ìŒ<br>
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/decomposition.html<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html<br>



| ëª¨ë¸                                                         | ìˆ˜ì‹                                                                                                                                                                                                                                                      | ì£¼ìš” ì ìš© ë¶„ì•¼                           |
| :------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------- |
| **[4-1] PCR (Principal Component Regression)**                 | ![pcr](https://latex.codecogs.com/svg.image?Z%3DXW_%7BPCA%7D%2C%20%5Chat{y}%3DZ%5Chat{%5Cbeta})                                                                                                                                                            | ë‹¤ì¤‘ê³µì„ ì„± ì™„í™” íšŒê·€, ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„, ê³µì • ë°ì´í„° ì˜ˆì¸¡(ë°˜ë„ì²´ ê³µì • ê²°í•¨ ì˜ˆì¸¡)    |
| **[4-2] PLS (Partial Least Squares)**                          | ![pls](https://latex.codecogs.com/svg.image?X%3DTP%5E%5Ctop%2BE%2C%20Y%3DUQ%5E%5Ctop%2BF%2C%20%5Cmax%20%5Coperatorname%7BCov%7D%28T%2CU%29)                                                                                                                | Xâ€“Y ìƒê´€ì´ ë†’ì€ ì˜ˆì¸¡, í™”í•™ê³„ëŸ‰í•™, ê³µì • ëª¨ë‹ˆí„°ë§(ìƒì‚°ë¼ì¸ í’ˆì§ˆê´€ë¦¬)      |
| **[4-3] PLS-DA (Partial Least Squares Discriminant Analysis)** | ![plsda](https://latex.codecogs.com/svg.image?Y%5Cin%7B0%2C1%2C%5Cdots%7D%2C%20T%3DXW%2C%20%5Cmax%20%5Coperatorname%7BCov%7D%28T%2CY%29)                                                                                                                   | ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜, ì˜¤ë¯¹ìŠ¤ ë¶„ì„, í’ˆì§ˆ ê²€ì‚¬, ë°”ì´ì˜¤ë§ˆì»¤ íƒìƒ‰(ì•” ë‹¨ë°±ì§ˆì²´ ë°ì´í„° ë¶„ì„) |
| **[4-4] Supervised PCA**                                       | ![spca](https://latex.codecogs.com/svg.image?%5Cmax_%7Bw%3A%5C%7Cw%5C%7C%3D1%7D%20%5Coperatorname%7BCorr%7D%28Xw%2C%20y%29)                                                                                                                                | ë¼ë²¨ ì •ë³´ í™œìš© ì°¨ì›ì¶•ì†Œ, ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì „ì²˜ë¦¬(ê°ì • ë¶„ë¥˜, ë‰´ìŠ¤ê¸°ì‚¬ ì£¼ì œ ë¶„ë¥˜)      |


<br>



| ì•Œê³ ë¦¬ì¦˜                                                           | ë¶„ë¥˜/íšŒê·€ | ì ìš© ë¶„ì•¼                                                                      |
| -------------------------------------------------------------- | ----- | -------------------------------------------------------------------------- |
| **[1-1] LDA (Linear Discriminant Analysis)**                   | ë¶„ë¥˜    | ì–¼êµ´ ì¸ì‹, ì§ˆë³‘ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ë¶„ë¥˜(ë¬¸ì„œ/ìŠ¤íŒ¸ ì´ë©”ì¼ íƒì§€), ìŒì„± ì¸ì‹ ë° íŒ¨í„´ ì¸ì‹                          |
| **[1-2] QDA (Quadratic Discriminant Analysis)**                | ë¶„ë¥˜    | ì–¼êµ´ ì¸ì‹, ì˜í•™ì  ì§ˆë³‘ ì˜ˆì¸¡(LDAë³´ë‹¤ ë¹„ì„ í˜• ë°ì´í„°ì— ì í•©), ì¬ë¬´ ë¦¬ìŠ¤í¬ í‰ê°€                             |
| **[2-1] ê²°ì • íŠ¸ë¦¬ (Decision Tree)**                                | ë¶„ë¥˜/íšŒê·€ | ê³ ê° ì´íƒˆ ì˜ˆì¸¡, ë§ˆì¼€íŒ… ë¶„ì„, ì˜ì‚¬ê²°ì • ì§€ì› ì‹œìŠ¤í…œ, ì˜ë£Œ ì§„ë‹¨ ë° ì˜ˆì¸¡, ê¸ˆìœµ ë° íˆ¬ì ë¶„ì„(ì‚¬ê¸° íƒì§€ ë“±)             |
| **[2-2] ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)**                              | ë¶„ë¥˜/íšŒê·€ | ì´ë¯¸ì§€ ë¶„ì„(ê°ì²´ ì¸ì‹, ì–¼êµ´ ì¸ì‹), ìœ ì „ì ë°ì´í„° ë¶„ì„, ê¸ˆìœµ ë¶„ì•¼(ë¦¬ìŠ¤í¬ ë¶„ì„, ì£¼ì‹ ì˜ˆì¸¡), ëŒ€ê·œëª¨ ë°ì´í„° ì˜ˆì¸¡(ê³¼ì í•© ë°©ì§€) |
| **[3-1] k-ìµœê·¼ì ‘ ì´ì›ƒ (k-Nearest Neighbors, K-NN)**                 | ë¶„ë¥˜/íšŒê·€ | ì¶”ì²œ ì‹œìŠ¤í…œ(ì½˜í…ì¸ /ìƒí’ˆ ì¶”ì²œ), ì§ˆë³‘ ì˜ˆì¸¡ ë° ì§„ë‹¨, ì´ë¯¸ì§€ ë¶„ë¥˜ ë° ì˜ìƒ ì²˜ë¦¬, ìŒì„± ì¸ì‹                       |
| **[3-2] ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (Support Vector Machine, SVM)**              | ë¶„ë¥˜/íšŒê·€ | í…ìŠ¤íŠ¸ ë¶„ë¥˜(ë¬¸ì„œ/ìŠ¤íŒ¸ íƒì§€), ì–¼êµ´ ì¸ì‹, ìŒì„± ì¸ì‹, ì´ë¯¸ì§€ ë¶„ë¥˜ ë° ë¬¼ì²´ ì¸ì‹, ì˜ë£Œ ë°ì´í„° ë¶„ì„(ì§ˆë³‘ ì˜ˆì¸¡, ìœ ì „ì ë¶„ì„)   |
| **[4-1] PCR (Principal Component Regression)**                 | íšŒê·€    | ê²½ì œí•™ ë° ì¬ë¬´ ë°ì´í„° ë¶„ì„(ìƒê´€ê´€ê³„ê°€ í° ê²½ìš°), í™”í•™ì  ë¶„ì„(ê³ ì°¨ì› ë°ì´í„°), ê¸°í›„ ì˜ˆì¸¡ ë° í™˜ê²½ ë°ì´í„° ë¶„ì„            |
| **[4-2] PLS (Partial Least Squares)**                          | íšŒê·€    | í™”í•™ ë° ìƒë¬¼í•™ì  ë°ì´í„° ë¶„ì„(ìŠ¤í™íŠ¸ëŸ¼, ìœ ì „ì ë°ì´í„°), ê³ ê° í–‰ë™ ë¶„ì„(êµ¬ë§¤ ì˜ˆì¸¡), ê²½ì œí•™ ë° í™˜ê²½ ëª¨ë¸ë§             |
| **[4-3] PLS-DA (Partial Least Squares Discriminant Analysis)** | ë¶„ë¥˜    | ìœ ì „ì ë°ì´í„° ë¶„ì„, ë¶„ì ìƒë¬¼í•™ ì—°êµ¬, í™”í•™ ë¶„ì„, ì•½ë¬¼ ë°˜ì‘ ì˜ˆì¸¡, ë§ˆì¼€íŒ… ë¶„ì„(ê³ ê° ì„¸ë¶„í™” ë° íƒ€ê¹ƒ ë§ˆì¼€íŒ…)            |
| **[4-4] Supervised PCA**                                       | íšŒê·€/ë¶„ë¥˜ | ë°ì´í„° ì°¨ì› ì¶•ì†Œ í›„ íšŒê·€ ë° ë¶„ë¥˜ ì˜ˆì¸¡(ê³ ì°¨ì› ë°ì´í„° ì„±ëŠ¥ í–¥ìƒ), ì´ë¯¸ì§€Â·ìŒì„± ë°ì´í„° ë¶„ì„, ê¸ˆìœµÂ·ì˜ë£Œ ë°ì´í„° ì°¨ì› ì¶•ì†Œ ë¶„ì„   |

<br>

| ì•Œê³ ë¦¬ì¦˜                                                      | ìºê¸€ ì‚¬ë¡€                                    | íŠ¹ì§•                                                                |
| :------------------------------------------------------------- | :--------------------------------------------- | :--------------------------------------------------------------------- |
| **[1-1] LDA (Linear Discriminant Analysis)**                   | ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹, ì™€ì¸ í’ˆì§ˆ ë¶„ë¥˜, ì§ˆë³‘ ì§„ë‹¨(ì˜ˆ: ë‹¹ë‡¨ë³‘ ìœ ë¬´)          | í´ë˜ìŠ¤ ê°„ ë¶„ì‚° ëŒ€ë¹„ í´ë˜ìŠ¤ ë‚´ ë¶„ì‚° ìµœì†Œí™”ë¡œ ì„ í˜• ë¶„ë¦¬ ìˆ˜í–‰, ì°¨ì› ì¶•ì†Œì™€ ë¶„ë¥˜ë¥¼ ë™ì‹œì— ìˆ˜í–‰, ì €ì°¨ì› ë°ì´í„°ì— ì í•©     |
| **[1-2] QDA (Quadratic Discriminant Analysis)**                | ì‹ ìš©ì¹´ë“œ ë¶€ì • ê±°ë˜ íƒì§€, ìƒë¬¼ ì¢… ë¶„ë¥˜, ê³ ê° ì´íƒˆ ì˜ˆì¸¡               | í´ë˜ìŠ¤ë³„ ê³µë¶„ì‚°ì„ ë‹¤ë¥´ê²Œ ê°€ì •í•˜ì—¬ ë¹„ì„ í˜• ê²½ê³„ ê°€ëŠ¥, ì†Œê·œëª¨ ë°ì´í„°ì— ìœ ë¦¬, ê³¼ì í•© ìœ„í—˜ ì¡´ì¬                   |
| **[2-1] ê²°ì • íŠ¸ë¦¬ (Decision Tree)**                                | íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡, ì£¼íƒ ê°€ê²© ì˜ˆì¸¡, ê³ ê° ì„¸ë¶„í™”                  | í•´ì„ ìš©ì´í•˜ê³  ì‹œê°í™” ê°€ëŠ¥, ë¹„ì„ í˜• ê´€ê³„ë„ í•™ìŠµ ê°€ëŠ¥, ê³¼ì í•© ìœ„í—˜ ë†’ìŒ                               |
| **[2-2] ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)**                              | íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡, ì‹ ìš©ì ìˆ˜ ì˜ˆì¸¡, ë¶€ë™ì‚° ê°€ê²© ì˜ˆì¸¡, ë²”ì£„ ë°œìƒ ì˜ˆì¸¡      | ì—¬ëŸ¬ ê²°ì •íŠ¸ë¦¬ë¥¼ ë°°ê¹…ìœ¼ë¡œ ê²°í•©í•œ ì•™ìƒë¸” ëª¨ë¸, ë³€ìˆ˜ ì¤‘ìš”ë„ í•´ì„ ê°€ëŠ¥, ìºê¸€ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ ì¤‘ í•˜ë‚˜        |
| **[3-1] k-ìµœê·¼ì ‘ ì´ì›ƒ (k-NN)**                                      | ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜, ì£¼íƒ ê°€ê²© ì˜ˆì¸¡, ê³ ê° í–‰ë™ ì˜ˆì¸¡              | ë‹¨ìˆœí•˜ë©´ì„œë„ ë¹„ì„ í˜• ë¶„ë¥˜ ê°€ëŠ¥, ë°ì´í„° ì •ê·œí™” í•„ìš”, ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œëŠ” ê³„ì‚°ëŸ‰ ë¶€ë‹´ í¼                      |
| **[3-2] ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (SVM)**                                      | ìŠ¤íŒ¸ë©”ì¼ ë¶„ë¥˜, ì–¼êµ´ ì¸ì‹, í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„, ì¬ê³  ìˆ˜ìš” ì˜ˆì¸¡            | ê³ ì°¨ì› ê³µê°„ì—ì„œ ìµœì  ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë¶„ë¥˜ê¸°, ì»¤ë„ íŠ¸ë¦­ìœ¼ë¡œ ë¹„ì„ í˜• ë¬¸ì œ í•´ê²°, ë°ì´í„°ê°€ ì ì„ ë•Œ ê°•ë ¥í•˜ì§€ë§Œ ëŒ€ìš©ëŸ‰ì—ëŠ” ë¹„íš¨ìœ¨ì  |
| **[4-1] PCR (Principal Component Regression)**                 | í™”í•™ ë¶„ì„(ìŠ¤í™íŠ¸ëŸ¼ ë°ì´í„°), ìƒë¬¼í•™ ì‹¤í—˜ ë°ì´í„°(ìœ ì „ì ë°œí˜„), ì œì¡° ê³µì • ë°ì´í„° | PCAë¡œ ì°¨ì› ì¶•ì†Œ í›„ íšŒê·€ ìˆ˜í–‰, ë…ë¦½ë³€ìˆ˜ ê°„ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ í•´ê²°, ì˜ˆì¸¡ë ¥ì€ ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ë‹¬ë¼ì§            |
| **[4-2] PLS (Partial Least Squares)**                          | í™”í•™ ê³µì • ë°ì´í„°, ì™€ì¸ í’ˆì§ˆ ì˜ˆì¸¡, ê³µì • ìµœì í™”                    | ì…ë ¥(X)ê³¼ ì¶œë ¥(Y) ëª¨ë‘ë¥¼ ê³ ë ¤í•˜ì—¬ ì¶•ì„ êµ¬ì„±, PCRë³´ë‹¤ ì˜ˆì¸¡ë ¥ ë†’ìŒ, ê³ ì°¨ì› ê³µì • ë°ì´í„°ì— ìœ ìš©              |
| **[4-3] PLS-DA (Partial Least Squares Discriminant Analysis)** | ìƒë¬¼í•™(ìœ ì „ìÂ·ëŒ€ì‚¬ì²´ ë¶„ì„), ì‹í’ˆ í’ˆì§ˆ ë¶„ë¥˜, ì§ˆë³‘ ì§„ë‹¨               | PLSë¥¼ ë¶„ë¥˜ ë¬¸ì œì— í™•ì¥í•œ ëª¨ë¸, ìƒ˜í”Œ ìˆ˜ ì ì€ ìƒë¬¼í•™ì  ë°ì´í„°ì— ì í•©, ë³€ìˆ˜ ì„ íƒ ê¸°ëŠ¥ ì œê³µ                  |
| **[4-4] Supervised PCA**                                       | ì´ë¯¸ì§€ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ìœ ì „ì ë°ì´í„° ë¶„ì„                     | ë ˆì´ë¸” ì •ë³´ë¥¼ í™œìš©í•œ ì°¨ì› ì¶•ì†Œ, ë…¸ì´ì¦ˆ ë§ì€ ë°ì´í„°ì—ì„œ ìœ ìš©, í•´ì„ë ¥ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ ëª¨ë‘ ìš°ìˆ˜                  |


<br>


**(ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì ˆì°¨ ì‚¬ìš©ì˜ˆì‹œ)**
 
![](./images/SLC.png)
<br>ì¶œì²˜ : https://towardsdatascience.com/top-machine-learning-algorithms-for-classification-2197870ff501
