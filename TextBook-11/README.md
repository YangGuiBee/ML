#  11 : ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning, UL) : ì—°ê´€ê·œì¹™, ì°¨ì›ì¶•ì†Œ

---

## ì—°ê´€ ê·œì¹™(Association Rule)
<br>

    [1] Apriori
    [2] FP-Growth(Frequent Pattern Growth) 
    [3] Eclat(Equivalence Class Transformation)
    [4] Multi-level Association Rules
    [5] Multi-dimensional Association Rules
    [6] AIS(Artificial Immune System)
       
    [ì—°ê´€ ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]
    â–£ ì§€ì§€ë„(Support) : íŠ¹ì • í•­ëª© ì§‘í•©ì´ ì „ì²´ ê±°ë˜ì—ì„œ ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ ì‹ ë¢°ë„(Confidence) : Aê°€ ì£¼ì–´ì¡Œì„ ë•Œ Bê°€ ë°œìƒí•  í™•ë¥ 
    â–£ í–¥ìƒë„(Lift) : Aì™€ Bê°€ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê²½ìš°ì— ë¹„í•´ Aê°€ ë°œìƒí–ˆì„ ë•Œ Bê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ ë ˆë²„ë¦¬ì§€(Leverage) : Aì™€ Bì˜ ê²°í•© ë¹ˆë„ê°€ ë‘ í•­ëª©ì´ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¹ˆë„ì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ Conviction(í™•ì‹ ë„) : Aê°€ ë°œìƒí•  ë•Œ Bê°€ ë°œìƒí•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë…ë¦½ì ì¸ ê²½ìš°ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“œëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ ì»¬ëŸ¬ë ˆì´ì…˜ ê³„ìˆ˜(Correlation Coefficient) = ìƒê´€ê³„ìˆ˜(Correlation Coefficient)ëŠ” ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ì˜ ê°•ë„ì™€ ë°©í–¥

    

## ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction)
<br>

    [1] PCA(Principal Component Analysis)
    [2] t-SNE(t-distributed Stochastic Neighbor Embedding)
    [3] UMAP(Uniform Manifold Approximation and Projection)
    [4] SVD(Singular Value Decomposition)
    [5] ICA(Independent Component Analysis)
    [6] LDA(Linear Discriminant Analysis)
    [7] Isomap
    [8] MDS(Multidimensional Scaling)
    [9] SOM(Self-Organizing Maps)

    [ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]
    â–£ ì¬êµ¬ì„± ì˜¤ë¥˜(Reconstruction Error) : ë³µì›ëœ ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„° ê°„ì˜ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)
    â–£ ë¶„ì‚° ìœ ì§€ìœ¨(Explained Variance Ratio) : ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨ë¡œ ë°ì´í„°ì˜ ì •ë³´ ì†ì‹¤ì •ë„ íŒŒì•…
    â–£ ìƒí˜¸ ì •ë³´ëŸ‰(Mutual Information) :  ì°¨ì› ì¶•ì†Œ ì „í›„ ë°ì´í„°ì˜ ì •ë³´ëŸ‰ì„ ë¹„êµ
    â–£ êµ°ì§‘ í‰ê°€ ì§€í‘œ : Silhouette Score, Davies-Bouldin Index, ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ(ARI, NMI)

---  
**ì—°ê´€ ê·œì¹™ ì¶”ì²œ(Assocication Rule based Recommendation) :** ë¹…ë°ì´í„° ê¸°ë°˜ì˜ ë°ì´í„° ë§ˆì´ë‹ê¸°ë²•<br>
"Aë¥¼ ì„ íƒí•˜ë©´(antecedent), Bë„ ì„ íƒí•œë‹¤(Consequent)"ëŠ” ê·œì¹™ì„ ì°¾ëŠ”ë‹¤.<br>
<br>
![](./images/data.PNG)
<br>

# [1] Apriori
![](./images/apriori.png)
<br>
https://nyamin9.github.io/data_mining/Data-Mining-Pattern-3/#-31-apriori-algorithm---example<br><br>
â–£ ì •ì˜ : ì—°ê´€ê·œì¹™ í•™ìŠµì„ ìœ„í•œ ê³ ì „ì ì¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë¹ˆë°œí•­ëª© ì§‘í•©(frequent itemsets)ì„ ì°¾ì•„ë‚´ê³  ê·¸ ì§‘í•© ê°„ ì—°ê´€ì„±ì„ ì¶”ì¶œ<br>
â–£ í•„ìš”ì„± : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ì—°ê´€ì„±ì„ ë°œê²¬í•˜ëŠ” ì‘ì—…ì€ ê³„ì‚° ë¹„ìš©ì´ ë†’ì„ ìˆ˜ ìˆëŠ”ë°, AprioriëŠ” ë¹ˆë°œí•˜ì§€ ì•Šì€ í•­ëª© ì§‘í•©ì„ ë¨¼ì € ì œê±°í•´ ê²€ìƒ‰ ê³µê°„ì„ ì¤„ì—¬ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ íš¨ìœ¨ì ì¸ íƒìƒ‰<br>
â–£ ì¥ì  : ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì´í•´í•˜ê¸° ì‰½ê³ , ê³„ì‚° ê³µê°„ì„ ì¤„ì´ê¸° ìœ„í•œ ì‚¬ì „ ë‹¨ê³„ë¥¼ ê°€ì§€ê³  ìˆì–´, íš¨ìœ¨ì ì¸ íƒìƒ‰ì´ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ íƒìƒ‰ ê³µê°„ì´ ì»¤ì§€ë©´ ì„±ëŠ¥ì´ ì €í•˜ë˜ê³  ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë©°, ë§¤ë²ˆ ìƒˆë¡œìš´ í›„ë³´ì§‘í•© ìƒì„±ì— ë”°ë¥¸ í° ê³„ì‚°ë¹„ìš©<br>
â–£ ì‘ìš©ë¶„ì•¼ : ì‹œì¥ ë°”êµ¬ë‹ˆ ë¶„ì„(ì¥ë°”êµ¬ë‹ˆ ë°ì´í„°ì—ì„œ ìì£¼ í•¨ê»˜ êµ¬ë§¤ë˜ëŠ” ìƒí’ˆì„ ì°¾ìŒ), ì¶”ì²œ ì‹œìŠ¤í…œ, ì›¹ í˜ì´ì§€ ì—°ê²°ì„± ë¶„ì„<br>
â–£ ëª¨ë¸ì‹ : ì§€ì§€ë„(Support): íŠ¹ì • í•­ëª© ì§‘í•©ì´ ì „ì²´ ê±°ë˜ì—ì„œ ë°œìƒí•˜ëŠ” ë¹ˆë„, ì‹ ë¢°ë„(Confidence): íŠ¹ì • í•­ëª©ì´ ë°œìƒí•œ ê²½ìš° ë‹¤ë¥¸ í•­ëª©ì´ í•¨ê»˜ ë°œìƒí•  í™•ë¥ , í–¥ìƒë„(Lift): í•­ëª© ê°„ì˜ ìƒí˜¸ì˜ì¡´ì„±ì„ ì¸¡ì •<br>

	import pandas as pd	
	import matplotlib.pyplot as plt
 	from mlxtend.frequent_patterns import apriori
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='blue')
	plt.title('Item Frequency')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# apriori ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)
	
	# ìˆ˜ë™ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ ê³„ì‚°
	rules = []
	for itemset in frequent_itemsets['itemsets']:
	    if len(itemset) > 1:
	        for antecedent in combinations(itemset, len(itemset) - 1):
	            antecedent = frozenset(antecedent)
	            consequent = itemset - antecedent
	            
	            # ì§€ì§€ë„ ê³„ì‚°
	            support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	            
	            # ì‹ ë¢°ë„ ê³„ì‚°
	            antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	            confidence = support / antecedent_support
	            
	            # í–¥ìƒë„ ê³„ì‚°
	            consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	            lift = confidence / consequent_support
	            
	            # ê·œì¹™ ì €ì¥
	            rules.append({
	                'antecedents': antecedent,
	                'consequents': consequent,
	                'support': support,
	                'confidence': confidence,
	                'lift': lift
	            })
	
	# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
	rules_df = pd.DataFrame(rules)
	print("Association Rules:")
 	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

![](./images/1-1.png)
<br>

# [2] FP-Growth(Frequent Pattern Growth)
â–£ ì •ì˜: Apriori ì•Œê³ ë¦¬ì¦˜ì˜ ëŒ€ì•ˆìœ¼ë¡œ FP-Tree(Frequent Pattern Tree)ë¥¼ í†µí•´ ë¹ˆë°œí•­ëª© ì§‘í•©ì„ ìƒì„±í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, Aprioriì™€ ë‹¬ë¦¬ ë§¤ë²ˆ í›„ë³´ì§‘í•©ì„ ìƒì„±í•˜ì§€ ì•Šìœ¼ë©°, ë°ì´í„°ì˜ íŠ¸ëœì­ì…˜ì„ ì§ì ‘ íƒìƒ‰í•˜ì—¬ ë¹ˆë°œí•­ëª© ì§‘í•©ì„ êµ¬í•œë‹¤.<br>
â–£ í•„ìš”ì„±: Aprioriì˜ ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆ<br>
â–£ ì¥ì : ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ê³ , ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë¹ ë¥´ê²Œ ì‘ë™<br>
â–£ ë‹¨ì : FP-íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë©°, êµ¬í˜„ì´ ë³µì¡í•˜ê³  FP-Tree ìƒì„±ì„ ìœ„í•œ í•™ìŠµì´ í•„ìš”<br>
â–£ ì‘ìš©ë¶„ì•¼: ëŒ€ê·œëª¨ ë°ì´í„° ë¶„ì„, ì „ììƒê±°ë˜ ì¶”ì²œ ì‹œìŠ¤í…œ<br>

	import pandas as pd	
	import matplotlib.pyplot as plt
 	from mlxtend.frequent_patterns import fpgrowth
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='green')
	plt.title('Item Frequency (FP-Growth)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# FP-Growth ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)
	
	# ìˆ˜ë™ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ ê³„ì‚°
	rules = []
	for itemset in frequent_itemsets['itemsets']:
	    if len(itemset) > 1:
	        for antecedent in combinations(itemset, len(itemset) - 1):
	            antecedent = frozenset(antecedent)
	            consequent = itemset - antecedent
	            
	            # ì§€ì§€ë„ ê³„ì‚°
	            support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	            
	            # ì‹ ë¢°ë„ ê³„ì‚°
	            antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	            confidence = support / antecedent_support
	            
	            # í–¥ìƒë„ ê³„ì‚°
	            consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	            lift = confidence / consequent_support
	            
	            # ê·œì¹™ ì €ì¥
	            rules.append({
	                'antecedents': antecedent,
	                'consequents': consequent,
	                'support': support,
	                'confidence': confidence,
	                'lift': lift
	            })
	
	# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
	rules_df = pd.DataFrame(rules)
	print("Association Rules (FP-Growth):")
	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

![](./images/1-2.png)   
<br>

# [3] Eclat(Equivalence Class Transformation)
![](./images/eclat.png)  
<br>
chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.philippe-fournier-viger.com/COURSES/Pattern_mining/Eclat.pdf
<br>
ë…¸ë€ìƒ‰ ë¹ˆë°œ ì§‘í•© : ì‚¬ì „ ì •ì˜ëœ ìµœì†Œ ì§€ì§€ë„(minimum support) ì´ìƒì˜ ì§€ì§€ë„ë¥¼ ê°€ì§€ëŠ” í•­ëª©ì˜ ì¡°í•©<br><br>
â–£ ì •ì˜: Aprioriì™€ FP-Growthì˜ ëŒ€ì•ˆìœ¼ë¡œ, íŠ¸ëœì­ì…˜ ê°„ì˜ ê³µí†µí•­ëª©(êµì§‘í•©)ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆë°œí•­ëª©ì„ ì¶”ì¶œí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ì•„ë„ íŠ¸ëœì­ì…˜ ê°„ êµì°¨ ê³„ì‚°ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ì„ ë„ì¶œ<br>
â–£ ì¥ì  : ìˆ˜í‰ì  ë°ì´í„° êµ¬ì¡°ë¥¼ ì´ìš©í•˜ì—¬ íŠ¸ëœì­ì…˜ ë°ì´í„°ì—ì„œ ë¹ˆë°œ í•­ëª© ì§‘í•©ì„ ë¹ ë¥´ê²Œ ì°¾ê³ , ì €ì¥ ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, êµì°¨ ì—°ì‚°ì„ í†µí•´ ë¹ˆë°œ í•­ëª©ì„ ì¶”ì¶œ<br>
â–£ ë‹¨ì  : íŠ¸ëœì­ì…˜ ID ì§‘í•©ì„ ê³„ì† ì—…ë°ì´íŠ¸í•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ì¦ê°€í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œëŠ” íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼ : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ë¹ˆë°œ íŒ¨í„´ ë¶„ì„, ì›¹ í´ë¦­ ë¡œê·¸ ë¶„ì„, í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© ë¶„ì„<br>
â–£ ëª¨ë¸ì‹ : í•­ëª© ì§‘í•©ì˜ ì§€ì§€ë„ ê³„ì‚°ì„ ìœ„í•´ íŠ¸ëœì­ì…˜ ID ì§‘í•©ì˜ êµì§‘í•©ì„ ì‚¬ìš©í•˜ë©° ë¹ˆë°œí•­ëª© ì§‘í•©ì˜ ì§€ì§€ë„ë¥¼ ê³„ì‚°í•  ë•Œ êµì§‘í•©ì„ í†µí•´ ë¹ˆë°œ í•­ëª©ì„ ì°¾ì•„ë‚¸ë‹¤<br>

	import pandas as pd
	import matplotlib.pyplot as plt
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='purple')
	plt.title('Item Frequency (Eclat)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# Eclat ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
	def eclat(data, min_support=0.4):
	    # í•­ëª©ë³„ ì§€ì§€ë„ ê³„ì‚°
	    itemsets = {}
	    for col in data.columns:
	        support = data[col].sum() / len(data)
	        if support >= min_support:
	            itemsets[frozenset([col])] = support
	    
	    # ë‘ ê°œ ì´ìƒì˜ í•­ëª© ì§‘í•©ì— ëŒ€í•´ ì§€ì§€ë„ ê³„ì‚°
	    for length in range(2, len(data.columns) + 1):
	        for comb in combinations(data.columns, length):
	            comb_set = frozenset(comb)
	            support = (data[list(comb)].sum(axis=1) == length).mean()
	            if support >= min_support:
	                itemsets[comb_set] = support
	
	    return itemsets
	
	# Eclat ì•Œê³ ë¦¬ì¦˜ ì ìš©í•˜ì—¬ ë¹ˆë°œ í•­ëª© ì§‘í•© ìƒì„±
	frequent_itemsets = eclat(df, min_support=0.4)
	
	# ë¹ˆë°œ í•­ëª© ì§‘í•©ì—ì„œ ì—°ê´€ ê·œì¹™ ê³„ì‚°
	rules = []
	for itemset, support in frequent_itemsets.items():
	    if len(itemset) > 1:
	        for antecedent in combinations(itemset, len(itemset) - 1):
	            antecedent = frozenset(antecedent)
	            consequent = itemset - antecedent
	            
	            # ì‹ ë¢°ë„ ê³„ì‚°
	            antecedent_support = frequent_itemsets[antecedent]
	            confidence = support / antecedent_support
	            
	            # í–¥ìƒë„ ê³„ì‚°
	            consequent_support = frequent_itemsets[consequent]
	            lift = confidence / consequent_support
	            
	            # ê·œì¹™ ì €ì¥
	            rules.append({
	                'antecedents': antecedent,
	                'consequents': consequent,
	                'support': support,
	                'confidence': confidence,
	                'lift': lift
	            })
	
	# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
	rules_df = pd.DataFrame(rules)
	print("Association Rules (Eclat):")
	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
    
![](./images/1-3.png)
<br>

# [4] Multi-level Association Rules
â–£ ì •ì˜: Aprioriì™€ FP-Growth í™•ì¥ë²„ì „ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ì„ ê³„ì¸µì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë‹¤ì¤‘ ìˆ˜ì¤€ì—ì„œ ê·œì¹™ì„ ìƒì„±í•˜ëŠ” ë°©ì‹<br>
â–£ í•„ìš”ì„±: ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°ì— ì í•©<br>
â–£ ì¥ì : ë” ì •êµí•œ ê·œì¹™ì„ ìƒì„±<br>
â–£ ë‹¨ì : ë³µì¡ì„±ì´ ì¦ê°€í•˜ë©°, í•´ì„ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼: ì „ììƒê±°ë˜, ì¶”ì²œ ì‹œìŠ¤í…œ, ë§ˆì¼€íŒ… ë¶„ì„<br>

	import pandas as pd
	from mlxtend.frequent_patterns import apriori
	import matplotlib.pyplot as plt
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„± (Multi-level êµ¬ì¡°)
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'Dairy_Milk': [1, 1, 0, 1, 0],
	    'Bakery_Bread': [1, 1, 1, 0, 1],
	    'Bakery_Butter': [0, 1, 1, 1, 1]
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ìƒìœ„ ê³„ì¸µ ë°ì´í„° ìƒì„±
	df['Dairy'] = df['Dairy_Milk']
	df['Bakery'] = df[['Bakery_Bread', 'Bakery_Butter']].max(axis=1)
	
	# ì›ë³¸ ë°ì´í„° ì‹œê°í™”
	item_counts = df[['Dairy_Milk', 'Bakery_Bread', 'Bakery_Butter']].sum()
	item_counts.plot(kind='bar', color='purple')
	plt.title('Item Frequency (Multi-level Association Rules)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# ìƒìœ„ ê³„ì¸µì—ì„œ apriori ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets_upper = apriori(df[['Dairy', 'Bakery']], min_support=0.4, use_colnames=True)
	frequent_itemsets_upper['length'] = frequent_itemsets_upper['itemsets'].apply(lambda x: len(x))
	
	# í•˜ìœ„ ê³„ì¸µì—ì„œ apriori ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets_lower = apriori(df[['Dairy_Milk', 'Bakery_Bread', 'Bakery_Butter']], min_support=0.4, use_colnames=True)
	frequent_itemsets_lower['length'] = frequent_itemsets_lower['itemsets'].apply(lambda x: len(x))
	
	# ì—°ê´€ ê·œì¹™ ìˆ˜ë™ ê³„ì‚°
	def generate_rules(frequent_itemsets):
	    rules = []
	    for itemset in frequent_itemsets['itemsets']:
	        if len(itemset) > 1:
	            for antecedent in combinations(itemset, len(itemset) - 1):
	                antecedent = frozenset(antecedent)
	                consequent = itemset - antecedent
	                
	                # ì§€ì§€ë„ ê³„ì‚°
	                support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	                
	                # ì‹ ë¢°ë„ ê³„ì‚°
	                antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	                confidence = support / antecedent_support
	                
	                # í–¥ìƒë„ ê³„ì‚°
	                consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	                lift = confidence / consequent_support
	                
	                # ê·œì¹™ ì €ì¥
	                rules.append({
	                    'antecedents': antecedent,
	                    'consequents': consequent,
	                    'support': support,
	                    'confidence': confidence,
	                    'lift': lift
	                })
	    return rules
	
	# ìƒìœ„ ê³„ì¸µ ì—°ê´€ ê·œì¹™ ìƒì„±
	rules_upper = generate_rules(frequent_itemsets_upper)
	rules_df_upper = pd.DataFrame(rules_upper)
	print("Association Rules (Upper Level):")
	print(rules_df_upper[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
	
	# í•˜ìœ„ ê³„ì¸µ ì—°ê´€ ê·œì¹™ ìƒì„±
	rules_lower = generate_rules(frequent_itemsets_lower)
	rules_df_lower = pd.DataFrame(rules_lower)
	print("\nAssociation Rules (Lower Level):")
	print(rules_df_lower[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

![](./images/1-4.png)
<br>

# [5] Multi-dimensional Association Rules
â–£ ì •ì˜: ì—¬ëŸ¬ ì†ì„±ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ì°¨ì›ì˜ ê·œì¹™ì„ ìƒì„±<br>
â–£ í•„ìš”ì„±: ì—°ê´€ ê·œì¹™ì„ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì³ ë¶„ì„í•˜ê³ ì í•  ë•Œ ìœ ìš©í•˜ë©°, íŠ¹ì • ì§‘ë‹¨ì— ëŒ€í•œ íŠ¹ì • íŒ¨í„´ì„ íƒì§€í•˜ëŠ” ë° ì í•©<br>
â–£ ì¥ì : ê·œì¹™ì˜ ë²”ìœ„ë¥¼ í™•ì¥í•  ìˆ˜ ìˆì–´ ë” ì„¸ë°€í•œ ê·œì¹™ ë„ì¶œ ê°€ëŠ¥.<br>
â–£ ë‹¨ì : ë³µì¡ì„±ê³¼ í•´ì„ì˜ ì–´ë ¤ì›€<br>
â–£ ì‘ìš©ë¶„ì•¼: ì‚¬ìš©ì ì†ì„± ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ, ë§ˆì¼€íŒ… ì¸í…”ë¦¬ì „ìŠ¤<br>

	import pandas as pd
	from mlxtend.frequent_patterns import apriori
	import matplotlib.pyplot as plt
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„± (Multi-dimensional êµ¬ì¡°)
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	    'Gender_Male': [1, 0, 0, 1, 1],
	    'Gender_Female': [0, 1, 1, 0, 0],
	    'Category_Dairy': [1, 1, 0, 1, 0],
	    'Category_Bakery': [1, 1, 1, 0, 1]
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ì›ë³¸ ë°ì´í„° ì‹œê°í™”
	item_counts = df[['milk', 'bread', 'butter']].sum()
	item_counts.plot(kind='bar', color='orange')
	plt.title('Item Frequency (Multi-dimensional Association Rules)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# apriori ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¹ˆë°œ í•­ëª© ì§‘í•© ìƒì„±
	frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)
	frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
	
	# ì—°ê´€ ê·œì¹™ ìˆ˜ë™ ê³„ì‚°
	def generate_rules(frequent_itemsets):
	    rules = []
	    for itemset in frequent_itemsets['itemsets']:
	        if len(itemset) > 1:
	            for antecedent in combinations(itemset, len(itemset) - 1):
	                antecedent = frozenset(antecedent)
	                consequent = itemset - antecedent
	                
	                # ì§€ì§€ë„ ê³„ì‚°
	                support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	                
	                # ì‹ ë¢°ë„ ê³„ì‚°
	                antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	                confidence = support / antecedent_support
	                
	                # í–¥ìƒë„ ê³„ì‚°
	                consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	                lift = confidence / consequent_support
	                
	                # ê·œì¹™ ì €ì¥
	                rules.append({
	                    'antecedents': antecedent,
	                    'consequents': consequent,
	                    'support': support,
	                    'confidence': confidence,
	                    'lift': lift
	                })
	    return rules
	
	# Multi-dimensional ì—°ê´€ ê·œì¹™ ìƒì„±
	rules = generate_rules(frequent_itemsets)
	rules_df = pd.DataFrame(rules)
	print("Association Rules (Multi-dimensional):")
	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
	
![](./images/1-5.png)
<br>

# [6] AIS(Artificial Immune System)
â–£ ì •ì˜: ê±°ë˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë¹ˆë²ˆí•œ í•­ëª© ì§‘í•©ì„ ì°¾ëŠ” ì´ˆê¸° ì—°ê´€ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜<br>
â–£ í•„ìš”ì„±: ì´ˆê¸° ì—°ê´€ ê·œì¹™ ì—°êµ¬ì—ì„œ í™œìš©ë˜ì—ˆìœ¼ë‚˜, ì„±ëŠ¥ì˜ í•œê³„ë¡œ í˜„ì¬ëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ<br>
â–£ ì¥ì : ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì´í•´í•˜ê¸° ì‰½ê³ , ë³µì¡í•œ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ì´ìƒ íŒ¨í„´ì„ ê°ì§€í•˜ëŠ” ë° ê°•ì <br>
â–£ ë‹¨ì : ë¹„íš¨ìœ¨ì ì´ë©°, Apriorië³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§<br>
â–£ ì‘ìš©ë¶„ì•¼: ì´ˆê¸° ì—°ê´€ ê·œì¹™ ì—°êµ¬, ì´ìƒíƒì§€<br>

	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='blue')
	plt.title('Item Frequency')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# AIS ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
	population_size = 10         # ì´ˆê¸° í•­ì²´(í•´) ê°œìˆ˜
	num_generations = 10         # ë°˜ë³µí•  ì„¸ëŒ€ ìˆ˜
	mutation_rate = 0.1          # ëŒì—°ë³€ì´ìœ¨
	selection_rate = 0.5         # ì„ íƒë¥ 
	
	# ì í•©ë„ í•¨ìˆ˜ ì •ì˜ (ì˜ˆ: milk, bread, butter êµ¬ë§¤ ì¡°í•©ì˜ ì ìˆ˜í™”)
	def fitness(antibody):
	    # í•­ì²´ì˜ ì í•©ë„ë¥¼ milk, bread, butterì˜ í•©ìœ¼ë¡œ ì •ì˜
	    return antibody['milk'] * 1 + antibody['bread'] * 1.2 + antibody['butter'] * 0.8
	
	# ì´ˆê¸° í•­ì²´(í•´) ìƒì„± (milk, bread, butter êµ¬ë§¤ ìœ ë¬´ì— ë”°ë¼ í•­ì²´ êµ¬ì„±)
	population = [df.sample(1, replace=True).squeeze() for _ in range(population_size)]
	fitness_scores = np.array([fitness(antibody) for antibody in population])
	
	# AIS ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
	for generation in range(num_generations):
	    # ì„ íƒ: ìƒìœ„ selection_rate ë¹„ìœ¨ì˜ í•­ì²´ë§Œ ìœ ì§€
	    num_selected = int(selection_rate * population_size)
	    selected_indices = np.argsort(fitness_scores)[-num_selected:]
	    selected_population = [population[i] for i in selected_indices]
	    
	    # ë³µì œ ë° ëŒì—°ë³€ì´
	    offspring = []
	    for antibody in selected_population:
	        # ë³µì œ
	        cloned = antibody.copy()
	        # ëŒì—°ë³€ì´ ì ìš©
	        for item in ['milk', 'bread', 'butter']:
	            if np.random.rand() < mutation_rate:
	                cloned[item] = 1 - cloned[item]  # 0ì´ë©´ 1ë¡œ, 1ì´ë©´ 0ìœ¼ë¡œ ë³€ê²½
	        offspring.append(cloned)
	    
	    # ìƒˆ ì„¸ëŒ€ ìƒì„±
	    population = offspring
	    fitness_scores = np.array([fitness(antibody) for antibody in population])
	
	# ìµœì ì˜ í•­ì²´ ì„ íƒ
	best_solution = population[np.argmax(fitness_scores)]
	best_fitness = fitness(best_solution)
	
	# ê²°ê³¼ ì¶œë ¥
	print("ìµœì ì˜ í•´:", best_solution)
	print("ìµœì ì˜ ì í•©ë„:", best_fitness)
	
	# í‰ê°€ ê²°ê³¼ (ìœ ì‚¬ ì§€ì§€ë„, ì‹ ë¢°ë„, í–¥ìƒë„)
	support = sum(best_solution) / len(best_solution)
	confidence = best_fitness / max(fitness_scores)
	lift = confidence / (support if support != 0 else 1)
	
	print("\ní‰ê°€ ê²°ê³¼:")
	print(f"ì§€ì§€ë„(Support): {support}")
	print(f"ì‹ ë¢°ë„(Confidence): {confidence}")
	print(f"í–¥ìƒë„(Lift): {lift}")
	
	# ìµœì¢… í•­ì²´ ì í•©ë„ ë¶„í¬ ì‹œê°í™”
	plt.plot(range(len(fitness_scores)), fitness_scores, 'bo')
	plt.xlabel("Antibody Index")
	plt.ylabel("Fitness Score")
	plt.title("AIS Antibody Fitness Distribution")
	plt.show()

![](./images/1-6.png)
<br>

---
## [ì—°ê´€ ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]

**â–£ ì§€ì§€ë„(Support):** íŠ¹ì • í•­ëª© ì§‘í•©ì´ ì „ì²´ ê±°ë˜ì—ì„œ ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.<br>
Support(A) = (ê±°ë˜ì—ì„œ Aê°€ ë°œìƒí•œ íšŸìˆ˜)/(ì „ì²´ ê±°ë˜ ìˆ˜)<br>

**â–£ ì‹ ë¢°ë„(Confidence):** Aê°€ ì£¼ì–´ì¡Œì„ ë•Œ Bê°€ ë°œìƒí•  í™•ë¥ <br>
Confidence(A â‡’ B) = Support(A âˆ© B)/Support(A)<br>

**â–£ í–¥ìƒë„(Lift):** Aì™€ Bê°€ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê²½ìš°ì— ë¹„í•´ Aê°€ ë°œìƒí–ˆì„ ë•Œ Bê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 1ì´ë©´ ë‘ í•­ëª©ì´ ë…ë¦½ì , 1ë³´ë‹¤ í¬ë©´ ì–‘ì˜ ìƒê´€ê´€ê³„, 1ë³´ë‹¤ ì‘ìœ¼ë©´ ìŒì˜ ìƒê´€ê´€ê³„<br>
Lift(A â‡’ B) = Confidence(A â‡’ B)/Support(B)<br>

**â–£ ë ˆë²„ë¦¬ì§€(Leverage):** Aì™€ Bì˜ ê²°í•© ë¹ˆë„ê°€ ë‘ í•­ëª©ì´ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¹ˆë„ì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤. 0ì´ë©´ ë‘ í•­ëª©ì´ ë…ë¦½ì <br>
Leverage(A â‡’ B) =  Support(A âˆ© B) - (Support(A) Ã— Support(B))<br>

**â–£ Conviction(í™•ì‹ ë„):** Aê°€ ë°œìƒí•  ë•Œ Bê°€ ë°œìƒí•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë…ë¦½ì ì¸ ê²½ìš°ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“œëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 1ì— ê°€ê¹Œìš°ë©´ Aì™€ BëŠ” ì„œë¡œ ë…ë¦½ì <br>
Conviction(A â‡’ B) = (1-Support(B))/(1-Confidence(A â‡’ B))<br>

**â–£ ì»¬ëŸ¬ë ˆì´ì…˜ ê³„ìˆ˜(Correlation Coefficient):** 0ì— ê°€ê¹Œìš°ë©´ ë‘ í•­ëª© ê°„ì— ìƒê´€ê´€ê³„ê°€ ì—†ê³ , ì–‘ìˆ˜ë‚˜ ìŒìˆ˜ë¡œ ê°ˆìˆ˜ë¡ ìƒê´€ê´€ê³„ê°€ ê°•í•˜ë‹¤.<br>

<br>

---
**ì°¨ì›ì¶•ì†Œì˜ í•„ìš”ì„± :** ë°ì´í„°ì— í¬í•¨ëœ ë…¸ì´ì¦ˆ(noise)ë¥¼ ì œê±°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ì°¨ì›ì¶•ì†Œ(dimension reduction)ì´ë‹¤. ì°¨ì›ì¶•ì†ŒëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì˜ ì •ë³´ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. ì°¨ì›ì¶•ì†Œë¥¼ í†µí•´ ì°¨ì›ì´ ëŠ˜ì–´ë‚  ìˆ˜ë¡ í•„ìš”í•œ ë°ì´í„°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë§ì•„ì§€ëŠ” ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality) ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ì§€ë„í•™ìŠµì˜ ëŒ€í‘œì ì¸ ì°¨ì›ì¶•ì†Œ ë°©ë²•ì€ ì„ í˜•íŒë³„ë¶„ì„(Linear Discriminant Analysis)ì´ ìˆê³ , ë¹„ì§€ë„í•™ìŠµì˜ ëŒ€í‘œì ì¸ ì°¨ì›ì¶•ì†Œ ë°©ë²•ì€ ì£¼ì„±ë¶„ë¶„ì„(Principal Component Anaysis)ì´ ìˆë‹¤.<br>

# [1] PCA(Principal Component Analysis)
![](./images/PCA.png)
<br>
â–£ ì •ì˜ : ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ë°ì´í„°ì˜ ì£¼ìš” ì„±ë¶„(ì£¼ì„±ë¶„)ì„ ì°¾ê¸° ìœ„í•´ ì„ í˜• ë³€í™˜ì„ ì ìš©í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜. ì—¬ëŸ¬ íŠ¹ì„±(Feature) ë³€ìˆ˜ë“¤ì´ í†µê³„ì ìœ¼ë¡œ ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ë„ë¡ ë³€í™˜ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²•. ì£¼ì„±ë¶„ë¶„ì„ì€ ì˜¤ì§ ê³µë¶„ì‚°í–‰ë ¬(convariance matrix) $\sum$ ì—ë§Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.<br> 
â–£ ì¥ì  : ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ, ë°ì´í„°ì˜ ì¡ìŒì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°, ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‰½ê²Œ ì´í•´í•˜ê³  ë¶„ì„<br>
â–£ ë‹¨ì  : ì„ í˜• ë³€í™˜ë§Œì„ ê°€ì •(ì»¤ë„PCA ê°™ì€ ë¹„ì„ í˜• ë³€í˜• ê¸°ë²•ì´ í•„ìš”), ê° ì£¼ì„±ë¶„ì´ ì›ë˜ ë°ì´í„°ì˜ ì–´ë–¤ íŠ¹ì„±ì„ ì„¤ëª…í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ í•´ì„í•˜ê¸° ì–´ë µë‹¤. ë¶„ì‚°ì— ì¤‘ìš”í•œ ì •ë³´ê°€ ìˆì„ ê²½ìš° ì´ë¥¼ ë†“ì¹  ìˆ˜ ìˆë‹¤.<br>
â–£ ì‘ìš©ë¶„ì•¼ : ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2D ë˜ëŠ” 3Dë¡œ ë³€í™˜í•´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”, ì¡ìŒ ì œê±°, ì–¼êµ´ ì¸ì‹ì—ì„œ ì–¼êµ´ ì´ë¯¸ì§€ì˜ ì£¼ìš” íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì–¼êµ´ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë¥˜<br>
â–£ ëª¨ë¸ì‹ : ì£¼ì„±ë¶„ì€ ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°<br>
ë°ì´í„° í–‰ë ¬ ğ‘‹ì˜ ê³µë¶„ì‚° í–‰ë ¬ ğ¶ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ í†µí•´ ìƒˆë¡œìš´ ì£¼ì„±ë¶„ì„ ê³„ì‚° : $C=\frac{1}{n-1}X^TX$<br>
ê³ ìœ ê°’ ë¶„í•´(v_iëŠ” ië²ˆì§¸ ê³ ìœ ë²¡í„°, \lambda_iëŠ” ië²ˆì§¸ ê³ ìœ ê°’) : $Cv_i = \lambda_iv_i$<br>
â–£ PCAì˜ ì ˆì°¨ : ë¶„ì‚°ì˜ ìµœëŒ€í™”: ì£¼ì„±ë¶„ì€ ë°ì´í„°ì˜ ë¶„ì‚°(ë³€ë™ì„±)ì„ ìµœëŒ€í•œ ë§ì´ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ë‹¤. ë°ì´í„°ì˜ ì£¼ìš”í•œ ë³€ë™ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì¶•ì„ ë¨¼ì € ì°¾ê³ , ê·¸ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ íˆ¬ì˜í•œë‹¤. ì§êµì„±: ê° ì£¼ì„±ë¶„ì€ ì„œë¡œ ì§êµ(orthogonal)í•´ì•¼ í•˜ëŠ”ë° ì´ëŠ” ê° ì£¼ì„±ë¶„ì´ ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ëŠ” ë…ë¦½ì ì¸ ì¶•ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.<br>
(1) ë°ì´í„° í‘œì¤€í™” : PCAë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì„ ë§ì¶”ê¸° ìœ„í•´ ê° ë³€ìˆ˜ì˜ í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“¤ê³  ë¶„ì‚°ì„ 1ë¡œ ë§ì¶”ëŠ” z-ì ìˆ˜ ì •ê·œí™” ê³¼ì •<br>
(2) ê³µë¶„ì‚° í–‰ë ¬ê³„ì‚° : ê³µë¶„ì‚°(ë‘ ë³€ìˆ˜ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„) í–‰ë ¬ ê³„ì‚°ì„ í†µí•´ ë°ì´í„°ì˜ ë¶„ì‚°ì´ ì–´ë–»ê²Œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ í™•ì¸<br>
  $Cov(X,Y)=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})$<br>
(3) ê³ ìœ ê°’ ë¶„í•´(Eigenvalue Decomposition) : ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ë²¡í„°(eigenvector)ëŠ” PCAì˜ ì£¼ì„±ë¶„ì— í•´ë‹¹, ê³ ìœ ê°’(eigenvalue)ì€ ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì˜ ì–‘ì„ ë‚˜íƒ€ëƒ„<br>
(4) ì£¼ì„±ë¶„ ì„ íƒ: ê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ì£¼ì„±ë¶„ì„ ì„ íƒ(ê°€ì¥ í° ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ ë²¡í„°ê°€ ì œ1ì£¼ì„±ë¶„, ê·¸ë‹¤ìŒ ê³ ìœ ê°’ì´ ì œ2ì£¼ì„±ë¶„ : ê³ ìœ ê°’ì´ í° ì£¼ì„±ë¶„ì¼ìˆ˜ë¡ ë°ì´í„°ì˜ ë¶„ì‚°ì„¤ëª…ë ¬ì´ ë†’ë‹¤)<br>
(5) ì°¨ì› ì¶•ì†Œ: ì„ íƒëœ ì£¼ì„±ë¶„ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜. ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì„±(ë¶„ì‚°)ì„ ìœ ì§€í•˜ë©´ì„œ ë¶ˆí•„ìš”í•œ ì°¨ì›ì„ ì œê±°í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œ<br>
 
<br>

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # PCA ì ìš©
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data.target)
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.title("PCA on Iris Dataset")
    plt.colorbar()
    plt.show()

    # ë¶„ì‚° ìœ ì§€ìœ¨ ì¶œë ¥
    print("Explained Variance Ratio:", pca.explained_variance_ratio_)
    print("Total Variance Retained:", sum(pca.explained_variance_ratio_))

![](./images/PCA.png)
<br>

# [2] t-SNE(t-distributed Stochastic Neighbor Embedding)
â–£ ì •ì˜: ê³ ì°¨ì› ë°ì´í„°ì˜ êµ­ì†Œ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ì—¬ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì˜ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ìœ ì§€í•œ ì±„ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë°ì´í„° ê°„ì˜ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì¥ì  : ê³ ì°¨ì› ë°ì´í„°ì˜ êµ°ì§‘ êµ¬ì¡°ë¥¼ ì˜ ë°˜ì˜í•˜ì—¬ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ ì˜ ë“œëŸ¬ë‚´ê³ , ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ê°€ì§„ ë°ì´í„°ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™<br>
â–£ ë‹¨ì  : ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê³„ì‚° ì‹œê°„ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³ , ì´ˆê¸° ë§¤ê°œë³€ìˆ˜(ì˜ˆ: Ïƒ ê°’ ë° í•™ìŠµë¥ )ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘<br> 
â–£ ì‘ìš© ë¶„ì•¼ : ì´ë¯¸ì§€ ë°ì´í„°, í…ìŠ¤íŠ¸ ë°ì´í„°, ìœ ì „ì í‘œí˜„ ë°ì´í„° ë“±ì˜ ì‹œê°í™”, í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„, ë°ì´í„° ì „ì²˜ë¦¬, ì‹ ê²½ë§ ëª¨ë¸ì˜ ì¤‘ê°„ ì¶œë ¥ì„ ì‹œê°í™”<br>
â–£ ëª¨ë¸ì‹: ê³ ì°¨ì› ë°ì´í„°ì˜ ìœ ì‚¬ë„ì™€ ì €ì°¨ì› ë°ì´í„°ì˜ ìœ ì‚¬ë„ ë¶„í¬ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì½”ìŠ¤íŠ¸ í•¨ìˆ˜ ğ¾ğ¿(ğ‘âˆ¥ğ‘)ë¥¼ ìµœì†Œí™”<br>

<br>

    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # t-SNE ì ìš©
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=data.target)
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")
    plt.title("t-SNE on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/tSNE.png)
<br>

# [3] UMAP(Uniform Manifold Approximation and Projection)
â–£ ì •ì˜: ë°ì´í„°ì˜ êµ­ì†Œ êµ¬ì¡°ì™€ ì „ì—­ êµ¬ì¡°ë¥¼ ë™ì‹œì— ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„±: ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ì—ì„œ ì‹œê°í™”í•˜ë©´ì„œ ë°ì´í„°ì˜ ì „ì²´ì  ë° êµ­ì†Œì  ê´€ê³„ë¥¼ ë™ì‹œì— ë³´ì¡´í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì¥ì : t-SNEë³´ë‹¤ ê³„ì‚°ì´ ë¹ ë¥´ê³ , ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™, ë°ì´í„°ì˜ ì „ì—­ì  ë° êµ­ì†Œì  êµ¬ì¡°ë¥¼ ë™ì‹œì— ë³´ì¡´<br>
â–£ ë‹¨ì : ì¼ë¶€ ë§¤ê°œë³€ìˆ˜ ì¡°ì •ì´ í•„ìš”í•˜ë©°, ê²°ê³¼ê°€ ë§¤ê°œë³€ìˆ˜ì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œê°í™”, ìƒë¬¼ì •ë³´í•™, í…ìŠ¤íŠ¸ ë¶„ì„ ë“±<br>
â–£ ëª¨ë¸ì‹: ì´ë¡ ì ìœ¼ë¡œëŠ” ë¦¬ë§Œ ê±°ë¦¬ì™€ ì´ˆêµ¬ ë©´ì  ê°œë…ì„ ì´ìš©í•˜ì—¬ ë°ì´í„°ì˜ ê·¼ì ‘ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê³ ì°¨ì›ì—ì„œ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜<br>

    !pip install umap-learn
    import umap
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # UMAP ì ìš©
    umap_model = umap.UMAP(n_components=2, random_state=42)
    X_umap = umap_model.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=data.target)
    plt.xlabel("UMAP Component 1")
    plt.ylabel("UMAP Component 2")
    plt.title("UMAP on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/UMAP.png)
<br>

# [4] SVD(Singular Value Decomposition)
![](./images/SVD.png)
<br>
â–£ ì •ì˜: ì„ì˜ì˜ í–‰ë ¬ì„ ì„¸ ê°œì˜ í–‰ë ¬ë¡œ ë¶„í•´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í–‰ë ¬ì˜ íŠ¹ì´ê°’ê³¼ íŠ¹ì´ë²¡í„°ë¥¼ í†µí•´ í–‰ë ¬ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µí•´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì°¾ê±°ë‚˜ ì••ì¶•í•˜ëŠ” ë° ì‚¬ìš©<br> 
â–£ ì¥ì  : ì •ë°©/ë¹„ì •ë°©/ë¹„ëŒ€ì¹­ í–‰ë ¬ ë“± ì–´ë–¤ í˜•íƒœì˜ í–‰ë ¬ì—ë„ ì ìš© ê°€ëŠ¥, ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ë©´ì„œë„ ì¤‘ìš”í•œ íŒ¨í„´ì„ ìœ ì§€, ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¸¸ ìˆ˜ ìˆìŒ<br>
â–£ ë‹¨ì  : íŠ¹íˆ ë§¤ìš° í° í–‰ë ¬ì˜ ê²½ìš° ê³„ì‚°ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë©°, ë¶„í•´ëœ í–‰ë ¬ë“¤ì´ ì›ë³¸ ë°ì´í„°ì™€ ì§ê´€ì ì¸ ê´€ê³„ë¥¼ ê°€ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì— ê²°ê³¼ë¥¼ í•´ì„í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼ : ë‹¨ì–´-ë¬¸ì„œ í–‰ë ¬ì˜ ì°¨ì› ì¶•ì†Œ, ë°ì´í„° ì••ì¶•, ë…¸ì´ì¦ˆ ì œê±°, ì¶”ì²œ ì‹œìŠ¤í…œ, ì´ë¯¸ì§€ ì••ì¶•<br> 
â–£ ëª¨ë¸ì‹ : $X=UÎ£V^T$<br>
ğ‘‹ëŠ” ğ‘šÃ—ğ‘› í¬ê¸°ì˜ ì›ë³¸ í–‰ë ¬, ğ‘ˆëŠ” ğ‘šÃ—ğ‘š í¬ê¸°ì˜ ì¢Œì¸¡ ì§êµ í–‰ë ¬, Î£ëŠ” ğ‘šÃ—ğ‘› í¬ê¸°ì˜ ëŒ€ê° í–‰ë ¬ë¡œ íŠ¹ì´ê°’ì´ ëŒ€ê° ì›ì†Œë¡œ ë°°ì¹˜, $ğ‘‰^ğ‘‡$ëŠ” ğ‘›Ã—ğ‘› í¬ê¸°ì˜ ìš°ì¸¡ ì§êµ í–‰ë ¬<br>

<br>

    import numpy as np
    from sklearn.decomposition import TruncatedSVD
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # SVD ì ìš©
    svd = TruncatedSVD(n_components=2)
    X_svd = svd.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_svd[:, 0], X_svd[:, 1], c=data.target)
    plt.xlabel("SVD Component 1")
    plt.ylabel("SVD Component 2")
    plt.title("SVD on Iris Dataset")
    plt.colorbar()
    plt.show()

    # ë¶„ì‚° ìœ ì§€ìœ¨ ì¶œë ¥
    print("Explained Variance Ratio:", svd.explained_variance_ratio_)
    print("Total Variance Retained:", sum(svd.explained_variance_ratio_))

![](./images/SVD.png)
<br>

# [5] ICA(Independent Component Analysis)
â–£ ì •ì˜ : ë‹¤ë³€ëŸ‰ ì‹ í˜¸ì—ì„œ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ì„±ë¶„ì„ ì¶”ì¶œí•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•. PCAëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ëŠ” ì¶•ì„ ì°¾ëŠ” ë°˜ë©´, ICAëŠ” ì‹ í˜¸ ê°„ì˜ ë…ë¦½ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ë¶„ì„ ì°¾ëŠ”ë‹¤. ë˜í•œ PCAëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ë°ì´í„°ì˜ ìƒê´€ê´€ê³„ë§Œì„ ì´ìš©í•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ê±°ë‚˜ ì„±ë¶„ì„ ì°¾ëŠ” ë°˜ë©´, ICAëŠ” ì‹ í˜¸ë“¤ ê°„ì˜ ê³ ì°¨ì›ì  í†µê³„ì  ë…ë¦½ì„±ì— ì´ˆì ì„ ë§ì¶”ê¸° ë•Œë¬¸ì— ë” ë³µì¡í•œ êµ¬ì¡°ì˜ ì‹ í˜¸ë¶„ë¦¬ ë¬¸ì œë¥¼ í•´ê²°<br>
â–£ í•„ìš”ì„± : ê´€ì¸¡ëœ ì‹ í˜¸ê°€ ì—¬ëŸ¬ ë…ë¦½ì ì¸ ì›ì²œ ì‹ í˜¸ì˜ í˜¼í•©ìœ¼ë¡œ êµ¬ì„±ë  ë•Œ ê° ë…ë¦½ì ì¸ ì‹ í˜¸ë¥¼ ë³µì›í•˜ëŠ” ë° í•„ìš”í•˜ë©° íŠ¹íˆ ì‹ í˜¸ ì²˜ë¦¬ ë° ìŒì„± ë¶„ë¦¬ì— ìœ ìš©<br>
â–£ ì‘ìš©ë¶„ì•¼ : ë‡ŒíŒŒ(EEG) ì‹ í˜¸ ë¶„ì„, ìŒì„± ì‹ í˜¸ ë¶„ë¦¬, ì´ë¯¸ì§€ ì²˜ë¦¬<br>
â–£ ì¥ì  : í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ì‹ í˜¸ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì‹ í˜¸ ì²˜ë¦¬, ì´ë¯¸ì§€ ë¶„í• , ìŒì„± ë¶„ë¦¬ ë“±ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜<br>
â–£ ë‹¨ì  : ì¡ìŒì— ë¯¼ê°í•˜ê³ , ì›ë˜ ì‹ í˜¸ì˜ ìˆœì„œë¥¼ ë³´ì¥í•˜ì§€ ì•Šìœ¼ë©°, ì„±ë¶„ì˜ í¬ê¸°ë„ ì›ë˜ ì‹ í˜¸ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ì¶”ê°€ì ì¸ í›„ì²˜ë¦¬ê°€ í•„ìš”<br>
â–£ ëª¨ë¸ì‹ : ê´€ì¸¡ ë°ì´í„° ğ‘‹=ğ´ğ‘†ì—ì„œ ğ´ëŠ” í˜¼í•© í–‰ë ¬, ğ‘†ëŠ” ë…ë¦½ ì„±ë¶„ í–‰ë ¬ì´ë©°, ğ´ì™€ ğ‘†ë¥¼ ì¶”ì •í•˜ì—¬ ğ‘†ë¥¼ ì¶”ì¶œ<br>
â–£ ì•Œê³ ë¦¬ì¦˜ : ë¹„ì„ í˜•ì„±ì„ ì´ìš©í•´ ë…ë¦½ ì„±ë¶„ì„ ë¹ ë¥´ê²Œ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ ì‹ í˜¸ì˜ ë¹„ì •ê·œì„±ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì„±ë¶„ì„ ì¶”ì •í•˜ëŠ” Fast ICAê³¼ ì •ë³´ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°©ë²•ìœ¼ë¡œ, ê´€ì¸¡ëœ ë°ì´í„°ì—ì„œ ì •ë³´ëŸ‰ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë…ë¦½ ì„±ë¶„ì„ ì¶”ì •í•˜ëŠ” Infomax ICA<br>

<br>

    import numpy as np
    from sklearn.decomposition import FastICA
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # ICA ì ìš©
    ica = FastICA(n_components=2, random_state=42)
    X_ica = ica.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_ica[:, 0], X_ica[:, 1], c=data.target)
    plt.xlabel("ICA Component 1")
    plt.ylabel("ICA Component 2")
    plt.title("ICA on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/ICA.png)
<br> 

# [6] LDA(Linear Discriminant Analysis)
![](./images/LDA.png)
<br>
â–£ ì •ì˜: í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ê³  í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ìœ¼ë¡œ ì£¼ë¡œ ì§€ë„ í•™ìŠµì—ì„œ ì‚¬ìš©<br>
â–£ í•„ìš”ì„±: í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë¶„ë¥˜ ë¬¸ì œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì : í´ë˜ìŠ¤ ë¶„ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ì—¬ ë¶„ë¥˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìœ¼ë©°, ì„ í˜• ë³€í™˜ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œ<br>
â–£ ë‹¨ì : ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ê²½ìš° ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë©°, í´ë˜ìŠ¤ ê°„ ë¶„í¬ê°€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ë” íš¨ê³¼ì <br>
â–£ ì‘ìš©ë¶„ì•¼: ì–¼êµ´ ì¸ì‹, ì´ë¯¸ì§€ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë“±<br>
â–£ ëª¨ë¸ì‹: ë‘ í´ë˜ìŠ¤ ê°„ì˜ ë¶„ì‚° ë¹„ìœ¨ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ íˆ¬ì˜<br>

    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data
    y = data.target

    # LDA ì ìš©
    lda = LinearDiscriminantAnalysis(n_components=2)
    X_lda = lda.fit_transform(X, y)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)
    plt.xlabel("LDA Component 1")
    plt.ylabel("LDA Component 2")
    plt.title("LDA on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/LDA.png)
<br><br>
![](./images/PCA_LDA.png)
<br>
https://nirpyresearch.com/classification-nir-spectra-linear-discriminant-analysis-python/
<br>

# [7] Isomap
â–£ ì •ì˜: ë°ì´í„°ì˜ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ì—¬ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•<br>
â–£ í•„ìš”ì„±: ë¹„ì„ í˜•ì ì¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ì €ì°¨ì›ì—ì„œë„ ìœ ì§€í•˜ë©° ì‹œê°í™”í•  ë•Œ ìœ ìš©<br>
â–£ ì¥ì : ê³ ì°¨ì› ë°ì´í„°ì˜ ë§¤ë‹ˆí´ë“œ(ì €ì°¨ì› ë‹¤ì–‘ì²´) êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ë©°, êµ­ì†Œì ì¸ ê±°ë¦¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ìœ ì§€<br>
â–£ ë‹¨ì : ë°ì´í„°ê°€ ê³ ì°¨ì›ì—ì„œ ë§¤ë‹ˆí´ë“œ êµ¬ì¡°ë¥¼ í˜•ì„±í•˜ì§€ ì•ŠëŠ” ê²½ìš° íš¨ê³¼ì ì´ì§€ ì•Šìœ¼ë©°, ê³„ì‚° ë¹„ìš©ì´ ë†’ì•„ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ëŠ” ë¶€ì í•©<br>
â–£ ì‘ìš©ë¶„ì•¼: ì‹œê°í™”, ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„, ìƒë¬¼ì •ë³´í•™<br>
â–£ ëª¨ë¸ì‹: ê·¼ì ‘ ê·¸ë˜í”„ì™€ ë‹¤ì°¨ì› ì²™ë„ë¥¼ ê²°í•©í•˜ì—¬ ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ë³´ì¡´<br>

    from sklearn.manifold import Isomap
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # Isomap ì ìš©
    isomap = Isomap(n_components=2)
    X_isomap = isomap.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=data.target)
    plt.xlabel("Isomap Component 1")
    plt.ylabel("Isomap Component 2")
    plt.title("Isomap on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/ISO.png)
<br>

# [8] MDS(Multidimensional Scaling)
â–£ ì •ì˜: MDSëŠ” ê³ ì°¨ì› ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ë³´ì¡´í•˜ë©° ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²•<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì˜ ìœ ì‚¬ì„± ë˜ëŠ” ê±°ë¦¬ ì •ë³´ë¥¼ ì €ì°¨ì›ì—ì„œë„ ìœ ì§€í•˜ì—¬ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì¥ì : ê±°ë¦¬ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë¯€ë¡œ ë°ì´í„°ì˜ ê¸°í•˜í•™ì  ê´€ê³„ë¥¼ ì˜ ìœ ì§€í•˜ë©°, ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ì¼ë¶€ ë³´ì¡´<br>
â–£ ë‹¨ì : ê³„ì‚° ë¹„ìš©ì´ ë†’ê³ , ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ëŠ” ì í•©í•˜ì§€ ì•Šìœ¼ë©°, ì´ˆê¸°í™”ì— ë¯¼ê°í•˜ì—¬ ê²°ê³¼ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼: ì‹¬ë¦¬í•™, ìƒë¬¼ì •ë³´í•™, ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ ë“±<br>
â–£ ëª¨ë¸ì‹: ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ í–‰ë ¬ì„ ìœ ì§€í•˜ë©° ì €ì°¨ì›ì—ì„œ êµ¬ì„±<br>

    from sklearn.manifold import MDS
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # MDS ì ìš©
    mds = MDS(n_components=2, random_state=42)
    X_mds = mds.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_mds[:, 0], X_mds[:, 1], c=data.target)
    plt.xlabel("MDS Component 1")
    plt.ylabel("MDS Component 2")
    plt.title("MDS on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/MDS.png)
<br>


# [9] SOM(Self-Organizing Maps)
â–£ ì •ì˜ : ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›(ì¼ë°˜ì ìœ¼ë¡œ 2ì°¨ì›) ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ëŠ” ë° ì‚¬ìš©. PCAëŠ” ì„ í˜• ë³€í™˜ì„ í†µí•´ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•˜ì§€ë§Œ, SOMì€ ë¹„ì„ í˜• ë³€í™˜ì„ ì‚¬ìš©í•˜ì—¬ ë” ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìœ¼ë©°, k-í‰ê· ì€ ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì„ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ êµ°ì§‘í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°˜ë©´, SOMì€ ë‰´ëŸ°ì´ ê²©ì í˜•íƒœë¡œ ì¡°ì§ë˜ì–´ ìˆì–´ ë” ì§ê´€ì ì¸ ì‹œê°í™”ê°€ ê°€ëŠ¥<br> 
â–£ ì ˆì°¨
(1) ì´ˆê¸°í™”: SOMì˜ ê° ë‰´ëŸ°ì— ì„ì˜ì˜ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ í• ë‹¹(ì´ ê°€ì¤‘ì¹˜ ë²¡í„°ëŠ” ì…ë ¥ ë°ì´í„°ì™€ ê°™ì€ ì°¨ì›)<br>
(2) ì…ë ¥ ë°ì´í„° ì„ íƒ: í•™ìŠµ ê³¼ì •ì—ì„œ ì…ë ¥ ë°ì´í„° ë²¡í„° í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ<br>
(3) ìŠ¹ì ë‰´ëŸ°(BMU, Best Matching Unit) ì°¾ê¸°: SOMì˜ ëª¨ë“  ë‰´ëŸ° ì¤‘ì—ì„œ í˜„ì¬ ì…ë ¥ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ(ê°€ì¤‘ì¹˜ ë²¡í„° ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ì¸¡ì •) ë‰´ëŸ°ì„ ì°¾ëŠ” ê²½ìŸ í•™ìŠµì˜ í•µì‹¬ ë‹¨ê³„<br>
(4) ê°€ì¤‘ì¹˜ ë²¡í„° ê°±ì‹ : ì„ íƒëœ ìŠ¹ì ë‰´ëŸ°ê³¼ ê·¸ ì£¼ë³€ ì´ì›ƒ ë‰´ëŸ°ë“¤ì˜ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ì¡°ì •í•œë‹¤. ì´ë•Œ, ê°€ì¤‘ì¹˜ ë²¡í„°ëŠ” ì…ë ¥ ë°ì´í„°ì— ë” ê°€ê¹ê²Œ ì´ë™<br> 
â–£ ì¥ì  : ë°ì´í„°ì— ëŒ€í•œ ì‚¬ì „ ì •ë³´ê°€ ì—†ì–´ë„ ìœ ìš©í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥, êµ°ì§‘ì˜ ë¶„í¬ë‚˜ ë°ì´í„°ì˜ ê²½í–¥ì„±ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´, ì…ë ¥ ë°ì´í„°ì˜ ì´ì›ƒ ê´€ê³„ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ë¯€ë¡œ ì›ë˜ ë°ì´í„°ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ ìœ ì§€<br> 
â–£ ë‹¨ì  : í•™ìŠµë¥ ê³¼ ì´ì›ƒ í¬ê¸° ë“± ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë¥¼ ì ì ˆíˆ ì„¤ì •í•´ì•¼ í•˜ë©°, ëŒ€ê·œëª¨ ë°ì´í„° í•™ìŠµì— ë¹„íš¨ìœ¨ì , ë³€í™˜ëœ ë§µì„ í•´ì„í•˜ëŠ” ê²ƒì´ PCA ë“±ì˜ ì„ í˜• ë³€í™˜ë³´ë‹¤ ë” ì–´ë ¤ì›€<br> 
â–£ ì‘ìš©ë¶„ì•¼ : ì´ë¯¸ì§€ ë¶„ì„, ë¬¸ì„œ ë¶„ë¥˜, ìŒì„± ì¸ì‹, ìƒë¬¼ì •ë³´í•™<br>
â–£ ëª¨ë¸ì‹ : ë‰´ëŸ°ì˜ ìœ„ì¹˜ ğ‘Ÿì™€ ì…ë ¥ ë²¡í„° ğ‘¥ ê°„ì˜ ê±°ë¦¬ í•¨ìˆ˜ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±(ğœ‚(ğ‘¡)ëŠ” í•™ìŠµë¥ , â„(ğ‘¡)ëŠ” ì´ì›ƒ í•¨ìˆ˜)<br>
$W(t+1)=W(t)+\theta(t)\cdot\eta(t)\cdot(X-W(t))$<br>

    !pip install minisom

    from minisom import MiniSom
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris
    from sklearn.preprocessing import MinMaxScaler

    # ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”
    data = load_iris()
    X = data.data
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    # SOM ì´ˆê¸°í™” ë° í•™ìŠµ
    som = MiniSom(x=10, y=10, input_len=4, sigma=1.0, learning_rate=0.5, random_seed=42)
    som.train_random(X_scaled, 100)  # 100íšŒ ë°˜ë³µ í•™ìŠµ

    # SOM ì‹œê°í™”
    plt.figure(figsize=(10, 10))
    for i, x in enumerate(X_scaled):
        w = som.winner(x)
        plt.text(w[0] + 0.5, w[1] + 0.5, str(data.target[i]),
        color=plt.cm.rainbow(data.target[i] / 2.0),
        fontdict={'weight': 'bold', 'size': 11})

    plt.title("SOM Clustering of Iris Data")
    plt.xlim([0, 10])
    plt.ylim([0, 10])
    plt.grid()
    plt.show()

![](./images/SOM.png)
<br> 

---
## [ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]

**â–£ ì¬êµ¬ì„± ì˜¤ë¥˜(Reconstruction Error) :** ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°ë¥¼ ì›ë³¸ ì°¨ì›ìœ¼ë¡œ ë³µì›í•˜ì—¬ ë³µì›ëœ ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„° ê°„ì˜ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)ë¥¼ í†µí•´ ì¬êµ¬ì„± ì˜¤ë¥˜ë¥¼ ê³„ì‚°

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    from sklearn.metrics import mean_squared_error

    # ë°ì´í„° ë¡œë“œ : Iris ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ ì…ë ¥ ë°ì´í„°(X)ë¥¼ ì¤€ë¹„
    data = load_iris()
    X = data.data  # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)  # ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°

    # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚° : ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°ë¥¼ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›í•˜ê³  ì›ë³¸ ë°ì´í„°ì™€ì˜ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)ë¥¼ ê³„ì‚°
    X_reconstructed = pca.inverse_transform(X_reduced)  # ì°¨ì› ì¶•ì†Œ í›„ ë³µì›ëœ ë°ì´í„°
    reconstruction_error = mean_squared_error(X, X_reconstructed)  # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
    print(f"Reconstruction Error (MSE): {reconstruction_error:.3f}")

<br>

**â–£ ë¶„ì‚° ìœ ì§€ìœ¨(Explained Variance Ratio) :** ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨ì„ í†µí•´ ë°ì´í„°ì˜ ì •ë³´ ì†ì‹¤ ì •ë„ë¥¼ íŒŒì•…

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA

    # ë°ì´í„° ë¡œë“œ : Iris ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ ì…ë ¥ ë°ì´í„°(X)ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
    data = load_iris()
    X = data.data  # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=2)
    pca.fit(X)  # PCA í•™ìŠµ

    # ë¶„ì‚° ìœ ì§€ìœ¨ ê³„ì‚° : ê° ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë¹„ìœ¨ë¡œ í™•ì¸
    explained_variance_ratio = pca.explained_variance_ratio_
    print(f"Explained Variance Ratio per Component: {explained_variance_ratio}")
    print(f"Total Variance Retained: {sum(explained_variance_ratio):.3f}")  # ì „ì²´ ë¶„ì‚° ìœ ì§€ìœ¨

<br>

**â–£ ìƒí˜¸ ì •ë³´ëŸ‰(Mutual Information) :** ì°¨ì› ì¶•ì†Œ ì „í›„ ë°ì´í„°ì˜ ì •ë³´ëŸ‰ì„ ë¹„êµ

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    from sklearn.metrics import adjusted_mutual_info_score, normalized_mutual_info_score

    # ë°ì´í„° ë¡œë“œ : Iris ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ ì…ë ¥ ë°ì´í„°(X)ì™€ ì‹¤ì œ ë ˆì´ë¸”(y_true)ë¥¼ ì¤€ë¹„
    data = load_iris()
    X = data.data         # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)
    y_true = data.target  # ì‹¤ì œ ë ˆì´ë¸” (í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ ì‹œ ì‚¬ìš©)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)  # ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°

    # KMeansë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰ : í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜ì™€ ë§ì¶”ê¸°
    kmeans = KMeans(n_clusters=3, random_state=42)
    y_pred = kmeans.fit_predict(X_reduced)  # í´ëŸ¬ìŠ¤í„°ë§ ì˜ˆì¸¡ ë ˆì´ë¸”

    # 4. ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°
    # (1) Adjusted Mutual Information (AMI) : ì‹¤ì œ ë ˆì´ë¸”(y_true)ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì˜ˆì¸¡ ë ˆì´ë¸”(y_pred) ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •
    ami = adjusted_mutual_info_score(y_true, y_pred)
    print(f"Adjusted Mutual Information (AMI): {ami:.3f}")

    # (2) Normalized Mutual Information (NMI) : ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ê°„ì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ì •ê·œí™”í•˜ì—¬ ì¸¡ì •
    nmi = normalized_mutual_info_score(y_true, y_pred)
    print(f"Normalized Mutual Information (NMI): {nmi:.3f}")

<br>

**â–£ êµ°ì§‘ í‰ê°€ ì§€í‘œ :** ì°¨ì› ì¶•ì†Œ í›„ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ê³  êµ°ì§‘ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ì°¨ì› ì¶•ì†Œì˜ ì„±ëŠ¥ì„ í‰ê°€

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score
    from sklearn.model_selection import train_test_split

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data
    y_true = data.target  # ì‹¤ì œ ë ˆì´ë¸” (í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)

    # KMeansë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    kmeans = KMeans(n_clusters=3, random_state=42)
    y_pred = kmeans.fit_predict(X_reduced)

    # êµ°ì§‘ í‰ê°€ ì§€í‘œ ê³„ì‚°
    # (1) Silhouette Score
    silhouette = silhouette_score(X_reduced, y_pred)
    print(f"Silhouette Score: {silhouette:.3f}")

    # (2) Davies-Bouldin Index (DBI) - í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë¦¬ë˜ê³  ì‘ì§‘ë˜ì–´ ìˆëŠ”ì§€ í‰ê°€(DBIê°€ ë‚®ì„ìˆ˜ë¡ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆì´ ë” ì¢‹ìŒ)
    davies_bouldin = davies_bouldin_score(X_reduced, y_pred)
    print(f"Davies-Bouldin Index: {davies_bouldin:.3f}")

    # (3) Adjusted Rand Index (ARI) - ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ(í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì¼ì¹˜ë„ë¥¼ ì¸¡ì •: 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìœ ì‚¬)
    ari = adjusted_rand_score(y_true, y_pred)
    print(f"Adjusted Rand Index (ARI): {ari:.3f}")

    # (4) Normalized Mutual Information (NMI) - ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ(í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì •ë³´ëŸ‰ì˜ ê³µìœ  ì •ë„ë¥¼ ì¸¡ì •: 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìœ ì‚¬)
    nmi = normalized_mutual_info_score(y_true, y_pred)
    print(f"Normalized Mutual Information (NMI): {nmi:.3f}")

<br>

![](./images/s.png)

<br>
