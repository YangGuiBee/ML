#  11 : 비지도 학습 (Unsupervised Learning, UL) : 연관규칙, 차원축소

---

## 연관 규칙 (Association Rule)
<br>

    [1] FP Growth
    [2] 이클렛 (Eclat)
    [3] 어프라이어리 (Apriori)
  

## 차원 축소 (Dimensionality Reduction)
<br>

    [4] 주성분 분석 (Principal Component Analysis, PCA)
    [5] 독립 성분 분석 (Independent Component Analysis, ICA)
    [6] 자기 조직화 지도 (Self-Organizing Maps, SOM)
    [7] 잠재 의미 분석 (Latent Semantic Analysis, LSA)
    [8] 특이값 분해 (Singular Value Decomposition, SVD)
    [9] 잠재 디리클레 할당 (Latent Dirichlet Allocation, LDA)
    [10]t-distributed Stochastic Neighbor Embedding (t-SNE)

---  

# [1] FP Growth

# [2] 이클렛 (Eclat)

# [3] 어프라이어리 (Apriori)

---

차원축소의 필요성 : 데이터에는 중요한 부분과 중요하지 않은 부분이 존재하는데, 여기서 중요하지 않은 부분이 노이즈(noise)이다. 머신러닝 과정에서는 데이터에서 정보를 언들때 방해가 되는 불필요한 노이즈를 제거하는 것이 중요한데, 이 노이즈를 제거할 때 사용하는 방법이 차원축소(dimension reduction)이다. 차원축소는 주어진 데이터의 정보손실을 최소화하면서 노이즈를 줄이는 것이 핵심이다. 차원축소를 통해 차원이 늘어날 수록 필요한 데이터가 기하급수적으로 많아지는 차원의 저주(curse of dimensionality) 문제를 해결할 수 있다. 지도학습의 대표적인 차원축소 방법은 선형판별분석(Linear Discriminant Analysis)이 있고, 비지도학습의 대표적인 차원축소 방법은 주성분분석(Principal Component Anaysis)이 있다.<br>

# [4] 주성분 분석 (Principal Component Analysis, PCA)

PCA는 여러 특성(Feature) 변수들이 통계적으로 서로 상관관계가 없도록 변환시키는 것이다. 특성이 $p$ 개가 있을 경우, 각 특성의 벡터는 $X_1, X_2, ... X_p$ 라고 나타낼 수 있으며, 주성분분석은 오직 공분산행렬(convariance matrix) $\sum$ 에만 영향을 받는다.<br> 

절차
 - (1) 데이터 표준화 : PCA를 수행하기 전에 데이터의 스케일을 맞춰주는 과정이다. 각 변수의 평균을 0으로 만들고 분산을 1로 맞추는 표준화를 통해 서로 다른 단위를 갖는 데이터들 간의 비교가 가능해지며, 이 과정은 보통 z-점수 정규화라고 한다.<br>
 - (2) 공분산 행렬 계산 : 각 변수 간의 상관관계를 확인하기 위해 공분산 행렬을 계산한다. 공분산은 두 변수가 함께 변하는 정도를 나타내며, 이를 통해 데이터의 분산이 어떻게 다른 변수들과 상호작용하는지 알 수 있다.<br>
 - (3) 고유값 분해(Eigenvalue Decomposition) : 공분산 행렬의 고유값(eigenvalue)과 고유벡터(eigenvector)를 구한다. 고유벡터는 PCA에서 주성분에 해당하며, 고유값은 주성분이 설명하는 분산의 양을 나타낸다.<br>
 - (4) 주성분 선택: 고유값이 큰 순서대로 주성분을 선택한다. 가장 큰 고유값에 해당하는 고유벡터가 제1 주성분이 되고, 그다음 고유값에 해당하는 것이 제2 주성분이 된다. 고유값이 큰 주성분일수록 데이터의 분산을 많이 설명한다.<br>
 - (5) 차원 축소: 선택된 주성분을 사용해 데이터를 저차원으로 투영한다. 데이터의 중요한 특성(분산)을 유지하면서 불필요한 차원을 제거하여 차원을 축소할 수 있다.<br>

응용분야
 - 차원 축소: PCA는 고차원 데이터를 저차원으로 변환하여 계산 비용을 줄이고, 데이터의 시각화 및 분석을 쉽게 할 수 있다.<br>
 - 데이터 시각화: PCA는 고차원 데이터를 2D 또는 3D로 변환해 데이터의 패턴을 직관적으로 시각화하는 데 자주 사용된다.<br>
 - 잡음 제거: 잡음이 포함된 데이터에서 PCA를 사용해 중요한 주성분만을 남기고, 잡음에 해당하는 성분을 제거하여 데이터를 정제할 수 있다.<br>
 - 특징 추출: PCA는 머신 러닝에서 데이터의 주요 특징을 추출하는 데 효과적이다. 예를 들어, 얼굴 인식에서 얼굴 이미지의 주요 특징을 추출하여 얼굴을 효율적으로 분류할 수 있다.<br>
장점
 - 효율적인 차원 축소: 정보 손실을 최소화하면서 고차원 데이터를 저차원으로 축소할 수 있다.<br>
 - 잡음 제거: 데이터의 잡음을 효과적으로 제거하고 중요한 정보만 남길 수 있다.<br>
 - 데이터 시각화: 고차원 데이터를 저차원으로 변환하여 데이터의 구조를 쉽게 이해하고 분석할 수 있다.<br>
한계
 - 선형성 가정: PCA는 선형 변환만을 가정하기 때문에, 데이터가 비선형적인 관계를 가질 때는 효과가 떨어지느데, 이러한 경우에는 커널PCA 같은 비선형 변형 기법이 필요하다.<br>
 - 해석의 어려움: 차원축소 후 결과는 원래 변수의 의미를 잃을 수 있으므로 각 주성분이 원래 데이터의 어떤 특성을 설명하는지 직관적으로 해석하기 어렵다.<br>
 - 분산에만 집중: PCA는 분산을 기준으로 차원을 축소하므로, 분산이 적지만 중요한 정보가 있을 경우 이를 놓칠 수 있다.<br>


# [5] 독립 성분 분석 (Independent Component Analysis, ICA)

PCA는 데이터의 분산을 최대화하는 축을 찾는 반면, ICA는 신호 간의 독립성을 기반으로 성분을 찾는다. 또한 PCA는 가우시안 분포를 가정하고 데이터의 상관관계만을 이용해 차원을 축소하거나 성분을 찾는 반면, ICA는 신호들 간의 고차원적 통계적 독립성에 초점을 맞추기 때문에 더 복잡한 구조의 신호 분리 문제를 해결할 수 있다.<br>

응용분야
 - 뇌파(EEG) 신호 분석: 뇌의 여러 부위에서 발생하는 신호들이 혼합되어 측정된 뇌파를 ICA를 사용하여 개별적인 신경 활동을 분리할 수 있다.<br>
 - 음성 신호 분리: 여러 사람이 동시에 이야기하는 상황에서, 각각의 사람의 목소리를 분리해 내는 문제를 해결하는 데 사용된다.<br>
 - 이미지 처리: 이미지에서 개별적인 패턴을 추출하거나 잡음을 제거하는 데 사용될 수 있다.<br>

알고리즘  
 - FastICA: 가장 널리 사용되는 ICA 알고리즘으로, 비선형성을 이용해 독립 성분을 빠르게 찾는 방법으로 주로 신경망 기반의 고속 수렴 특성을 이용해 구현된다. FastICA는 신호의 비정규성(정규 분포에서 얼마나 벗어나는지)을 최대화하는 방향으로 성분을 찾는데, 이는 신호가 정규 분포에서 멀어질수록 독립적인 성분일 가능성이 크다는 통계적 특성에 기반한다.<br>
 - Infomax ICA: 정보 이론을 기반으로 한 ICA 방법으로, 관측된 데이터에서 정보량을 최대화하는 방식으로 독립 성분을 추정하며, 신경망의 학습 방식과 유사한 방식으로 작동한다.<br>
 
한계
 - 혼합 행렬의 정확성: ICA는 관측 데이터가 독립적인 신호들의 선형 혼합으로 이루어졌다고 가정하지만 이 가정이 항상 맞는 것은 아니다.<br>
 - 잡음에 민감함: 데이터에 잡음이 많이 포함되어 있으면 성분 분리가 어려워질 수 있다.<br>
 - 순서 문제: ICA에서 추출된 독립 성분은 원래 신호의 순서를 보장하지 않으며, 성분의 크기도 원래 신호와 다를 수 있는데 이는 추가적인 후처리가 필요할 수 있음을 의미한다.<br>



 

# [6] 자기 조직화 지도 (Self-Organizing Maps, SOM)

# [7] 잠재 의미 분석 (Latent Semantic Analysis, LSA)

# [8] 특이값 분해 (Singular Value Decomposition, SVD)

# [9] 잠재 디리클레 할당 (Latent Dirichlet Allocation, LDA)

# [10] t-distributed Stochastic Neighbor Embedding (t-SNE)


