#  11 : 비지도 학습(Unsupervised Learning, UL) : 연관규칙, 차원축소

---

## 연관 규칙(Association Rule)
<br>

    [1] FP Growth
    [2] 이클렛(Eclat)
    [3] 어프라이어리(Apriori)
  

## 차원 축소(Dimensionality Reduction)
<br>

    [4] 주성분 분석(Principal Component Analysis, PCA)
    [5] 독립 성분 분석(Independent Component Analysis, ICA)
    [6] 자기 조직화 지도(Self-Organizing Maps, SOM)
    [7] 잠재 의미 분석(Latent Semantic Analysis, LSA)
    [8] 특이값 분해(Singular Value Decomposition, SVD)
    [9] t-distributed Stochastic Neighbor Embedding(t-SNE)

---  

# [1] FP Growth
▣ 정의 : FP-Growth는 Apriori 알고리즘의 대안으로, 빈발 항목 집합을 효율적으로 찾는 방법이다. FP-트리(Frequent Pattern Tree)를 만들어 빈발 항목들을 저장하고, 트리 구조를 사용해 자주 발생하는 패턴을 빠르게 찾는다. Apriori와 달리 매번 후보 집합을 생성하지 않으며, 데이터의 트랜잭션을 직접 탐색하여 빈발 항목 집합을 구한다.<br>
▣ 필요성 : Apriori의 계산 비용 문제를 해결하기 위해 고안된 알고리즘으로 FP-Growth는 트리 구조를 이용하여 대규모 데이터에서도 효율적으로 빈발 항목 집합을 찾을 수 있다.<br>
▣ 장점 : Apriori보다 빠르고 메모리 효율적이며, 후보 집합 생성을 피하므로 계산 비용이 낮다.<br>
▣ 단점 : FP-트리 구조를 구축하는 데 추가 메모리가 필요하며, 트리 구조가 복잡해질 경우 구현 및 이해가 어려울 수 있다.<br>
▣ 응용분야 : 시장 바구니 분석, 로그 데이터 분석, 웹 사이트에서 자주 방문한 페이지 패턴 분석<br>
▣ 모델식 : FP-Growth는 트리 기반의 빈발 항목 탐색 알고리즘으로, 트리에서 공통 패턴을 찾아 빈발 항목 집합을 추출한다.<br>

    from mlxtend.frequent_patterns import fpgrowth

    # FP-Growth 알고리즘을 사용하여 빈발 항목 집합 찾기
    frequent_itemsets_fp = fpgrowth(df, min_support=0.6, use_colnames=True)

    # 연관 규칙 찾기
    rules_fp = association_rules(frequent_itemsets_fp, metric="confidence", min_threshold=0.7)

    print(frequent_itemsets_fp)
    print(rules_fp)
    
<br>

# [2] 이클렛(Eclat)
▣ 정의 : Eclat은 연관 규칙 학습의 또 다른 방법으로, 집합 간의 교집합을 이용하여 빈발 항목 집합을 찾아낸다. Eclat은 트랜잭션 ID(TID) 집합을 사용하여 항목 집합의 지지도를 계산한다. 트랜잭션 간의 공통 항목을 기반으로 빈발 항목을 추출하는 방식이다.<br>
▣ 필요성 : Eclat은 Apriori와 FP-Growth의 대안으로, TID 집합을 활용하여 빈발 항목 집합을 계산한다. 데이터의 수가 많아도 TID 간 교차 계산을 통해 효율적으로 연관 규칙을 도출할 수 있다.<br>
▣ 장점 : 수평적 데이터 구조를 이용하여 트랜잭션 데이터에서 빈발 항목 집합을 빠르게 찾고, 저장 공간을 효율적으로 사용하며, 교차 연산을 통해 빈발 항목을 추출한다.<br>
▣ 단점 : 트랜잭션 ID 집합을 계속 업데이트해야 하므로 메모리 사용이 증가할 수 있으며, 대규모 데이터셋에서는 효율성이 떨어질 수 있다.<br>
▣ 응용분야 : 대규모 데이터에서 빈발 패턴 분석, 웹 클릭 로그 분석, 텍스트 마이닝에서 자주 나타나는 단어 조합 분석<br>
▣ 모델식 : Eclat는 항목 집합의 지지도 계산을 위해 트랜잭션 ID 집합의 교집합을 사용하며 빈발 항목 집합의 지지도를 계산할 때 교집합을 통해 빈발 항목을 찾아낸다.<br>

Eclat은 일반적으로 FP-Growth와 유사하게 작동하며, 이클렛 알고리즘은 아래와 같은 기본 TID 기반 교차 연산 방식으로 구현된다.br>

    from itertools import combinations

    # 데이터 집합에서 항목별로 트랜잭션 ID 집합 생성
    def tid_lists(df):
        tid_dict = {}
        for col in df.columns:
            tid_dict[col] = set(df.index[df[col] == 1])
        return tid_dict

    # 빈발 항목 집합을 찾는 Eclat 알고리즘 구현
    def eclat(tid_dict, min_support=0.6):
        n_transactions = len(df)
        frequent_itemsets = {}
    
        for k in range(1, len(tid_dict)+1):
            for comb in combinations(tid_dict.keys(), k):
                intersect_tids = set.intersection(*[tid_dict[item] for item in comb])
                support = len(intersect_tids) / n_transactions
                if support >= min_support:
                    frequent_itemsets[comb] = support
        return frequent_itemsets

    # 트랜잭션 ID 집합 계산
    tid_dict = tid_lists(df)

    # Eclat 알고리즘 실행
    frequent_itemsets_eclat = eclat(tid_dict, min_support=0.6)
    print(frequent_itemsets_eclat)
    
<br>

# [3] 어프라이어리(Apriori)
▣ 정의 : Apriori는 연관 규칙 학습을 위한 고전적인 알고리즘으로, 빈발한 항목 집합(frequent itemsets)을 찾아내고, 그 집합들 간의 연관성을 추출한다. 알고리즘의 핵심은 자주 발생하지 않는 항목이 포함된 집합은 자주 발생할 수 없다는 "항목 집합의 부분 집합도 빈발하다"는 특성을 이용한다.<br>
▣ 필요성 : 대규모 데이터에서 연관성을 발견하는 작업은 계산 비용이 높을 수 있으며, Apriori는 빈발하지 않은 항목 집합을 먼저 제거해 검색 공간을 줄여주는 방식으로 효율적인 탐색이 가능하다.<br>
▣ 장점 : 간단한 구조로 이해하기 쉽고, 계산 공간을 줄이기 위한 사전 단계를 가지고 있어, 효율적인 탐색이 가능하다.<br>
▣ 단점 : 탐색 공간이 커지면 성능이 저하됩니다. 대규모 데이터에서 비효율적일 수 있으며, 매번 새로운 후보 집합을 생성해야 하므로 계산 비용이 크다.<br>
▣ 응용분야 : 시장 바구니 분석 (장바구니 데이터에서 자주 함께 구매되는 상품을 찾음), 추천 시스템, 웹 페이지 연결성 분석<br>
▣ 모델식 : 
지지도(Support): 특정 항목 집합이 전체 거래에서 얼마나 자주 발생하는가<br>
신뢰도(Confidence): 특정 항목이 발생한 경우 다른 항목이 함께 발생할 확률<br>
향상도(Lift): 항목 간의 상호 의존성을 측정<br>

    from mlxtend.frequent_patterns import apriori, association_rules
    import pandas as pd

    # 예시 데이터 생성 (장바구니 데이터)
    data = {'milk': [1, 0, 1, 1, 0],
            'bread': [1, 1, 1, 0, 1],
            'butter': [0, 1, 0, 1, 0],
            'beer': [1, 1, 1, 0, 0]}
    df = pd.DataFrame(data)

    # Apriori 알고리즘을 사용하여 빈발 항목 집합 찾기
    frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)

    # 연관 규칙 찾기
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)

    print(frequent_itemsets)
    print(rules)

<br>

---

차원축소의 필요성 : 데이터에는 중요한 부분과 중요하지 않은 부분이 존재하는데, 여기서 중요하지 않은 부분이 노이즈(noise)이다. 머신러닝 과정에서는 데이터에서 정보를 언들때 방해가 되는 불필요한 노이즈를 제거하는 것이 중요한데, 이 노이즈를 제거할 때 사용하는 방법이 차원축소(dimension reduction)이다. 차원축소는 주어진 데이터의 정보손실을 최소화하면서 노이즈를 줄이는 것이 핵심이다. 차원축소를 통해 차원이 늘어날 수록 필요한 데이터가 기하급수적으로 많아지는 차원의 저주(curse of dimensionality) 문제를 해결할 수 있다. 지도학습의 대표적인 차원축소 방법은 선형판별분석(Linear Discriminant Analysis)이 있고, 비지도학습의 대표적인 차원축소 방법은 주성분분석(Principal Component Anaysis)이 있다.<br>

# [4] 주성분 분석(Principal Component Analysis, PCA)
▣ 정의 : PCA는 여러 특성(Feature) 변수들이 통계적으로 서로 상관관계가 없도록 변환시키는 것으로 고차원 데이터를 저차원으로 변환하는 차원 축소 기법으로 데이터의 분산을 최대화하는 방향으로 새로운 축(주성분)을 찾아 데이터를 투영한다. 특성이 $p$ 개가 있을 경우, 각 특성의 벡터는 $X_1, X_2, ... X_p$ 라고 나타낼 수 있으며, 주성분분석은 오직 공분산행렬(convariance matrix) $\sum$ 에만 영향을 받는다.<br> 
▣ PCA의 핵심개념
 - 분산의 최대화: 주성분은 데이터의 분산(변동성)을 최대한 많이 설명할 수 있는 방향으로 정해진다. 즉, 데이터의 주요한 변동성을 나타내는 축을 먼저 찾고, 그 축을 기준으로 데이터를 투영한다.<br>
 - 직교성: 각 주성분은 서로 직교(orthogonal)해야 하느데 이는 각 주성분이 서로 상관관계가 없는 독립적인 축이라는 것을 의미한다.<br>
▣ PCA의 모델식 : 주성분은 공분산 행렬의 고유값과 고유벡터를 사용하여 계산됩니다. 데이터 행렬 𝑋의 공분산 행렬 𝐶의 고유값과 고유벡터를 통해 새로운 주성분을 계산 
▣ PCA의 절차
 - (1) 데이터 표준화 : PCA를 수행하기 전에 데이터의 스케일을 맞춰주는 과정이다. 각 변수의 평균을 0으로 만들고 분산을 1로 맞추는 표준화를 통해 서로 다른 단위를 갖는 데이터들 간의 비교가 가능해지며, 이 과정은 보통 z-점수 정규화라고 한다.<br>
 - (2) 공분산 행렬 계산 : 각 변수 간의 상관관계를 확인하기 위해 공분산 행렬을 계산한다. 공분산은 두 변수가 함께 변하는 정도를 나타내며, 이를 통해 데이터의 분산이 어떻게 다른 변수들과 상호작용하는지 알 수 있다.<br>
  $Cov(X,Y)=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})$<br>
 - (3) 고유값 분해(Eigenvalue Decomposition) : 공분산 행렬의 고유값(eigenvalue)과 고유벡터(eigenvector)를 구한다. 고유벡터는 PCA에서 주성분에 해당하며, 고유값은 주성분이 설명하는 분산의 양을 나타낸다.<br>
 - (4) 주성분 선택: 고유값이 큰 순서대로 주성분을 선택한다. 가장 큰 고유값에 해당하는 고유벡터가 제1 주성분이 되고, 그다음 고유값에 해당하는 것이 제2 주성분이 된다. 고유값이 큰 주성분일수록 데이터의 분산을 많이 설명한다.<br>
 - (5) 차원 축소: 선택된 주성분을 사용해 데이터를 저차원으로 투영한다. 데이터의 중요한 특성(분산)을 유지하면서 불필요한 차원을 제거하여 차원을 축소할 수 있다.<br>
▣ PCA의 응용분야
 - 차원 축소: PCA는 고차원 데이터를 저차원으로 변환하여 계산 비용을 줄이고, 데이터의 시각화 및 분석을 쉽게 할 수 있다.<br>
 - 데이터 시각화: PCA는 고차원 데이터를 2D 또는 3D로 변환해 데이터의 패턴을 직관적으로 시각화하는 데 자주 사용된다.<br>
 - 잡음 제거: 잡음이 포함된 데이터에서 PCA를 사용해 중요한 주성분만을 남기고, 잡음에 해당하는 성분을 제거하여 데이터를 정제할 수 있다.<br>
 - 특징 추출: PCA는 머신 러닝에서 데이터의 주요 특징을 추출하는 데 효과적이다. 예를 들어, 얼굴 인식에서 얼굴 이미지의 주요 특징을 추출하여 얼굴을 효율적으로 분류할 수 있다.<br>
▣ PCA의 장점
 - 효율적인 차원 축소: 정보 손실을 최소화하면서 고차원 데이터를 저차원으로 축소할 수 있다.<br>
 - 잡음 제거: 데이터의 잡음을 효과적으로 제거하고 중요한 정보만 남길 수 있다.<br>
 - 데이터 시각화: 고차원 데이터를 저차원으로 변환하여 데이터의 구조를 쉽게 이해하고 분석할 수 있다.<br>
▣ PCA의 단점
 - 선형성 가정: PCA는 선형 변환만을 가정하기 때문에, 데이터가 비선형적인 관계를 가질 때는 효과가 떨어지느데, 이러한 경우에는 커널PCA 같은 비선형 변형 기법이 필요하다.<br>
 - 해석의 어려움: 차원축소 후 결과는 원래 변수의 의미를 잃을 수 있으므로 각 주성분이 원래 데이터의 어떤 특성을 설명하는지 직관적으로 해석하기 어렵다.<br>
 - 분산에만 집중: PCA는 분산을 기준으로 차원을 축소하므로, 분산이 적지만 중요한 정보가 있을 경우 이를 놓칠 수 있다.<br>

<br>

    from sklearn.decomposition import PCA
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np

    # 예시 데이터 생성
    data = np.random.rand(100, 5)

    # PCA 적용 (주성분 2개로 차원 축소)
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(data)

    # 결과 시각화
    plt.scatter(pca_result[:, 0], pca_result[:, 1])
    plt.title('PCA Result')
    plt.show()

<br>

# [5] 독립 성분 분석(Independent Component Analysis, ICA)
▣ ICA의 정의 : ICA는 다변량 신호에서 통계적으로 독립적인 성분을 추출하는 비선형 차원 축소 기법으로 주로 관찰된 신호를 독립적인 원천 신호로 분리하는 데 사용된다. PCA는 데이터의 분산을 최대화하는 축을 찾는 반면, ICA는 신호 간의 독립성을 기반으로 성분을 찾는다. 또한 PCA는 가우시안 분포를 가정하고 데이터의 상관관계만을 이용해 차원을 축소하거나 성분을 찾는 반면, ICA는 신호들 간의 고차원적 통계적 독립성에 초점을 맞추기 때문에 더 복잡한 구조의 신호 분리 문제를 해결할 수 있다.<br>
▣ ICA의 필요성 : 관측된 신호가 여러 독립적인 원천 신호의 혼합으로 구성될 때, 각 독립적인 신호를 복원하는 데 필요하며 특히 신호 처리 및 음성 분리에 유용하다.<br>
▣ ICA의 응용분야
 - 뇌파(EEG) 신호 분석: 뇌의 여러 부위에서 발생하는 신호들이 혼합되어 측정된 뇌파를 ICA를 사용하여 개별적인 신경 활동을 분리할 수 있다.<br>
 - 음성 신호 분리: 여러 사람이 동시에 이야기하는 상황에서, 각각의 사람의 목소리를 분리해 내는 문제를 해결하는 데 사용된다.<br>
 - 이미지 처리: 이미지에서 개별적인 패턴을 추출하거나 잡음을 제거하는 데 사용될 수 있다.<br>
▣ ICA의 ICA의 알고리즘  
 - FastICA: 가장 널리 사용되는 ICA 알고리즘으로, 비선형성을 이용해 독립 성분을 빠르게 찾는 방법으로 주로 신경망 기반의 고속 수렴 특성을 이용해 구현된다. FastICA는 신호의 비정규성(정규 분포에서 얼마나 벗어나는지)을 최대화하는 방향으로 성분을 찾는데, 이는 신호가 정규 분포에서 멀어질수록 독립적인 성분일 가능성이 크다는 통계적 특성에 기반한다.<br>
 - Infomax ICA: 정보 이론을 기반으로 한 ICA 방법으로, 관측된 데이터에서 정보량을 최대화하는 방식으로 독립 성분을 추정하며, 신경망의 학습 방식과 유사한 방식으로 작동한다.<br>
▣ 모델식 : ICA는 신호의 혼합 행렬 𝑋를 독립적인 신호 𝑆와 혼합 행렬 𝐴로 분해하는 문제로 정의하며, 목표는 𝑆를 추출<br> 
▣ ICA의 장점 : 통계적으로 독립적인 신호를 분리할 수 있으며 신호 처리, 이미지 분할, 음성 분리 등에서 강력한 성능을 발휘한다. 
▣ ICA의 ICA의 단점
 - 혼합 행렬의 정확성: ICA는 관측 데이터가 독립적인 신호들의 선형 혼합으로 이루어졌다고 가정하지만 이 가정이 항상 맞는 것은 아니다.<br>
 - 잡음에 민감함: 데이터에 잡음이 많이 포함되어 있으면 성분 분리가 어려워질 수 있다.<br>
 - 순서 문제: ICA에서 추출된 독립 성분은 원래 신호의 순서를 보장하지 않으며, 성분의 크기도 원래 신호와 다를 수 있는데 이는 추가적인 후처리가 필요할 수 있음을 의미한다.<br>

<br>
    from sklearn.decomposition import FastICA
    import numpy as np

    # 예시 데이터 생성
    data = np.random.rand(100, 5)

    # ICA 적용
    ica = FastICA(n_components=2)
    ica_result = ica.fit_transform(data)

    # 결과 출력
    print(ica_result)

<br> 

# [6] 자기 조직화 지도(Self-Organizing Maps, SOM)
▣ SOM의 정의 : SOM은 비지도 학습 알고리즘 중 하나로, 고차원의 데이터를 저차원(일반적으로 2차원) 공간으로 투영하여 데이터의 구조를 시각화하는 데 사용된다. PCA는 선형 변환을 통해 차원 축소를 수행하지만, SOM은 비선형 변환을 사용하여 더 복잡한 데이터 구조를 반영할 수 있으며, k-평균은 각 군집의 중심을 찾는 방식으로 군집화를 수행하는 반면, SOM은 뉴런이 격자 형태로 조직되어 있어 더 직관적인 시각화가 가능하다.<br> 

▣ SOM의 핵신개념
 - 차원 축소: 고차원의 데이터를 2차원 맵으로 변환하여 시각화하여 고차원 데이터의 구조를 쉽게 이해하고 분석하는 데 유용하다.<br> 
 - 군집화: SOM은 데이터를 자연스럽게 군집화하는 효과를 가지고 있으며, 유사한 패턴을 가진 데이터는 SOM의 인접한 뉴런들에 할당된다.<br> 
 - 비선형 변환: SOM은 데이터의 비선형 구조도 반영하여 선형적인 변환 기법보다 복잡한 데이터 구조를 효과적으로 다룰 수 있는 특징이 있다.<br>

▣ SOM의 모델식 : SOM은 뉴런의 위치 𝑟와 입력 벡터 𝑥 간의 거리 함수로 클러스터를 형성한다.<br>
학습 과정에서 뉴런의 가중치 𝑤_𝑖 : $w_i(t+1)=w_i(t)+η(t)h(t)(x(t)−w_i(t))$, 𝜂(𝑡)는 학습률, ℎ(𝑡)는 이웃 함수<br>

▣ SOM의 절차
 - (1) 초기화: SOM의 각 뉴런에 임의의 가중치 벡터를 할당한다. 이 가중치 벡터는 입력 데이터와 같은 차원을 가진다.<br>
 - (2) 입력 데이터 선택: 학습 과정에서 입력 데이터 벡터 하나를 무작위로 선택한다.<br>
 - (3) 승자 뉴런(BMU, Best Matching Unit) 찾기: SOM의 모든 뉴런 중에서 현재 입력 벡터와 가장 유사한(가중치 벡터 간의 유클리드 거리로 측정) 뉴런을 찾는 경쟁 학습의 핵심 단계이다.<br>
 - (4) 가중치 벡터 갱신: 선택된 승자 뉴런과 그 주변 이웃 뉴런들의 가중치 벡터를 조정한다. 이때, 가중치 벡터는 입력 데이터에 더 가깝게 이동한다.<br>
   $W(t+1)=W(t)+\theta(t)\cdot\eta(t)\cdot(X-W(t))$<br>

▣ SOM의 장점
 - 비지도 학습: SOM은 라벨링되지 않은 데이터를 학습할 수 있으므로, 데이터에 대한 사전 정보가 없어도 유용하게 사용할 수 있다.<br> 
 - 시각화 및 직관적 분석: 고차원 데이터를 저차원 맵으로 변환하여 데이터를 시각적으로 분석할 수 있으며, 군집의 분포나 데이터의 경향성을 직관적으로 이해할 수 있다.<br> 
 - 이웃 구조 보존: SOM은 입력 데이터의 이웃 관계를 보존하면서 저차원으로 투영하므로, 원래 데이터의 공간적 관계를 유지한다.<br> 

▣ SOM의 단점
 - 복잡한 파라미터 설정: 학습률, 이웃 크기 등 여러 파라미터를 적절히 설정해야 하며, 잘못된 설정은 모델의 성능에 부정적인 영향을 미칠 수 있다.<br> 
 - 대규모 데이터 학습에 비효율적: SOM은 입력 데이터가 많거나 차원이 매우 높을 때 학습 시간이 길어질 수 있다.<br> 
 - 해석의 어려움: SOM의 결과는 비선형 변환을 통해 얻어진 것이므로, 변환된 맵을 해석하는 것이 PCA 등의 선형 변환보다 더 어려울 수 있다.<br> 

▣ SOM의 응용분야
 - 이미지 분석: 이미지에서 패턴을 추출하거나, 유사한 이미지들을 군집화하는 데 사용된다.<br>
 - 문서 분류: 텍스트 데이터를 저차원 공간에 투영하여 유사한 문서를 군집화할 수 있다.<br>
 - 음성 인식: 음성 데이터를 학습하여 유사한 음성 패턴을 인식하고 분류하는 데 사용된다.<br>
 - 생물정보학: 유전자 데이터를 시각화하고 패턴을 찾아내는 데 활용된다.<br>

    from minisom import MiniSom
    import numpy as np

    #예시 데이터 생성
    data = np.random.rand(100, 5)

    #SOM 정의 및 학습
    som = MiniSom(10, 10, 5, sigma=0.3, learning_rate=0.5)
    som.train_random(data, 100)

    #SOM 시각화
    from pylab import plot, show, colorbar, bone
    bone()
    for i, x in enumerate(data):
        w = som.winner(x)
        plot(w[0]+0.5, w[1]+0.5, 'ro')
    show()

<br> 

# [7] 잠재 의미 분석(Latent Semantic Analysis, LSA)
▣ LSA의 정의 : LSA는 텍스트 데이터를 분석할 때 사용되는 자연어 처리(Natural Language Processing, NLP) 기법으로, 문서 간의 잠재적인 의미적 관계를 분석하고, 차원을 축소하여 텍스트의 중요한 의미적 패턴을 파악하는 방법이다. LSA는 특히 문서내 단어들 간의 공통적인 의미 구조를 찾는 데 중점을 두며, 이를 통해 문서 간의 유사성을 계산하고, 텍스트 데이터에서 의미적 관계를 도출한다.<br>

▣ LSA의 핵심개념
 - 의미 공간: LSA는 문서와 단어 간의 관계를 다차원 공간에서 표현한다. 여기서 각 단어와 문서는 고차원 벡터로 표현되며, LSA는 이러한 벡터들을 더 작은 의미 공간으로 투영해 문서와 단어 간의 잠재적인 의미적 관계를 파악한다.<br>
 - 차원 축소: LSA의 핵심 아이디어는 고차원의 단어-문서 행렬을 더 작은 차원으로 축소한다. 이렇게 하면 텍스트 데이터의 주요 패턴이 유지되면서도 불필요한 노이즈와 차원이 제거된다. 이를 위해 LSA는 주로 특이값 분해(Singular Value Decomposition, SVD)를 사용한다.<br>
 - 잠재 의미: LSA는 문서 내 단어들의 공통적인 의미 패턴을 학습하여, 단어가 명시적으로 나오지 않더라도 그 단어의 잠재적 의미를 파악할 수 있다. 예를 들어, "컴퓨터"와 "노트북"이라는 단어가 직접적인 상관관계를 가지지 않더라도 LSA는 이들이 비슷한 맥락에서 사용될 가능성을 학습한다.<br>
▣ LSA의 장점
 - 차원 축소: LSA는 텍스트 데이터의 고차원성을 효과적으로 축소하면서도 중요한 의미적 관계를 유지한다.<br>
 - 노이즈 제거: 텍스트 데이터에서 불필요한 노이즈를 제거하고, 중요한 의미 패턴을 추출하는 데 유리하다.<br>
 - 숨겨진 의미 관계 파악: 직접적으로 나타나지 않은 단어들 간의 의미적 유사성을 찾아내는 데 효과적이다.<br>
 - 문서 간 유사성 계산: LSA는 문서 간의 의미적 유사성을 쉽게 계산할 수 있으며, 이는 정보 검색 및 문서 추천 시스템에서 유용하게 사용된다.<br>
▣ LSA의 단점
 - 단어 순서 무시: LSA는 문서-단어 행렬을 기반으로 하므로, 단어의 순서나 문맥 정보를 고려하지 않는다. 이는 문맥을 중요시하는 텍스트 분석에서는 한계가 될 수 있다.<br>
 - 선형적 관계: LSA는 주로 선형적인 관계를 가정하기 때문에 비선형적인 의미 관계를 학습하는 데는 한계가 있다.<br>
 - 큰 데이터에서의 한계: 매우 큰 문서 집합에서는 SVD 계산이 복잡해지고 시간이 오래 걸릴 수 있다.<br>
▣ LSA의 응용분야
 - 정보 검색(IR, Information Retrieval): 검색 엔진에서 사용자가 입력한 질의(query)와 데이터베이스에 저장된 문서 간의 의미적 유사성을 분석하여, 더 관련성 높은 문서를 검색하는 데 LSA가 사용된다.<br>
 - 문서 분류 및 군집화: LSA는 문서 간의 잠재적인 의미적 유사성을 파악하여 문서들을 자동으로 분류하거나 군집화하는 데 사용된다.<br>
 - 추천 시스템: 사용자가 관심을 가질 만한 문서를 추천하는 시스템에서 LSA는 사용자와 문서 간의 잠재적인 의미적 관계를 찾아내는 데 도움을 준다.<br>
 - 주제 모델링(Topic Modeling): 문서 내에 숨겨진 주제(토픽)를 추출하는 데 LSA가 사용될 수 있다. 이는 LSA가 문서 내에서 자주 발생하는 단어 패턴을 학습하는 과정에서 주제를 식별할 수 있기 때문이다.<br>
 - 텍스트 유사도 계산: 두 문서가 의미적으로 얼마나 유사한지를 계산하는 데 LSA가 사용된다. 이는 중복 콘텐츠 탐지나 텍스트 유사도 기반 추천 시스템에서 유용하다.<br>

<br>

    from sklearn.decomposition import TruncatedSVD
    from sklearn.feature_extraction.text import TfidfVectorizer

    # 예시 문서
    docs = ["dog cat", "cat mouse", "dog mouse", "dog", "mouse"]

    # TF-IDF 행렬 생성
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(docs)

    # LSA 적용
    svd = TruncatedSVD(n_components=2)
    lsa_result = svd.fit_transform(X)

    print(lsa_result)



<br>

# [8] 특이값 분해(Singular Value Decomposition, SVD)
SVD는 선형대수학에서 매우 중요한 행렬 분해 기법으로, 임의의 행렬을 세 개의 행렬로 분해하는 방식이다. SVD는 데이터 분석, 차원 축소, 신호 처리, 머신러닝 등 다양한 분야에서 활용되며, 특히 고차원의 데이터를 다룰 때 중요한 역할을 한다. SVD는 주로 행렬의 특이값과 특이벡터를 통해 행렬의 구조를 파악하고, 이를 통해 데이터의 패턴을 찾거나 압축하는 데 사용된다.<br>

SVD의 핵심개념
 - 모든 행렬에 적용 가능: SVD는 정방행렬뿐만 아니라 비정방행렬이나 비대칭 행렬에도 적용할 수 있다는 점에서 SVD는 다른 행렬 분해 기법들에 비해 범용성이 높다.<br>
 - 특이값: Σ의 대각 성분인 특이값은 A의 행렬에 대한 중요한 정보를 담고 있으며 특이값이 클수록 데이터의 변동성이 크다는 것을 의미하며, 작은 특이값은 데이터의 변동이 적은 방향을 나타낸다.<br>
 - 차원 축소: SVD는 특이값을 기준으로 가장 중요한 축을 선택하여 차원을 축소할 수 있다. 가장 큰 특이값에 해당하는 축만 남기고 나머지를 제거하면, 데이터의 중요한 정보는 유지하면서 노이즈나 불필요한 차원을 줄일 수 있다.<br>
SVD의 장점
 - 모든 행렬에 적용 가능: SVD는 정방, 비정방, 비대칭 행렬 등 어떤 형태의 행렬에도 적용할 수 있다.<br>
 - 차원 축소: 데이터를 저차원 공간으로 변환하면서도 중요한 패턴을 유지할 수 있다.<br>
 - 노이즈 제거: 데이터에서 노이즈를 제거하여 중요한 정보만 남길 수 있다.<br>
 - 추천 시스템에서 활용: 사용자-아이템 간의 관계를 분석하고, 효과적인 추천을 가능하게 한다.<br>
SVD의 단점
 - 계산 복잡도: SVD는 계산 비용이 매우 높으며, 특히 매우 큰 행렬의 경우 계산이 오래 걸릴 수 있다.<br>
 - 해석 어려움: 분해된 행렬들이 원본 데이터와 직관적인 관계를 가지지 않기 때문에, 결과를 해석하는 것이 어려울 수 있다.<br>
SVD의 응용분야
 - 차원 축소(Dimensionality Reduction) : 고차원 데이터를 저차원으로 변환할 때 SVD는 중요한 역할을 한다. 데이터에서 가장 중요한 패턴을 유지하면서도 차원을 줄일 수 있습니다. 예를 들어, 텍스트 데이터를 다루는 잠재 의미 분석(LSA)에서는 단어-문서 행렬을 SVD를 사용하여 차원을 축소하고, 문서 간의 의미적 관계를 파악한다.<br>
 - 데이터 압축(Data Compression) : 이미지, 영상, 신호 데이터와 같은 큰 데이터를 압축할 때도 SVD가 유용하다. 원래 데이터의 중요한 정보를 유지하면서 압축된 데이터를 생성할 수 있으며, 이를 통해 메모리 사용량을 줄이고 계산 시간을 단축할 수 있다.<br>
 - 노이즈 제거(Denoising) : 데이터에서 노이즈를 제거하는 데 SVD가 사용된다. 노이즈는 주로 작은 특이값에 해당하는 방향에서 발생하므로, 작은 특이값을 제거하고 큰 특이값에 해당하는 정보만 남기면 노이즈를 제거할 수 있다.<br>
 - 추천 시스템(Recommendation Systems) : 추천 시스템에서 사용자와 아이템 간의 상호작용 행렬을 SVD로 분해하여, 사용자에게 가장 적합한 아이템을 추천할 수 있다. 이는 영화 추천, 제품 추천 등 다양한 도메인에서 활용된다.<br>
 - 이미지 압축(Image Compression) : SVD는 이미지 데이터를 효율적으로 압축하는 데 사용된다. 이미지를 행렬로 표현한 후 SVD를 통해 차원을 축소하면, 원본 이미지의 중요한 정보는 유지하면서도 크기가 줄어든 이미지를 생성할 수 있다.<br> 

<br>
    import numpy as np
    from numpy.linalg import svd

    # 예시 데이터 생성
    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

    # SVD 적용
    U, Sigma, VT = svd(A)

    print("U matrix:")
    print(U)
    print("Sigma values:")
    print(Sigma)
    print("VT matrix:")
    print(VT)

<br>

# [9] t-distributed Stochastic Neighbor Embedding(t-SNE)
t-SNE는 주로 데이터 시각화와 고차원 데이터의 구조를 이해하기 위한 기법으로 사용되는 차원 축소의 방법<br>

t-SNE의 장점
 - 우수한 시각화: 고차원 데이터의 군집 구조를 잘 반영하므로, 데이터의 숨겨진 패턴을 시각적으로 잘 드러낸다.<br>
 - 비선형 데이터 처리: 비선형 구조를 가진 데이터에서도 효과적으로 작동한다.<br>

t-SNE의 단점
 - 계산 비용: 대규모 데이터셋에서는 계산량이 많아 느릴 수 있다. 특히, 데이터 포인트 수가 많아질수록 계산 시간이 급격히 증가한다.<br>
 - 초매개변수의 민감성: t-SNE의 성능은 초기 매개변수(예: σ 값 및 학습률)에 민감하게 반응하므로, 이 값을 잘 조정해야 최적의 결과를 얻을 수 있다.<br>
 - 재현성 문제: 초기 위치에 따라 결과가 달라질 수 있으므로, 같은 데이터셋에서도 다른 결과를 얻을 수 있다.<br>

t-SNE의 응용 분야
 - 데이터 시각화: 고차원 데이터의 클러스터 및 패턴을 시각적으로 탐색하는 데 매우 유용하다. 예를 들어, 이미지 데이터, 텍스트 데이터, 유전자 표현 데이터 등을 시각화할 수 있다.<br>
 - 클러스터링 분석: t-SNE를 사용하여 데이터의 자연스러운 클러스터를 발견하고, 이를 바탕으로 클러스터링 알고리즘을 개선할 수 있다.<br>
 - 차원 축소: 데이터 전처리 과정에서, t-SNE를 사용하여 데이터의 차원을 줄이고, 이후 분석이나 모델링에 사용될 수 있다.<br>
 - 딥러닝 모델의 시각화: 신경망 모델의 중간 출력을 t-SNE로 시각화하여, 모델이 학습한 데이터의 특징을 이해할 수 있다.<br>

<br>

    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt
    import numpy as np

    # 예시 데이터 생성
    data = np.random.rand(100, 50)  # 50차원의 데이터 100개

    # t-SNE 적용 (2차원으로 차원 축소)
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    tsne_result = tsne.fit_transform(data)

    # 결과 시각화
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1])
    plt.title('t-SNE Result')
    plt.show()


