


#  11 : ì§€ë„ í•™ìŠµ(Supervised Learning, SL) : íšŒê·€(regression) + ë¶„ë¥˜(classification)

---

	[1] k-ìµœê·¼ì ‘ ì´ì›ƒ(k-Nearest Neighbors, K-NN) 	
		k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€(k-Nearest Neighbors Regression)
		k-ìµœê·¼ì ‘ ì´ì›ƒ ë¶„ë¥˜(k-Nearest Neighbors Classification)
 	[2] ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (Support Vector Machine, SVM)
		ì„œí¬íŠ¸ ë²¡í„° íšŒê·€(Support Vector Regression, SVR)
		ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜(Support Vector Classification, SVC)
	[3] ê²°ì • íŠ¸ë¦¬(Decision Tree)
 		ê²°ì • íŠ¸ë¦¬ íšŒê·€(Decision Tree Regression)
   		ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜(Decision Tree Classification)
 	[4] ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest) : ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)ì— í•´ë‹¹(12ê°•)
		ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€(Random Forest Regression)  
		ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜(Random Forest Classification)    	  	
	
---  

# [1] k-ìµœê·¼ì ‘ ì´ì›ƒ(k-Nearest Neighbors, K-NN) 	
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/neighbors.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/neighbors/index.html<br>
â–£ ì •ì˜ : ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ìœ ì‚¬ì†ì„±ì— ë”°ë¼ ë¶„ë¥˜í•˜ì—¬ ë°ì´í„°ë¥¼ ê±°ë¦¬ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ë¶„ì„í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ,<br>
ë¹„ì§€ë„í•™ìŠµì¸ êµ°ì§‘í™”(Clustering)ê³¼ ìœ ì‚¬í•œ ê°œë…ì´ë‚˜ ê¸°ì¡´ ê´€ì¸¡ì¹˜ì˜ Yê°’ì´ ì¡´ì¬í•œë‹¤ëŠ” ì ì—ì„œ ì§€ë„í•™ìŠµì— í•´ë‹¹í•œë‹¤.<br>

| ì¥ì                              | ë‹¨ì                                               |
|----------------------------------|---------------------------------------------------|
| ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€  | ëª¨ë¸ ë¯¸ìƒì„±ìœ¼ë¡œ íŠ¹ì§•ê³¼ í´ë˜ìŠ¤ ê°„ ê´€ê³„ ì´í•´ê°€ ì œí•œì  |
| í•™ìŠµ ë°ì´í„°ë¶„í¬ ê³ ë ¤ ë¶ˆìš” | ì ì ˆí•œ Kì˜ ì„ íƒì´ í•„ìš” |
| ë¹ ë¥¸ í›ˆë ¨ ë‹¨ê³„ | ë°ì´í„°ê°€ ë§ì•„ì§€ë©´ ëŠë¦¼ : ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality) |
| ìˆ˜ì¹˜ê¸°ë°˜ ë°ì´í„° ë¶„ë¥˜ ì„±ëŠ¥ìš°ìˆ˜ | ëª…ëª©íŠ¹ì§• ë° ëˆ„ë½ë°ì´í„°ìœ„í•œ ì¶”ê°€ì²˜ë¦¬ í•„ìš”(ì´ìƒì¹˜ì— ë¯¼ê°)|

ë°ì´í„°ë¡œë¶€í„° ê±°ë¦¬ê°€ ê°€ê¹Œìš´ 'K'ê°œì˜ ë‹¤ë¥¸ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ ì°¸ì¡°í•˜ì—¬ ë¶„ë¥˜í• ë•Œ ê±°ë¦¬ì¸¡ì •ì€ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê³„ì‚°ë²•ì„ ì‚¬ìš©í•œë‹¤.<br>
![](./images/distance.PNG)

K-NN ëª¨ë¸ì€ ê° ë³€ìˆ˜ë“¤ì˜ ë²”ìœ„ë¥¼ ì¬ì¡°ì •(í‘œì¤€í™”, ì •ê·œí™”)í•˜ì—¬ ê±°ë¦¬í•¨ìˆ˜ì˜ ì˜í–¥ì„ ì¤„ì—¬ì•¼ í•œë‹¤.<br>
(1) ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”(min-max normalization) : ë³€ìˆ˜ Xì˜ ë²”ìœ„ë¥¼ 0(0%)ì—ì„œ 1(100%)ì‚¬ì´ë¡œ ë‚˜íƒ€ëƒ„<br><br>
$X_{new} = \frac{X-min(X)}{max(X)-min(X)}$<br>

(2) z-ì ìˆ˜ í‘œì¤€í™”(z-score standardization) : ë³€ìˆ˜ Xì˜ ë²”ìœ„ë¥¼ í‰ê· ì˜ ìœ„ë˜ëŠ” ì•„ë˜ë¡œ í‘œì¤€í¸ì°¨ë§Œí¼ ë–¨ì–´ì ¸ ìˆëŠ” ì§€ì ìœ¼ë¡œ í™•ëŒ€ ë˜ëŠ” ì¶•ì†Œ(ë°ì´í„°ë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜)í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ë°ì´í„°ì˜ ì¤‘ì‹¬ì„ 0ìœ¼ë¡œ ë§ì¶”ê³ , ë°ì´í„°ë¥¼ ë‹¨ìœ„ í‘œì¤€ í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ ê°’ì„ ì¬ì¡°ì •<br><br>
$X_{new} = \frac{X-\mu}{\sigma}= \frac{X-min(X)}{StdDev(X)}$

<br>

## k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€(k-Nearest Neighbors Regression)
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html<br>
ì£¼ë³€ì˜ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ìƒ˜í”Œ í‰ê· ì„ í†µí•´ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì´ë‹¤.<br> 
í•œê³„ : í…ŒìŠ¤íŠ¸í•˜ê³ ì í•˜ëŠ” ìƒ˜í”Œì— ê·¼ì ‘í•œ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°, ì¦‰ í›ˆë ¨ ì…‹ì˜ ë²”ìœ„ë¥¼ ë§ì´ ë²—ì–´ë‚˜ëŠ” ìƒ˜í”Œì¸ ê²½ìš° ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ê¸° ì–´ë µë‹¤.Â 

	class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', 
	leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
	 
	# n_neighbors : int
	# ì´ì›ƒì˜ ìˆ˜ì¸ Kë¥¼ ê²°ì •í•œë‹¤. defaultëŠ” 5ë‹¤.Â 
	Â 
  	# weights : {'uniform', 'distance'} or callable
	# ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ ë°©ë²•ì„ ê²°ì •í•œë‹¤. defaultëŠ” uniformì´ë‹¤.Â 
	# 'uniform' : ê°ê°ì˜ ì´ì›ƒì´ ëª¨ë‘ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤.Â 
	# 'distance' : ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ ë” í° ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤.
	# callable : ì‚¬ìš©ìê°€ ì •ì˜í•œ í•¨ìˆ˜(ê±°ë¦¬ê°€ ì €ì¥ëœ ë°°ì—´ì„ ì…ë ¥ë°›ê³  ê°€ì¤‘ì¹˜ê°€ ì €ì¥ëœ ë°°ì—´ì„ ë°˜í™˜)
 	
	# algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}Â 
	# ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒë“¤ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê²°ì •í•œë‹¤. defaultëŠ” autoì´ë‹¤.Â 
	# 'auto' : ì…ë ¥ëœ í›ˆë ¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œë‹¤.Â 
	# 'ball_tree' : Ball-Tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.(https://nobilitycat.tistory.com/entry/ball-tree)
	# 'kd_tree' : KD-Tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.
	# 'brute' : Brute-Force íƒìƒ‰ì„ ì‚¬ìš©í•œë‹¤.Â Â 	
 	
	# leaf_size : int
	# Ball-Treeë‚˜ KD-Treeì˜ leaf sizeë¥¼ ê²°ì •í•œë‹¤. defaultê°’ì€ 30ì´ë‹¤.
	# íŠ¸ë¦¬ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ë¿ë§Œ ì•„ë‹ˆë¼, íŠ¸ë¦¬ì˜ êµ¬ì„±ê³¼ ì¿¼ë¦¬ ì²˜ë¦¬ì˜ ì†ë„ì—ë„ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.Â 
 	
	# p : int
	# ë¯¼ì½”í”„ìŠ¤í‚¤ ë¯¸í„°ë²•(Minkowski)ì˜ ì°¨ìˆ˜ë¥¼ ê²°ì •í•œë‹¤. 
	# p = 1ì´ë©´ ë§¨í•´íŠ¼ ê±°ë¦¬(Manhatten distance)
	# p = 2ì´ë©´ ìœ í´ë¦¬ë“œ ê±°ë¦¬(Euclidean distance)ì´ë‹¤.Â 

<br>

## k-ìµœê·¼ì ‘ ì´ì›ƒ ë¶„ë¥˜(k-Nearest Neighbors Classification)
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

	from sklearn.neighbors import KNeighborsClassifier
	kn = KNeighborsClassifier()

	#í›ˆë ¨
	kn.fit(train_input, train_target)
	#í‰ê°€
	print(kn.score(test_input, test_target))

<br>
 
# [2] ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (Support Vector Machine, SVM)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/svm.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/svm/index.html<br>
â–£ ì •ì˜ : SVMì€ Nì°¨ì› ê³µê°„ì„ (N-1)ì°¨ì›ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë¶„ë¥˜ ê¸°ë²•ìœ¼ë¡œ 2ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ìµœì ì˜ ê²½ê³„ë¥¼ ì°¾ëŠ”ë‹¤.<br>

![](./images/margin.png)

- ìµœì ì˜ ê²½ê³„ : ê° í´ë˜ìŠ¤ì˜ ë§ë‹¨ì— ìœ„ì¹˜í•œ ë°ì´í„°ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” ê²½ê³„<br>
- ì´ˆí‰ë©´(hyper plane) : ê³ ì°¨ì›(Nì°¨ì›)ì—ì„œ ë°ì´í„°ë¥¼ ë‘ ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²°ì • ê²½ê³„<br>
- Support Vector : ë°ì´í„°ë“¤ ì¤‘ì—ì„œ ê²°ì • ê²½ê³„ì— ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë“¤<br>
- ë§ˆì§„(Margin) : ê²°ì • ê²½ê³„ì™€ support vectorì‚¬ì´ì˜ ê±°ë¦¬<br>
- ë¹„ìš©(Cost) : ë§ˆì§„(Margin) í¬ê¸°ì˜ ë°˜ë¹„ë¡€<br>
- ê°ë§ˆ(Gamma) : train data í•˜ë‚˜ ë‹¹ ê²°ì • ê²½ê³„ì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ë²”ìœ„ë¥¼ ì¡°ì ˆí•˜ëŠ” ë³€ìˆ˜(í¬ë©´ ì˜¤ë²„í”¼íŒ…, ì‘ìœ¼ë©´ ì–¸ë”í”¼íŒ…)<br>


| ì¥ì                              | ë‹¨ì                                               |
|----------------------------------|---------------------------------------------------|
| ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆë‹¤ | ì»¤ë„í•¨ìˆ˜ ì„ íƒì´ ëª…í™•í•˜ì§€ ì•Šë‹¤ |
| ë¶„ë¥˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ | íŒŒë¼ë¯¸í„° ì¡°ì ˆì„ ì ì ˆíˆ ìˆ˜í–‰í•´ì•¼ë§Œ ìµœì ì˜ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ |
| ì €ì°¨ì›, ê³ ì°¨ì› ê³µê°„ì˜ ì ì€ ë°ì´í„°ì— ëŒ€í•´ì„œ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ìš°ìˆ˜ | ê³„ì‚°ëŸ‰ ë¶€ë‹´ì´ ìˆë‹¤ |
| ì¡ìŒì— ê°•í•˜ë‹¤ | ë°ì´í„° íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ë§ì— ë¯¼ê°í•˜ë‹¤|
| ë°ì´í„° íŠ¹ì„±ì´ ì ì–´ë„ ì¢‹ì€ ì„±ëŠ¥ | | 

â–£ ìœ í˜• : ì„ í˜•SVM(í•˜ë“œë§ˆì§„, ì†Œí”„íŠ¸ë§ˆì§„), ë¹„ì„ í˜•SVM<br>
- í•˜ë“œë§ˆì§„ : ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ë§ˆì§„ì˜ ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ, ëª¨ë“  í›ˆë ¨ë°ì´í„°ëŠ” ë§ˆì§„ì˜ ë°”ê¹¥ì¡±ì— ìœ„ì¹˜í•˜ê²Œ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ í•˜ë‚˜ì˜ ì˜¤ì°¨ë„ í—ˆìš©í•˜ë©´ ì•ˆëœë‹¤. ëª¨ë“  ë°ì´í„°ë¥¼ ì„ í˜•ìœ¼ë¡œ ì˜¤ì°¨ì—†ì´ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê²°ì •ê²½ê³„ë¥¼ ì°¾ëŠ” ê²ƒì€ ì‚¬ì‹¤ìƒ ì–´ë µë‹¤.<br><br>
$\displaystyle \min_{w}\frac{1}{2}\left\|\left\|w\right\|\right\|^2$     ì œì•½ ì¡°ê±´ì€ ëª¨ë“  iì— ëŒ€í•´ $ğ‘¦_ğ‘–(ğ‘¤â‹…ğ‘¥_ğ‘–+ğ‘)â‰¥1$ <br>

![](./images/hmargin.png)

- ì†Œí”„íŠ¸ë§ˆì§„ :  í•˜ë“œë§ˆì§„ì´ ê°€ì§„ í•œê³„ë¥¼ ê°œì„ í•˜ê³ ì ë‚˜ì˜¨ ê°œë…ìœ¼ë¡œ, ì™„ë²½í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–´ëŠ ì •ë„ì˜ ì˜¤ë¶„ë¥˜ë¥¼ í—ˆìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì†Œí”„íŠ¸ë§ˆì§„ì—ì„œëŠ” ì˜¤ë¶„ë¥˜ë¥¼ í—ˆìš©í•˜ê³  ì´ë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ slack variableì„ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ê²°ì •ê²½ê³„ë¡œë¶€í„° ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„°ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•œë‹¤.<br><br>
$\displaystyle \min_{w}\frac{1}{2}\left\|\left\|w\right\|\right\|^2 + C\sum_{i=1}^{n}\xi_i$

![](./images/smargin.png)

- ë¹„ì„ í˜•ë¶„ë¥˜ : ì„ í˜•ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•œ ì…ë ¥ê³µê°„ì„ ì„ í˜•ë¶„ë¦¬ê°€ ê°€ëŠ¥í•œ ê³ ì°¨ì› íŠ¹ì„±ê³µê°„ìœ¼ë¡œ ë³´ë‚´ ì„ í˜•ë¶„ë¦¬ë¥¼ ì§„í–‰í•˜ê³  ê·¸ í›„ ë‹¤ì‹œ ê¸°ì¡´ì˜ ì…ë ¥ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ë©´ ë¹„ì„ í˜• ë¶„ë¦¬ë¥¼ í•˜ê²Œ ëœë‹¤.<br><br>
![](./images/nlsvm.png)

<br>

	from sklearn.datasets import make_moons
	from sklearn.pipeline import Pipeline
	from sklearn.preprocessing import PolynomialFeatures

	polynomial_svm_clf = Pipeline([("poly_features", PolynomialFeatures(degree=3)),("scaler", StandardScaler()),("svm_clf", LinearSVC(C=10, loss="hinge", max_iter=2000, random_state=42))])
	polynomial_svm_clf.fit(X, y)

<br>

ì…ë ¥ê³µê°„ì„ íŠ¹ì„±ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œ mapping functionì„ ì‚¬ìš©í•œë‹¤<br>
$\Phi(x) = Ax$<br><br>
ê³ ì°¨ì›ì˜ íŠ¹ì„±ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ëª©ì í•¨ìˆ˜ì— ëŒ€í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì´ ê°„ë‹¨í•œ ì°¨ì›ì—ì„œëŠ” ê°€ëŠ¥í•˜ë‚˜ ê·¸ ì°¨ìˆ˜ê°€ ì»¤ì§ˆìˆ˜ë¡ ê³„ì‚°ëŸ‰ì˜ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë‹¤ì‹œ í•´ê²°í•˜ê³ ì ë‚˜ì˜¤ëŠ” ê°œë…ì´ ì»¤ë„íŠ¸ë¦­(Kernel trick) : ë¹„ì„ í˜• ë¶„ë¥˜ë¥¼ í•˜ê¸° ìœ„í•´ ì°¨ì›ì„ ë†’ì—¬ì¤„ ë•Œë§ˆë‹¤ í•„ìš”í•œ ì—„ì²­ë‚œ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì»¤ë„ íŠ¸ë¦­ì€ ì‹¤ì œë¡œëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ì„ í™•ì¥í•˜ì§€ ì•Šìœ¼ë©´ì„œ íŠ¹ì„±ì„ í™•ì¥í•œ ê²ƒê³¼ ë™ì¼í•œ íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê¸°ë²•<br>
$k(x_i, x_j) =\Phi(x_i)^T\Phi(x_j)$<br><br>
í™•ì¥ëœ íŠ¹ì„±ê³µê°„ì˜ ë‘ ë²¡í„°ì˜ ë‚´ì ë§Œì„ ê³„ì‚°í•˜ì—¬ ê³ ì°¨ì›ì˜ ë³µì¡í•œ ê³„ì‚° ì—†ì´ ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ê°„ë‹¨í•˜ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ê³  ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ê°€ìš°ì‹œì•ˆ RBF(Radial basis function)ìœ¼ë¡œ ë‘ ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë¹„ì„ í˜• ë°©ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³ ì°¨ì› íŠ¹ì§• ê³µê°„ì—ì„œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì‚¬ìš©.<br><br>
$k(x,y) = e^{-\frac{-\left\|x_i-x_j\right\|^2}{2\sigma^2}}$<br><br>
ì§ì ‘ ì°¨ìˆ˜ë¥¼ ì •í•˜ëŠ” ë°©ì‹(Polynomial) : $k(x,y) = (1+x^Ty)^p$<br>
ì‹ ê²½ë§ í•™ìŠµ(Signomail) : $k(x,y) = tanh(kx_ix_j-\delta)$<br>

![](./images/hnlsvm.png)

<br>  

## ì„œí¬íŠ¸ ë²¡í„° íšŒê·€(Support Vector Regression, SVR)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/svm.html#regression<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/svm.html#svr<br>
â–£ ì •ì˜ : ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ì´ˆí‰ë©´ ê·¼ì²˜ì— ë°°ì¹˜í•˜ë©´ì„œ, í—ˆìš© ì˜¤ì°¨ $Ïµ$ ë‚´ì—ì„œ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ.<br>



	from sklearn.svm import SVR
 
 	svr = SVR(kernel='rbf', gamma='auto')
	svr.fit(xtrain, ytrain)

	score = svr.score(xtest, ytest)
	print("R-squared: ", score)

<br> 

## ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜(Support Vector Classification, SVC)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/svm.html#classification<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC<br>
â–£ ëª¨ë¸ì‹ : https://scikit-learn.org/stable/modules/svm.html#svc<br>
â–£ ì •ì˜ : ë‘ í´ë˜ìŠ¤(ë˜ëŠ” ë‹¤ìˆ˜ì˜ í´ë˜ìŠ¤)ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ìµœëŒ€ ë§ˆì§„ì„ ê°€ì§€ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ê²ƒ.<br>

	import sklearn.svm as svm

 	# ì„ í˜•ì¼ ê²½ìš°
	svm_clf =svm.SVC(kernel = 'linear')
 	# ë¹„ì„ í˜•ì¼ ê²½ìš°
 	svm_clf =svm.SVC(kernel = 'rbf')

	# êµì°¨ê²€ì¦
	scores = cross_val_score(svm_clf, X, y, cv = 5)
 	scores.mean()

<br>

# [3] ê²°ì • íŠ¸ë¦¬(Decision Tree)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/tree.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/tree/index.html<br>
â–£ ì •ì˜ : ì–´ë–¤ í•­ëª©ì— ëŒ€í•œ ê´€ì¸¡ê°’ê³¼ ëª©í‘œê°’ì„ ì—°ê²°ì‹œì¼œì£¼ëŠ” ì˜ˆì¸¡ ëª¨ë¸ë¡œ, ëŒ€í‘œì ì¸ ì§€ë„í•™ìŠµ ë¶„ë¥˜ ëª¨ë¸ì´ë©°, ìŠ¤ë¬´ê³ ê°œì™€ ê°™ì´ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¥¼ ê²°ì •í•˜ì—¬ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.<br> 

![](./images/tree.png)

| ì¥ì                              | ë‹¨ì                                               |
|----------------------------------|---------------------------------------------------|
| ì‹œê°í™”ë¥¼ í†µí•œ í•´ì„ì˜ ìš©ì´ì„±(ë‚˜ë¬´ êµ¬ì¡°ë¡œ í‘œí˜„ë˜ì–´ ì´í•´ê°€ ì‰¬ì›€, ìƒˆë¡œìš´ ê°œì²´ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë£¨íŠ¸ ë…¸ë“œë¶€í„° ë ë…¸íŠ¸ê¹Œì§€ ë”°ë¼ê°€ë©´ ë˜ë¯€ë¡œ ë¶„ì„ ìš©ì´) | íœ´ë¦¬ìŠ¤í‹±ì— ê·¼ê±°í•œ ì‹¤ìš©ì  ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµìš© ìë£Œì— ì˜ì¡´í•˜ê¸°ì— ì „ì—­ ìµœì í™”ë¥¼ ì–»ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ(ê²€ì¦ìš© ë°ì´í„°ë¥¼ í™œìš©í•œ êµì°¨ íƒ€ë‹¹ì„± í‰ê°€ë¥¼ ì§„í–‰í•˜ëŠ” ê³¼ì •ì´ í•„ìš”) |
| ë°ì´í„° ì „ì²˜ë¦¬, ê°€ê³µì‘ì—…ì´ ë¶ˆí•„ìš” | ìë£Œì— ë”°ë¼ ë¶ˆì•ˆì •í•¨(ì ì€ ìˆ˜ì˜ ìë£Œë‚˜ í´ë˜ìŠ¤ ìˆ˜ì— ë¹„êµí•˜ì—¬ í•™ìŠµ ë°ì´í„°ê°€ ì ìœ¼ë©´ ë†’ì€ ë¶„ë¥˜ì—ëŸ¬ ë°œìƒ) | 
| ìˆ˜ì¹˜í˜•, ë²”ì£¼í˜• ë°ì´í„° ëª¨ë‘ ì ìš© ê°€ëŠ¥ | ê° ë³€ìˆ˜ì˜ ê³ ìœ í•œ ì˜í–¥ë ¥ì„ í•´ì„í•˜ê¸° ì–´ë ¤ì›€ | 
| ë¹„ëª¨ìˆ˜ì ì¸ ë°©ë²•ìœ¼ë¡œ ì„ í˜•ì„±, ì •ê·œì„± ë“±ì˜ ê°€ì •ì´ í•„ìš”ì—†ê³  ì´ìƒê°’ì— ë¯¼ê°í•˜ì§€ ì•ŠìŒ | ìë£Œê°€ ë³µì¡í•˜ë©´ ì‹¤í–‰ì‹œê°„ì´ ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•¨ | 
| ëŒ€ëŸ‰ì˜ ë°ì´í„° ì²˜ë¦¬ì—ë„ ì í•©í•˜ê³  ëª¨í˜• ë¶„ë¥˜ ì •í™•ë„ê°€ ë†’ìŒ | ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ë¹„ì—°ì†ì  ê°’ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ë¶„ë¦¬ ê²½ê³„ì ì—ì„œëŠ” ì˜ˆì¸¡ì˜¤ë¥˜ê°€ ë§¤ìš° ì»¤ì§€ëŠ” í˜„ìƒ ë°œìƒ | 

![](./images/trees.png)

<br>

# ê²°ì • íŠ¸ë¦¬ íšŒê·€(Decision Tree Regression)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/tree.html#regression<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html<br>
â–£ ì •ì˜ : ë°ì´í„°ì— ë‚´ì¬ë˜ì–´ ìˆëŠ” íŒ¨í„´ì„ ë¹„ìŠ·í•œ ìˆ˜ì¹˜ì˜ ê´€ì¸¡ì¹˜ ë³€ìˆ˜ì˜ ì¡°í•©ìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ ë‚˜ë¬´ í˜•íƒœë¡œ ë§Œë“ ë‹¤.<br>
â–£ ëª¨ë¸ì‹ : $\widehat{f}(x) = \sum_{m=1}^{M}C_mI((x_1,x_2)\in R_m)$<br>
ë¹„ìš©í•¨ìˆ˜(cost function)ë¥¼ ìµœì†Œë¡œ í• ë•Œ ìµœìƒì˜ ë¶„í•  : ë°ì´í„°ë¥¼ Mê°œë¡œ ë¶„í• ($R_1,R_2,...R_M$)<br> 
$\underset{C_m}{min}\sum_{i=1}^{N}(y_i-f(x_i))^2=\underset{C_m}{min}\sum_{i=1}^{N}(y_i-\sum_{m=1}^{M}C_mI(x\in R_m))^2$<br>
ê° ë¶„í• ì— ì†í•´ ìˆëŠ” yê°’ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡í–ˆì„ë•Œ ì˜¤ë¥˜ê°€ ìµœì†Œí™” : $\widehat{C}_m=ave(y_i|x_i\in R_m)$<br>

	from sklearn.tree import DecisionTreeRegressor
 	from sklearn.metrics import mean_squared_error
 
 	# ê²°ì • íŠ¸ë¦¬ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ (ìµœëŒ€ ê¹Šì´ 5)
	tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
	tree_reg.fit(X_train, y_train)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
	y_pred = tree_reg.predict(X_test)

	# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (í‰ê·  ì œê³± ì˜¤ì°¨)
	mse = mean_squared_error(y_test, y_pred)
	print(f"Mean Squared Error: {mse}")

<br>

# ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜(Decision Tree Classification)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/tree.html#classification<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html<br>
â–£ ì •ì˜ : ë°ì´í„°ì— ë‚´ì¬ë˜ì–´ ìˆëŠ” íŒ¨í„´ì„ ë¹„ìŠ·í•œ ë²”ì£¼ì˜ ê´€ì¸¡ì¹˜ ë³€ìˆ˜ì˜ ì¡°í•©ìœ¼ë¡œ ë¶„ë¥˜ ëª¨ë¸ì„ ë‚˜ë¬´ í˜•íƒœë¡œ ë§Œë“ ë‹¤.<br>
â–£ ëª¨ë¸ì‹ : $\widehat{f(x)} = \sum_{m=1}^{M}k(m)I((x_1,x_2)\in R_m)$<br>
ëë…¸ë“œ(m)ì—ì„œ í´ë˜ìŠ¤(k)ì— ì†í•  ê´€ì¸¡ì¹˜ì˜ ë¹„ìœ¨ : $\widehat{P_{mk}}=\frac{1}{N_m}\sum_{x_i\in R_m}^{}I(y_i=k)$<br>
ëë…¸ë“œ mìœ¼ë¡œ ë¶„ë¥˜ëœ ê´€ì¸¡ì¹˜ : $k(m) = \underset{k}{argmax}\widehat{P_{mk}}$<br><br>
â–£ ë¹„ìš©í•¨ìˆ˜(ë¶ˆìˆœë„ ì¸¡ì •) : ë¶ˆìˆœë„(Impurity)ê°€ ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë“¤ì´ ì„ì—¬ ìˆê³ , ë¶ˆìˆœë„ê°€ ë‚®ì„ìˆ˜ë¡ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•œ ë°ì´í„°ê°€ ëª…í™•<br>
(1) ì˜¤ë¶„ë¥˜ìœ¨(Misclassification rate, Error rate) : ë¶„ë¥˜ ëª¨ë¸ì´ ì˜ëª» ë¶„ë¥˜í•œ ìƒ˜í”Œì˜ ë¹„ìœ¨ë¡œ, ì „ì²´ ìƒ˜í”Œ ì¤‘ì—ì„œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì´ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.(0ì—ì„œ 100 ì‚¬ì´ì˜ ê°’, 0%: ëª¨ë¸ì´ ëª¨ë“  ìƒ˜í”Œì„ ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡, 100%: ëª¨ë¸ì´ ëª¨ë“  ìƒ˜í”Œì„ ì˜ëª» ì˜ˆì¸¡)<br><br>
$\frac{FP+FN}{TP+TN+FP+FN}$ 
###### FP(False Positive) : ì‹¤ì œ ê°’ì´ Negativeì¸ë° Positiveë¡œ ì˜ˆì¸¡, FN(False Negative) : ì‹¤ì œ ê°’ì´ Positiveì¸ë° Negativeë¡œ ì˜ˆì¸¡, TP(True Positive) : ì‹¤ì œ ê°’ì´ Positiveì´ê³  Positiveë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡, TN(True Negative) : ì‹¤ì œ ê°’ì´ Negativeì´ê³  Negativeë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡<br>
(2) ì§€ë‹ˆê³„ìˆ˜(Gini Coefficient) : ë°ì´í„°ì…‹ì´ ì–¼ë§ˆë‚˜ í˜¼í•©ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆìˆœë„ì˜ ì¸¡ì •ì¹˜(0ì—ì„œ 0.5 ì‚¬ì´ì˜ ê°’, 0: ë°ì´í„°ê°€ ì™„ë²½í•˜ê²Œ í•œ í´ë˜ìŠ¤ì— ì†í•´ ìˆìŒì„ ì˜ë¯¸í•˜ë©°, ë¶ˆìˆœë„ê°€ ì „í˜€ ì—†ëŠ” ìƒíƒœ, 0.5: ë‘ ê°œì˜ í´ë˜ìŠ¤ê°€ ì™„ë²½í•˜ê²Œ ì„ì—¬ ìˆëŠ” ìƒíƒœ)<br>
$Gini(p)=1-\sum_{i=1}^{n}p_i^2$<br><br>
(3) ì—”íŠ¸ë¡œí”¼(Entropy) : í™•ë¥  ì´ë¡ ì—ì„œ ì˜¨ ê°œë…ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ë˜ëŠ” ì •ë³´ì˜ ë¬´ì§ˆì„œë¥¼ ì¸¡ì •í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ, ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ í˜¼ë€ìŠ¤ëŸ½ê³  ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ì§€ë¥¼ ì¸¡ì •(0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’, 0 : ë°ì´í„°ê°€ ì™„ë²½í•˜ê²Œ í•œ í´ë˜ìŠ¤ì— ì†í•´ ìˆìœ¼ë©°, ë¶ˆí™•ì‹¤ì„±ì´ ì—†ëŠ” ìƒíƒœ, 1 : ë°ì´í„°ê°€ ì™„ì „íˆ ì„ì—¬ ìˆê³ , ê°€ì¥ í° ë¶ˆí™•ì‹¤ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.<br>
$Entropy(p) = -\sum_{i=1}^{n}p_ilog_2p_i$<br> 

â–£ ìœ í˜• :  ID3, CART
 - ID3 : ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ë²”ì£¼í˜• ë°ì´í„°ì¸ ê²½ìš°ì—ë§Œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë‹¤. ì •ë³´íšë“ëŸ‰(Infomation Gain)ì´ ë†’ì€ íŠ¹ì§•ë¶€í„° ë¶„ê¸°í•´ë‚˜ê°€ëŠ”ë° ì •ë³´íšë“ëŸ‰ì€ ë¶„ê¸°ì „ ì—”íŠ¸ë¡œí”¼ì™€ ë¶„ê¸°í›„ ì—”íŠ¸ë¡œí”¼ì˜ ì°¨ì´ë¥¼ ë§í•œë‹¤.(ì—”íŠ¸ë¡œí”¼ ì‚¬ìš©)<br><br>
$IG(S, A) = E(S) - E(S|A)$<br>
 - CART : Classification and Regression Treeì˜ ì•½ìë¡œ, ì´ë¦„ ê·¸ëŒ€ë¡œ ë¶„ë¥˜ì™€ íšŒê·€ê°€ ëª¨ë‘ ê°€ëŠ¥í•œ ê²°ì •íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ yes ë˜ëŠ” no ë‘ ê°€ì§€ë¡œ ë¶„ê¸°í•œë‹¤.(ì§€ë‹ˆê³„ìˆ˜ ì‚¬ìš©)<br><br> 
$f(k,t_k) = \frac{m_{left}}{m}G_{left}+\frac{m_{right}}{m}G_{right}$<br>

<br>

	from sklearn.tree import DecisionTreeClassifier
	from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

 	#ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ ëª¨ë¸ ìƒì„± (ìµœëŒ€ ê¹Šì´ 3ìœ¼ë¡œ ì„¤ì •)
	clf = DecisionTreeClassifier(max_depth=3, random_state=42)
	clf.fit(X_train, y_train)  # í•™ìŠµ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ í•™ìŠµ

	#í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
	y_pred = clf.predict(X_test)

	#ì •í™•ë„ ì¶œë ¥
	accuracy = accuracy_score(y_test, y_pred)  # ì •í™•ë„ ê³„ì‚°
	print(f"Accuracy: {accuracy * 100:.2f}%")  # ì •í™•ë„ ì¶œë ¥


<br>

	(ê°œë³„ íŠ¸ë¦¬ ëª¨ë¸ì˜ ë‹¨ì )	
 	ê³„ì¸µì  êµ¬ì¡°ë¡œ ì¸í•´ ì¤‘ê°„ì— ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì—ëŸ¬ê°€ ê³„ì† ì „íŒŒ
  	í•™ìŠµ ë°ì´í„°ì˜ ë¯¸ì„¸í•œ ë³€ë™ì—ë„ ìµœì¢…ê²°ê³¼ì— í° ì˜í–¥
   	ì ì€ ê°œìˆ˜ì˜ ë…¸ì´ì¦ˆì—ë„ í° ì˜í–¥
	ë‚˜ë¬´ì˜ ìµœì¢… ë…¸ë“œ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ê³¼ì í•© ìœ„í•¨(Low Bias, Large Variance)

	(í•´ê²°ë°©ì•ˆ) ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random forest)

<br> 

# [4] ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)  
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/ensemble.html#random-forests<br>
â–£ ì •ì˜ : ë¶„ë¥˜ì™€ íšŒê·€ì— ì‚¬ìš©ë˜ëŠ” ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)ë¥¼ ì¡°í•©í•œ **ì•™ìƒë¸” í•™ìŠµ(ensemble learning)** ì„ ì ìš©í•œ ëª¨ë¸ì´ë‹¤. ì—¬ëŸ¬ê°œì˜ Training dataë¥¼ ìƒì„±í•˜ì—¬ ê° ë°ì´í„°ë§ˆë‹¤ ê°œë³„ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°°ê¹…(bootstrap aggregation, bagging)ê³¼ ì˜ì‚¬ê²°ì • ëª¨ë¸ êµ¬ì¶•ì‹œ ë³€ìˆ˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” Random subspaceê°€ íŠ¹ì§•.<br>
â–£ ëª¨ë¸ì‹ : $\widehat{y}=\frac{1}{N}\sum_{i=1}^{N}T_i(X)$ ($N$ : ê²°ì •íŠ¸ë¦¬ì˜ ìˆ˜, $T_i(X)$ : ê° ê²°ì •íŠ¸ë¦¬ $i$ê°€ ì…ë ¥ê°’ $X$ì— ëŒ€í•´ ì˜ˆì¸¡í•œ ê°’)

![](./images/Bootstrap.png)
ì¶œì²˜: https://www.researchgate.net/figure/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method_fig1_309031320<br>


| ì¥ì                              | ë‹¨ì                                               |
|----------------------------------|---------------------------------------------------|
| ëª¨ë¸ì´ ë‹¨ìˆœ, ê³¼ì í•©ì´ ì˜ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ | ì—¬ëŸ¬ê°œì˜ ê²°ì •íŠ¸ë¦¬ ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í¼ |
| ìƒˆë¡œìš´ ë°ì´í„°ì— ì¼ë°˜í™”ê°€ ìš©ì´í•¨ | ê³ ì°¨ì› ë° í¬ì†Œ ë°ì´í„°ì— ì˜ ì‘ë™í•˜ì§€ ì•ŠìŒ |

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€(Random Forest Regression)  
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#randomforestregressor<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor<br>
â–£ ì •ì˜ : ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸ì€ ê° íŠ¸ë¦¬ê°€ ì˜ˆì¸¡í•œ ê°’ë“¤ì˜ í‰ê· ì„ í†µí•´ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ëŠ” ëª¨ë¸ë¡œ, ë‹¤ìˆ˜ê²° ëŒ€ì‹ , íŠ¸ë¦¬ì—ì„œ ì–»ì€ ì˜ˆì¸¡ê°’ì˜ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•œë‹¤.<br>
â–£ ëª¨ë¸ì‹ : $\widehat{y}= \frac{1}{B}\sum_{i=1}^{B}T_i(x)$<br>
###### $T_i(x)$: ì…ë ¥ ë°ì´í„° ğ‘¥ì— ëŒ€í•œ ğ‘–ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’, B: ì „ì²´ íŠ¸ë¦¬ì˜ ê°œìˆ˜


	from sklearn.ensemble import RandomForestRegressor
 
 	# íŠ¸ë¦¬ ê°œìˆ˜ë¥¼ ë³€í™”ì‹œí‚¤ë©° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	for iTrees in nTreeList:
    		depth = None  # íŠ¸ë¦¬ ê¹Šì´ ì œí•œ ì—†ìŒ
    		maxFeat = 4  # ì‚¬ìš©í•  ìµœëŒ€ íŠ¹ì§• ìˆ˜
    		# ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    		wineRFModel = ensemble.RandomForestRegressor(n_estimators=iTrees,
			max_depth=depth, max_features=maxFeat,
			oob_score=False, random_state=531)
    		wineRFModel.fit(xTrain, yTrain)  # ëª¨ë¸ í•™ìŠµ
    		# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ê³„ì‚°
    		prediction = wineRFModel.predict(xTest)
    		# MSE ê³„ì‚° ë° ëˆ„ì 
    		mseOos.append(mean_squared_error(yTest, prediction))
     
	# MSE ì¶œë ¥
	print("MSE")
	print(mseOos)


<br>

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜(Random Forest Classification)    	  	
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#randomforestclassifier<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier<br>
â–£ ì •ì˜ : ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì€ ë‹¤ìˆ˜ì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Trees)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•™ìƒë¸” ëª¨ë¸ë¡œ, ê° ë‚˜ë¬´ëŠ” ë…ë¦½ì ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œ í›„ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ í†µí•´ ìµœì¢… í´ë˜ìŠ¤ë¥¼ ê²°ì •í•œë‹¤.<br>
â–£ ëª¨ë¸ì‹ : $\widehat{y}=mode(T_1(x),T_2(x),...,T_B(x))$<br>
###### $T_i(x)$: ì…ë ¥ ë°ì´í„° ğ‘¥ì— ëŒ€í•œ ğ‘–ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’, B: ì „ì²´ íŠ¸ë¦¬ì˜ ê°œìˆ˜, mode í•¨ìˆ˜ : ë‹¤ìˆ˜ê²° íˆ¬í‘œë°©ì‹

	from sklearn.ensemble import RandomForestClassifier
	from sklearn.model_selection import train_test_split
	from sklearn.metrics import accuracy_score
	from sklearn import datasets

	# ë¶“ê½ƒ ë°ì´í„°ì…‹ ë¡œë“œ
	iris = datasets.load_iris()

	# ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ë¶„ë¦¬
	X = iris.data
	y = iris.target

	# í•™ìŠµìš© ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì´ˆê¸°í™”
	model = RandomForestClassifier()

	# ëª¨ë¸ í•™ìŠµ
	model.fit(X_train, y_train)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
	y_pred = model.predict(X_test)

	# ì •í™•ë„ ê³„ì‚°
	accuracy = accuracy_score(y_test, y_pred)
	print("Accuracy:", accuracy)
 
<br>
