#  12 : ë¹„ì§€ë„ í•™ìŠµ (Unsupervised Learning, UL) : ê°•í™”í•™ìŠµ, ì•™ìƒë¸”í•™ìŠµ

---

## ê°•í™” í•™ìŠµ (Reinforcement Learning, RL)
<br>

    [1] Q-learning
    [2] Deep Q-Network (DQN)
    [3] State-Action-Reward-State-Action (SARSA)
    [4] ìœ ì „ ì•Œê³ ë¦¬ì¦˜ (Genetic Algorithm)
    [5] Asynchronous Advantage Actor-Critic (A3C)
  

## ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning, EL)
<br>

    [6] ìŠ¤íƒœí‚¹ (Stacking)
    [7] ë°°ê¹… (Bagging)
    [8] ë¶€ìŠ¤íŒ… (Boosting)

---  
# ê°•í™” í•™ìŠµ (Reinforcement Learning, RL)

# [1] Q-learning

<br>

# [2] Deep Q-Network (DQN)

<br>

# [3] State-Action-Reward-State-Action (SARSA)

<br>

# [4] ìœ ì „ ì•Œê³ ë¦¬ì¦˜ (Genetic Algorithm)

<br>

# [5] Asynchronous Advantage Actor-Critic (A3C)

<br>

---

# ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning, EL)
â–£ API : https://scikit-learn.org/stable/api/sklearn.ensemble.html<br>
â–£ ì •ì˜ : ì•™ìƒë¸” í•™ìŠµì´ë€ ë‹¤ìˆ˜ì˜ ê¸°ì´ˆ ì•Œê³ ë¦¬ì¦˜(base algorithm)ì„ ê²°í•©í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ì˜ ì˜ˆì¸¡ ëª¨ë¸ì„ í˜•ì„±í•˜ëŠ” ê²ƒì„ ë§í•˜ë©°, ì‚¬ìš© ëª©ì ì— ë”°ë¼ ë°°ê¹…(Bagging), ë¶€ìŠ¤íŒ…(Boosting), ìŠ¤íƒí‚¹(Stacking)ìœ¼ë¡œ ë¶„ë¥˜ëœë‹¤.<br>

# [6] ìŠ¤íƒœí‚¹ (Stacking)
â–£ ì •ì˜ : ìŠ¤íƒœí‚¹ì€ ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê¸°ë°˜ ëª¨ë¸(base model) ì—¬ëŸ¬ ê°œë¥¼ í•™ìŠµí•œ í›„, ì´ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ëŠ” ë°©ì‹ì´ë‹¤. ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ì˜ ë©”íƒ€ ëª¨ë¸(meta-model)ë¡œ í•™ìŠµì‹œì¼œ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.<br>
â–£ í•„ìš”ì„± : ë‹¨ì¼ ëª¨ë¸ì˜ ì•½ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ëª¨ë¸ì„ ì¡°í•©í•¨ìœ¼ë¡œì¨ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë„ì¶œí•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê²°ì • íŠ¸ë¦¬, ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹ (SVM), ì‹ ê²½ë§ ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê²°í•©í•  ìˆ˜ ìˆë‹¤.<br>
â–£ ì¥ì  : ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ë”ìš± ê°•ë ¥í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í¸í–¥ê³¼ ë¶„ì‚°ì„ ë³´ì™„í•  ìˆ˜ ìˆë‹¤.<br>
â–£ ë‹¨ì  : ëª¨ë¸ ì¡°í•©ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ì´ ì»¤ì§€ê³ , ë©”íƒ€ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° ì¶”ê°€ì ì¸ ì‹œê°„ì´ ì†Œìš”ë˜ë©° ê³¼ì í•©(overfitting)ì˜ ìœ„í—˜ì´ ìˆë‹¤.<br>
â–£ ì‘ìš©ë¶„ì•¼ : ì—¬ëŸ¬ ëª¨ë¸ì˜ íŠ¹ì„±ì´ ìœ ìš©í•  ë•Œ ì‚¬ìš©í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê¸ˆìœµ ì˜ˆì¸¡, ì´ë¯¸ì§€ ë¶„ë¥˜ ë“± ë‹¤ì–‘í•œ ë¬¸ì œì—ì„œ í™œìš©ëœë‹¤.<br>
â–£ ëª¨ë¸ì‹ : $ğ‘“_1$ ì€ ê°ê°ì˜ ê°œë³„ ëª¨ë¸, $ğ‘“_2$ ëŠ” ë©”íƒ€ ëª¨ë¸, $\widehat{y}=f_2(f_1(x_1),f_1(x_2),...f_1(x_n))$<br>
â–£ python ì˜ˆì œ : 

    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import StackingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    X, y = load_iris(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ê°œë³„ ëª¨ë¸ ì •ì˜
    estimators = [('svc', SVC(probability=True)),('tree', DecisionTreeClassifier())]

    # ìŠ¤íƒœí‚¹ ëª¨ë¸ ì •ì˜
    stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
    stacking_clf.fit(X_train, y_train)

    # ì„±ëŠ¥ í‰ê°€
    print(stacking_clf.score(X_test, y_test))
    
<br>

# [7] ë°°ê¹… (Bagging)
â–£ ì •ì˜ : ë°°ê¹…ì€ ë™ì¼í•œ ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ í•™ìŠµí•˜ë˜, ê° í•™ìŠµë§ˆë‹¤ ë‹¤ë¥¸ ë°ì´í„° ìƒ˜í”Œì„ ì‚¬ìš©í•œë‹¤. ì£¼ë¡œ ë¶€íŠ¸ìŠ¤íŠ¸ë©(bootstrap) ë°©ë²•ìœ¼ë¡œ ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ë©°, ìµœì¢… ì˜ˆì¸¡ì€ ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê·  ë˜ëŠ” íˆ¬í‘œë¡œ ê²°í•©í•œë‹¤. ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)ì´ë‹¤.<br>
â–£ í•„ìš”ì„± : ë‹¨ì¼ ëª¨ë¸ì´ ë°ì´í„°ì˜ íŠ¹ì • ë¶€ë¶„ì— ê³¼ì í•©í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ì˜ˆì¸¡ì˜ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.<br>
â–£ ì¥ì  : ë¶„ì‚°ì„ ì¤„ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ê³¼ì í•©(overfitting)ì„ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ëœë‹¤.<br>
â–£ ë‹¨ì  : í¸í–¥ì„ ì¤„ì´ëŠ” ë°ëŠ” íš¨ê³¼ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ë§ì€ ëª¨ë¸ì„ í•™ìŠµí•˜ë¯€ë¡œ ê³„ì‚° ìì›ì´ ë§ì´ í•„ìš”í•˜ë‹¤.<br>
â–£ ì‘ìš©ë¶„ì•¼ : ëœë¤ í¬ë ˆìŠ¤íŠ¸ì²˜ëŸ¼ ê²°ì • íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ê¸ˆìœµ ì˜ˆì¸¡ ë“±ì— ë„ë¦¬ ì‚¬ìš©ëœë‹¤.
â–£ ëª¨ë¸ì‹ : 
â–£ python ì˜ˆì œ : 

    from sklearn.ensemble import BaggingClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    X, y = load_iris(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ë°°ê¹… ëª¨ë¸ ì •ì˜
    bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)
    bagging_clf.fit(X_train, y_train)

    # ì„±ëŠ¥ í‰ê°€
    print(bagging_clf.score(X_test, y_test))

<br>

# [8] ë¶€ìŠ¤íŒ… (Boosting)
â–£ ì •ì˜ : ë¶€ìŠ¤íŒ…ì€ ì•½í•œ í•™ìŠµê¸°(weak learner)ë¥¼ ì—°ì†ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©°, ì´ì „ í•™ìŠµì—ì„œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ë‹¤ìŒ ëª¨ë¸ì´ ì´ë¥¼ ë” ì˜ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” AdaBoost, Gradient Boosting, XGBoost ë“±ì´ ìˆë‹¤.<br>
â–£ í•„ìš”ì„± : ì•½í•œ í•™ìŠµê¸°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ê°•ë ¥í•œ í•™ìŠµê¸°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ì˜ëª»ëœ ì˜ˆì¸¡ì— ì§‘ì¤‘í•˜ì—¬ ì„±ëŠ¥ì„ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•œë‹¤.<br>
â–£ ì¥ì  : ëª¨ë¸ì´ ì—°ì†ì ìœ¼ë¡œ ê°œì„ ë˜ê¸° ë•Œë¬¸ì— ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìœ¼ë©°, ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì´ë‹¤.<br>
â–£ ë‹¨ì  : ì—°ì†ì ì¸ í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ê³¼ì í•©í•  ìœ„í—˜ì´ ìˆìœ¼ë©°, í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆë‹¤.<br>
â–£ ì‘ìš©ë¶„ì•¼ : ê¸ˆìœµ ì˜ˆì¸¡, ë¶„ë¥˜ ë¬¸ì œ, íšŒê·€ ë¶„ì„ ë“±ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ XGBoostëŠ” ëŒ€íšŒì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤.<br>
â–£ ëª¨ë¸ì‹ : 
â–£ python ì˜ˆì œ : 

    from sklearn.ensemble import AdaBoostClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    X, y = load_iris(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ë¶€ìŠ¤íŒ… ëª¨ë¸ ì •ì˜
    boosting_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)
    boosting_clf.fit(X_train, y_train)

    # ì„±ëŠ¥ í‰ê°€
    print(boosting_clf.score(X_test, y_test))

<br>

