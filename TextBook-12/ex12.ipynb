{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "772899f5-4a57-4b55-a137-d67c53024e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68279676 0.29722763]\n",
      " [0.61318348 0.18622236]\n",
      " [0.64308809 0.13316469]\n",
      " [0.65223923 0.14596641]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# [1] Q-learning\n",
    "#######################################################\n",
    "import numpy as np\n",
    "\n",
    "# 환경 설정 (간단한 그리드 월드 환경 가정)\n",
    "n_states = 5  # 총 5개의 상태 (0~4)\n",
    "n_actions = 2  # 각 상태에서 선택할 수 있는 2가지 행동 (예: 0, 1)\n",
    "Q = np.zeros((n_states, n_actions))  # Q-테이블 초기화, 모든 상태-행동 값이 0으로 시작\n",
    "\n",
    "alpha = 0.1  # 학습률: Q-값 업데이트 시 새로운 정보 반영 비율\n",
    "gamma = 0.9  # 할인 계수: 미래 보상의 중요도를 조정\n",
    "epsilon = 0.1  # 탐험 확률: 랜덤 행동 선택 비율\n",
    "\n",
    "# 행동 선택 함수 (ε-greedy 정책)\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:  # ε 확률로 탐험\n",
    "       return np.random.choice(n_actions)  # 랜덤으로 행동 선택\n",
    "    else:  # 1-ε 확률로 활용\n",
    "       return np.argmax(Q[state, :])  # Q-값이 가장 큰 행동 선택\n",
    "\n",
    "# Q-값 업데이트 함수\n",
    "def update_q(state, action, reward, next_state):\n",
    "    predict = Q[state, action]  # 현재 상태-행동 값 예측\n",
    "    target = reward + gamma * np.max(Q[next_state, :])  # 보상 + 다음 상태에서의 최대 Q-값\n",
    "    Q[state, action] = predict + alpha * (target - predict)  # Q-값 업데이트 공식\n",
    "\n",
    "# 학습 과정 반복\n",
    "for episode in range(100):  # 100번의 학습 에피소드 실행\n",
    "    state = np.random.randint(0, n_states)  # 임의의 상태에서 에피소드 시작\n",
    "    while state != 4:  # 종료 상태(4)에 도달하면 에피소드 종료\n",
    "        action = choose_action(state)  # 현재 상태에서 행동 선택 (탐험 또는 활용)\n",
    "        next_state = np.random.randint(0, n_states)  # 랜덤으로 다음 상태로 전이\n",
    "        reward = 1 if next_state == 4 else 0  # 보상 설정: 종료 상태로 전이 시 보상 1, 그 외 0\n",
    "        update_q(state, action, reward, next_state)  # Q-값 업데이트\n",
    "        state = next_state  # 다음 상태로 전이\n",
    "\n",
    "# 최종 Q-테이블 출력\n",
    "print(Q)  # 학습 완료된 Q-테이블 출력\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de016196-dc30-4804-90de-daa6d6bdabd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 2: Total Reward = 1, Steps = 3, Epsilon = 0.5000\n",
      "Episode 3: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 4: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 5: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 6: Total Reward = 1, Steps = 2, Epsilon = 0.5000\n",
      "Episode 7: Total Reward = 1, Steps = 2, Epsilon = 0.5000\n",
      "Episode 8: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 9: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 10: Total Reward = 1, Steps = 1, Epsilon = 0.5000\n",
      "Episode 11: Total Reward = 1, Steps = 2, Epsilon = 0.5000\n",
      "Episode 12: Total Reward = 1, Steps = 3, Epsilon = 0.5000\n",
      "Episode 13: Total Reward = 1, Steps = 2, Epsilon = 0.5000\n",
      "Episode 14: Total Reward = 1, Steps = 2, Epsilon = 0.5000\n",
      "Episode 15: Total Reward = 1, Steps = 4, Epsilon = 0.5000\n",
      "Episode 16: Total Reward = 1, Steps = 3, Epsilon = 0.5000\n",
      "Episode 17: Total Reward = 1, Steps = 2, Epsilon = 0.4975\n",
      "Episode 18: Total Reward = 1, Steps = 1, Epsilon = 0.4950\n",
      "Episode 19: Total Reward = 1, Steps = 1, Epsilon = 0.4925\n",
      "Episode 20: Total Reward = 1, Steps = 6, Epsilon = 0.4779\n",
      "Episode 21: Total Reward = 1, Steps = 1, Epsilon = 0.4756\n",
      "Episode 22: Total Reward = 1, Steps = 6, Epsilon = 0.4615\n",
      "Episode 23: Total Reward = 1, Steps = 1, Epsilon = 0.4592\n",
      "Episode 24: Total Reward = 1, Steps = 3, Epsilon = 0.4523\n",
      "Episode 25: Total Reward = 1, Steps = 1, Epsilon = 0.4500\n",
      "Episode 26: Total Reward = 1, Steps = 1, Epsilon = 0.4478\n",
      "Episode 27: Total Reward = 1, Steps = 1, Epsilon = 0.4456\n",
      "Episode 28: Total Reward = 1, Steps = 1, Epsilon = 0.4433\n",
      "Episode 29: Total Reward = 1, Steps = 2, Epsilon = 0.4389\n",
      "Episode 30: Total Reward = 1, Steps = 1, Epsilon = 0.4367\n",
      "Episode 31: Total Reward = 1, Steps = 1, Epsilon = 0.4345\n",
      "Episode 32: Total Reward = 1, Steps = 1, Epsilon = 0.4324\n",
      "Episode 33: Total Reward = 1, Steps = 1, Epsilon = 0.4302\n",
      "Episode 34: Total Reward = 1, Steps = 1, Epsilon = 0.4280\n",
      "Episode 35: Total Reward = 1, Steps = 1, Epsilon = 0.4259\n",
      "Episode 36: Total Reward = 1, Steps = 2, Epsilon = 0.4217\n",
      "Episode 37: Total Reward = 1, Steps = 1, Epsilon = 0.4195\n",
      "Episode 38: Total Reward = 1, Steps = 1, Epsilon = 0.4174\n",
      "Episode 39: Total Reward = 1, Steps = 1, Epsilon = 0.4154\n",
      "Episode 40: Total Reward = 1, Steps = 1, Epsilon = 0.4133\n",
      "Episode 41: Total Reward = 1, Steps = 6, Epsilon = 0.4010\n",
      "Episode 42: Total Reward = 1, Steps = 1, Epsilon = 0.3990\n",
      "Episode 43: Total Reward = 1, Steps = 2, Epsilon = 0.3951\n",
      "Episode 44: Total Reward = 1, Steps = 2, Epsilon = 0.3911\n",
      "Episode 45: Total Reward = 1, Steps = 1, Epsilon = 0.3892\n",
      "Episode 46: Total Reward = 1, Steps = 1, Epsilon = 0.3872\n",
      "Episode 47: Total Reward = 1, Steps = 1, Epsilon = 0.3853\n",
      "Episode 48: Total Reward = 1, Steps = 1, Epsilon = 0.3833\n",
      "Episode 49: Total Reward = 1, Steps = 7, Epsilon = 0.3701\n",
      "Episode 50: Total Reward = 1, Steps = 2, Epsilon = 0.3664\n",
      "Episode 51: Total Reward = 1, Steps = 3, Epsilon = 0.3610\n",
      "Episode 52: Total Reward = 1, Steps = 5, Epsilon = 0.3520\n",
      "Episode 53: Total Reward = 1, Steps = 2, Epsilon = 0.3485\n",
      "Episode 54: Total Reward = 1, Steps = 3, Epsilon = 0.3433\n",
      "Episode 55: Total Reward = 1, Steps = 1, Epsilon = 0.3416\n",
      "Episode 56: Total Reward = 1, Steps = 1, Epsilon = 0.3399\n",
      "Episode 57: Total Reward = 1, Steps = 3, Epsilon = 0.3348\n",
      "Episode 58: Total Reward = 1, Steps = 1, Epsilon = 0.3331\n",
      "Episode 59: Total Reward = 1, Steps = 1, Epsilon = 0.3315\n",
      "Episode 60: Total Reward = 1, Steps = 2, Epsilon = 0.3282\n",
      "Episode 61: Total Reward = 1, Steps = 1, Epsilon = 0.3265\n",
      "Episode 62: Total Reward = 1, Steps = 1, Epsilon = 0.3249\n",
      "Episode 63: Total Reward = 1, Steps = 2, Epsilon = 0.3217\n",
      "Episode 64: Total Reward = 1, Steps = 3, Epsilon = 0.3169\n",
      "Episode 65: Total Reward = 1, Steps = 4, Epsilon = 0.3106\n",
      "Episode 66: Total Reward = 1, Steps = 2, Epsilon = 0.3075\n",
      "Episode 67: Total Reward = 1, Steps = 1, Epsilon = 0.3059\n",
      "Episode 68: Total Reward = 1, Steps = 1, Epsilon = 0.3044\n",
      "Episode 69: Total Reward = 1, Steps = 3, Epsilon = 0.2999\n",
      "Episode 70: Total Reward = 1, Steps = 5, Epsilon = 0.2924\n",
      "Episode 71: Total Reward = 1, Steps = 2, Epsilon = 0.2895\n",
      "Episode 72: Total Reward = 1, Steps = 2, Epsilon = 0.2866\n",
      "Episode 73: Total Reward = 1, Steps = 2, Epsilon = 0.2838\n",
      "Episode 74: Total Reward = 1, Steps = 1, Epsilon = 0.2824\n",
      "Episode 75: Total Reward = 1, Steps = 2, Epsilon = 0.2795\n",
      "Episode 76: Total Reward = 1, Steps = 1, Epsilon = 0.2781\n",
      "Episode 77: Total Reward = 1, Steps = 1, Epsilon = 0.2768\n",
      "Episode 78: Total Reward = 1, Steps = 1, Epsilon = 0.2754\n",
      "Episode 79: Total Reward = 1, Steps = 1, Epsilon = 0.2740\n",
      "Episode 80: Total Reward = 1, Steps = 3, Epsilon = 0.2699\n",
      "Episode 81: Total Reward = 1, Steps = 1, Epsilon = 0.2686\n",
      "Episode 82: Total Reward = 1, Steps = 1, Epsilon = 0.2672\n",
      "Episode 83: Total Reward = 1, Steps = 3, Epsilon = 0.2632\n",
      "Episode 84: Total Reward = 1, Steps = 1, Epsilon = 0.2619\n",
      "Episode 85: Total Reward = 1, Steps = 2, Epsilon = 0.2593\n",
      "Episode 86: Total Reward = 1, Steps = 3, Epsilon = 0.2554\n",
      "Episode 87: Total Reward = 1, Steps = 1, Epsilon = 0.2541\n",
      "Episode 88: Total Reward = 1, Steps = 2, Epsilon = 0.2516\n",
      "Episode 89: Total Reward = 1, Steps = 3, Epsilon = 0.2479\n",
      "Episode 90: Total Reward = 1, Steps = 5, Epsilon = 0.2417\n",
      "Episode 91: Total Reward = 1, Steps = 2, Epsilon = 0.2393\n",
      "Episode 92: Total Reward = 1, Steps = 2, Epsilon = 0.2369\n",
      "Episode 93: Total Reward = 1, Steps = 4, Epsilon = 0.2322\n",
      "Episode 94: Total Reward = 1, Steps = 4, Epsilon = 0.2276\n",
      "Episode 95: Total Reward = 1, Steps = 2, Epsilon = 0.2253\n",
      "Episode 96: Total Reward = 1, Steps = 5, Epsilon = 0.2198\n",
      "Episode 97: Total Reward = 1, Steps = 8, Epsilon = 0.2111\n",
      "Episode 98: Total Reward = 1, Steps = 1, Epsilon = 0.2101\n",
      "Episode 99: Total Reward = 1, Steps = 1, Epsilon = 0.2090\n",
      "Episode 100: Total Reward = 1, Steps = 1, Epsilon = 0.2080\n",
      "학습 완료\n",
      "Test State: [0.25960352 0.06952445 0.82368307 0.0228719  0.90104538]\n",
      "Predicted Q-Values: [[1.0298501 0.8123386]]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# [2] DQN(Deep Q-Network) \n",
    "#######################################################\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# 상태 및 행동 정의\n",
    "n_states = 5  # 상태 공간 크기\n",
    "n_actions = 2  # 행동 공간 크기\n",
    "\n",
    "# 신경망 모델 정의\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, input_dim=n_states, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_actions, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Q-learning 파라미터 설정\n",
    "gamma = 0.95  # 할인율\n",
    "epsilon = 0.5  # 초기 탐험 확률\n",
    "epsilon_min = 0.01  # 최소 탐험 확률\n",
    "epsilon_decay = 0.995  # 탐험 확률 감소율\n",
    "batch_size = 32  # 배치 크기\n",
    "memory = deque(maxlen=2000)  # 경험 재플레이 버퍼\n",
    "\n",
    "# 행동 선택 함수 (ε-greedy)\n",
    "def choose_action(state):\n",
    "    global epsilon\n",
    "    if np.random.rand() <= epsilon:  # 탐험\n",
    "        return np.random.choice(n_actions)\n",
    "    state = np.reshape(state, [1, n_states])  # 상태를 신경망 입력 크기로 변환\n",
    "    return np.argmax(model.predict(state, verbose=0))  # Q-값이 최대인 행동 선택\n",
    "\n",
    "# 경험 재플레이 함수 (배치 학습)\n",
    "def replay():\n",
    "    global epsilon\n",
    "    if len(memory) < batch_size:\n",
    "        return  # 메모리가 충분하지 않으면 학습하지 않음\n",
    "    \n",
    "    # 배치 샘플링\n",
    "    minibatch = np.random.choice(len(memory), batch_size, replace=False)\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    for i in minibatch:\n",
    "        state, action, reward, next_state, done = memory[i]\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "    \n",
    "    # 배열 변환\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    next_states = np.array(next_states, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.float32)\n",
    "    \n",
    "    # Q-값 업데이트\n",
    "    target_q_values = model.predict(next_states, verbose=0)\n",
    "    max_target_q_values = np.amax(target_q_values, axis=1)\n",
    "    targets = model.predict(states, verbose=0)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        targets[i, actions[i]] = rewards[i] + (1 - dones[i]) * gamma * max_target_q_values[i]\n",
    "    \n",
    "    # 배치 학습\n",
    "    model.fit(states, targets, epochs=1, verbose=0)\n",
    "    \n",
    "    # 탐험 확률 감소\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# 학습 반복 (에피소드 수 감소)\n",
    "for episode in range(100):  # 에피소드 수 감소\n",
    "    state = np.random.rand(n_states)  # 초기 상태를 임의로 설정\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    while not done:\n",
    "        action = choose_action(state)  # 행동 선택\n",
    "        next_state = np.random.rand(n_states)  # 다음 상태 임의 생성\n",
    "        reward = 1 if np.random.rand() > 0.5 else 0  # 보상 설정\n",
    "        done = True if reward == 1 else False  # 보상이 1이면 종료\n",
    "        memory.append((state, action, reward, next_state, done))  # 경험 저장\n",
    "        state = next_state  # 상태 업데이트\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        replay()  # 경험 재플레이로 학습\n",
    "    \n",
    "    # 에피소드 결과 출력\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Steps = {step_count}, Epsilon = {epsilon:.4f}\")\n",
    "\n",
    "print(\"학습 완료\")\n",
    "\n",
    "# 최종 Q-값 확인 (랜덤 샘플)\n",
    "test_state = np.random.rand(n_states)\n",
    "test_q_values = model.predict(test_state[np.newaxis], verbose=0)\n",
    "print(f\"Test State: {test_state}\")\n",
    "print(f\"Predicted Q-Values: {test_q_values}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba916112-0c45-410a-b19a-d6c524952130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 0\n",
      "Episode: 2, Total Reward: -3\n",
      "Episode: 3, Total Reward: 0\n",
      "Episode: 4, Total Reward: 0\n",
      "Episode: 5, Total Reward: 1\n",
      "Episode: 6, Total Reward: 1\n",
      "Episode: 7, Total Reward: -2\n",
      "Episode: 8, Total Reward: 1\n",
      "Episode: 9, Total Reward: 1\n",
      "Episode: 10, Total Reward: -3\n",
      "Episode: 11, Total Reward: -1\n",
      "Episode: 12, Total Reward: -1\n",
      "Episode: 13, Total Reward: -2\n",
      "Episode: 14, Total Reward: 0\n",
      "Episode: 15, Total Reward: 1\n",
      "Episode: 16, Total Reward: 0\n",
      "Episode: 17, Total Reward: -2\n",
      "Episode: 18, Total Reward: -4\n",
      "Episode: 19, Total Reward: 1\n",
      "Episode: 20, Total Reward: 1\n",
      "Episode: 21, Total Reward: -6\n",
      "Episode: 22, Total Reward: 1\n",
      "Episode: 23, Total Reward: -2\n",
      "Episode: 24, Total Reward: 1\n",
      "Episode: 25, Total Reward: 0\n",
      "Episode: 26, Total Reward: 0\n",
      "Episode: 27, Total Reward: -1\n",
      "Episode: 28, Total Reward: -2\n",
      "Episode: 29, Total Reward: 1\n",
      "Episode: 30, Total Reward: 1\n",
      "Episode: 31, Total Reward: -4\n",
      "Episode: 32, Total Reward: -4\n",
      "Episode: 33, Total Reward: -13\n",
      "Episode: 34, Total Reward: 0\n",
      "Episode: 35, Total Reward: -3\n",
      "Episode: 36, Total Reward: 1\n",
      "Episode: 37, Total Reward: 1\n",
      "Episode: 38, Total Reward: -1\n",
      "Episode: 39, Total Reward: -3\n",
      "Episode: 40, Total Reward: -2\n",
      "Episode: 41, Total Reward: -4\n",
      "Episode: 42, Total Reward: -2\n",
      "Episode: 43, Total Reward: 0\n",
      "Episode: 44, Total Reward: 0\n",
      "Episode: 45, Total Reward: -7\n",
      "Episode: 46, Total Reward: 0\n",
      "Episode: 47, Total Reward: 0\n",
      "Episode: 48, Total Reward: -1\n",
      "Episode: 49, Total Reward: -5\n",
      "Episode: 50, Total Reward: 1\n",
      "Episode: 51, Total Reward: -3\n",
      "Episode: 52, Total Reward: -4\n",
      "Episode: 53, Total Reward: -3\n",
      "Episode: 54, Total Reward: -4\n",
      "Episode: 55, Total Reward: -5\n",
      "Episode: 56, Total Reward: 1\n",
      "Episode: 57, Total Reward: -7\n",
      "Episode: 58, Total Reward: -6\n",
      "Episode: 59, Total Reward: -3\n",
      "Episode: 60, Total Reward: 1\n",
      "Episode: 61, Total Reward: 1\n",
      "Episode: 62, Total Reward: 1\n",
      "Episode: 63, Total Reward: 1\n",
      "Episode: 64, Total Reward: -6\n",
      "Episode: 65, Total Reward: -5\n",
      "Episode: 66, Total Reward: -4\n",
      "Episode: 67, Total Reward: 1\n",
      "Episode: 68, Total Reward: 1\n",
      "Episode: 69, Total Reward: -4\n",
      "Episode: 70, Total Reward: -1\n",
      "Episode: 71, Total Reward: -1\n",
      "Episode: 72, Total Reward: -5\n",
      "Episode: 73, Total Reward: -1\n",
      "Episode: 74, Total Reward: -5\n",
      "Episode: 75, Total Reward: 0\n",
      "Episode: 76, Total Reward: 0\n",
      "Episode: 77, Total Reward: 1\n",
      "Episode: 78, Total Reward: -3\n",
      "Episode: 79, Total Reward: -3\n",
      "Episode: 80, Total Reward: 0\n",
      "Episode: 81, Total Reward: -1\n",
      "Episode: 82, Total Reward: -5\n",
      "Episode: 83, Total Reward: -3\n",
      "Episode: 84, Total Reward: 0\n",
      "Episode: 85, Total Reward: 1\n",
      "Episode: 86, Total Reward: -15\n",
      "Episode: 87, Total Reward: -1\n",
      "Episode: 88, Total Reward: -5\n",
      "Episode: 89, Total Reward: 0\n",
      "Episode: 90, Total Reward: 0\n",
      "Episode: 91, Total Reward: 0\n",
      "Episode: 92, Total Reward: 1\n",
      "Episode: 93, Total Reward: 1\n",
      "Episode: 94, Total Reward: -1\n",
      "Episode: 95, Total Reward: -2\n",
      "Episode: 96, Total Reward: 1\n",
      "Episode: 97, Total Reward: -3\n",
      "Episode: 98, Total Reward: 1\n",
      "Episode: 99, Total Reward: -4\n",
      "Episode: 100, Total Reward: -1\n",
      "Final Model Accuracy: 0.3111\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# [2] DQN(Deep Q-Network) iris\n",
    "#######################################################\n",
    "#!pip install tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 데이터 준비\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 환경 변수\n",
    "n_states = X_train.shape[1]  # 상태 차원 (Iris의 피처 수)\n",
    "n_actions = len(np.unique(y))  # 행동 차원 (Iris 클래스 수)\n",
    "episodes = 100  # 학습 에피소드\n",
    "gamma = 0.9  # 할인 계수\n",
    "epsilon = 1.0  # 초기 탐험 확률\n",
    "epsilon_min = 0.1  # 최소 탐험 확률\n",
    "epsilon_decay = 0.995  # 탐험 감소율\n",
    "batch_size = 32  # 배치 크기\n",
    "learning_rate = 0.001  # 학습률\n",
    "\n",
    "# 2. DQN 신경망 정의\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(n_states,)),  # Input 레이어 사용\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(n_actions, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "# Q-네트워크와 타겟 네트워크 초기화\n",
    "q_network = build_model()\n",
    "target_network = build_model()\n",
    "target_network.set_weights(q_network.get_weights())  # 가중치 동기화\n",
    "\n",
    "# 경험 재플레이 버퍼\n",
    "replay_buffer = []\n",
    "\n",
    "# 행동 선택 함수 (ε-greedy)\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:  # 탐험\n",
    "        return np.random.choice(n_actions)\n",
    "    else:  # 활용\n",
    "        q_values = q_network.predict(state[np.newaxis], verbose=0)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "# 경험 재플레이 함수\n",
    "def replay():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # 랜덤 샘플링\n",
    "    minibatch = np.random.choice(range(len(replay_buffer)), batch_size, replace=False)\n",
    "    minibatch = [replay_buffer[i] for i in minibatch]\n",
    "    \n",
    "    # 데이터 분리\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "    \n",
    "    # 배열로 변환 (데이터 일관성 유지)\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    next_states = np.array(next_states, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "    # Q-값 업데이트\n",
    "    target_q_values = target_network.predict(next_states, verbose=0)\n",
    "    max_target_q_values = np.max(target_q_values, axis=1)\n",
    "    targets = q_network.predict(states, verbose=0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        targets[i, actions[i]] = rewards[i] + (1 - dones[i]) * gamma * max_target_q_values[i]\n",
    "\n",
    "    # 모델 학습\n",
    "    q_network.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "# 타겟 네트워크 업데이트 함수\n",
    "def update_target_network():\n",
    "    target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "# 3. DQN 학습\n",
    "for episode in range(episodes):\n",
    "    state_idx = np.random.randint(0, len(X_train))\n",
    "    state = np.array(X_train[state_idx], dtype=np.float32)  # NumPy 배열로 변환\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(100):  # 최대 100 스텝 제한\n",
    "        action = choose_action(state, epsilon)  # 행동 선택\n",
    "        next_state_idx = np.random.randint(0, len(X_train))\n",
    "        next_state = np.array(X_train[next_state_idx], dtype=np.float32)  # NumPy 배열로 변환\n",
    "\n",
    "        # 보상 정의\n",
    "        reward = 1 if y_train[next_state_idx] == action else -1\n",
    "        done = (reward == 1)\n",
    "\n",
    "        # 경험 저장 (NumPy 배열로 변환된 상태 저장)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(replay_buffer) > 2000:  # 버퍼 크기 제한\n",
    "            replay_buffer.pop(0)\n",
    "\n",
    "        # 학습\n",
    "        replay()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # 탐험 감소\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # 타겟 네트워크 업데이트\n",
    "    if episode % 10 == 0:\n",
    "        update_target_network()\n",
    "\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# 4. 테스트 및 평가\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    action = choose_action(x, epsilon=0.0)  # 테스트 시 탐험 제거\n",
    "    y_pred.append(action)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Final Model Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3fa463a-b7c5-4fc1-9ed2-422b8788eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료!\n",
      "총 에피소드: 100\n",
      "성공적으로 종료 상태에 도달한 에피소드 수: 77\n",
      "성공 비율: 0.77\n",
      "평균 에피소드 보상: 0.77\n",
      "최종 Q-테이블:\n",
      "[[0.63674193 0.25212404]\n",
      " [0.33953311 0.59365598]\n",
      " [0.59980783 0.05818173]\n",
      " [0.47228527 0.21979548]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# [3] SARSA(State-Action-Reward-State-Action)\n",
    "#######################################################\n",
    "import numpy as np\n",
    "\n",
    "# SARSA 알고리즘을 위한 환경 설정\n",
    "n_states = 5  # 상태의 개수\n",
    "n_actions = 2  # 행동의 개수\n",
    "Q = np.zeros((n_states, n_actions))  # Q-테이블 초기화\n",
    "alpha = 0.1  # 학습률\n",
    "gamma = 0.9  # 할인 계수\n",
    "epsilon = 0.1  # 탐험 확률\n",
    "\n",
    "# 행동 선택 함수 (ε-greedy)\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:  # 탐험\n",
    "        return np.random.choice(n_actions)\n",
    "    else:  # 활용\n",
    "        return np.argmax(Q[state, :])\n",
    "\n",
    "# Q-값 업데이트 함수\n",
    "def update_q(state, action, reward, next_state, next_action):\n",
    "    predict = Q[state, action]  # 현재 상태-행동의 Q-값\n",
    "    target = reward + gamma * Q[next_state, next_action]  # SARSA의 타겟 값\n",
    "    Q[state, action] = predict + alpha * (target - predict)  # 업데이트 공식\n",
    "\n",
    "# 학습 반복\n",
    "num_episodes = 100  # 에피소드 수\n",
    "total_rewards = []  # 에피소드별 총 보상 기록\n",
    "success_count = 0  # 종료 상태에 도달한 에피소드 수\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = np.random.randint(0, n_states)  # 초기 상태를 무작위로 설정\n",
    "    action = choose_action(state)  # 초기 행동 선택\n",
    "    episode_reward = 0  # 에피소드별 총 보상\n",
    "    \n",
    "    while state != 4:  # 종료 상태(4)에 도달하면 종료\n",
    "        next_state = np.random.randint(0, n_states)  # 다음 상태 무작위 생성\n",
    "        reward = 1 if next_state == 4 else 0  # 종료 상태에 도달하면 보상 1\n",
    "        next_action = choose_action(next_state)  # 다음 행동 선택\n",
    "        update_q(state, action, reward, next_state, next_action)  # Q-값 업데이트\n",
    "        state, action = next_state, next_action  # 상태와 행동 업데이트\n",
    "        episode_reward += reward  # 보상 누적\n",
    "        \n",
    "        if reward == 1:  # 종료 상태에 도달\n",
    "            success_count += 1\n",
    "\n",
    "    total_rewards.append(episode_reward)  # 에피소드별 총 보상 기록\n",
    "\n",
    "# 학습 결과 평가\n",
    "print(\"학습 완료!\")\n",
    "print(f\"총 에피소드: {num_episodes}\")\n",
    "print(f\"성공적으로 종료 상태에 도달한 에피소드 수: {success_count}\")\n",
    "print(f\"성공 비율: {success_count / num_episodes:.2f}\")\n",
    "print(f\"평균 에피소드 보상: {np.mean(total_rewards):.2f}\")\n",
    "print(\"최종 Q-테이블:\")\n",
    "print(Q)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0446922d-c0ab-467e-aefe-7a1bd6b2be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.9778\n",
      "Logistic Regression Accuracy: 0.9333\n",
      "Random Forest Accuracy: 0.9111\n",
      "XGBoost Accuracy: 0.9333\n",
      "Final Model (LightGBM) Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# 스태킹(Stacking)\n",
    "#######################################################\n",
    "#!pip install lightgbm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 로드\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 데이터 분할 (Train, Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Base Models 정의\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # KNN 하이퍼파라미터 튜닝\n",
    "logistic = LogisticRegression(max_iter=300)  # Logistic Regression 개선\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)  # Random Forest 개선\n",
    "xgboost = xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, eval_metric='mlogloss', random_state=42)  # XGBoost 개선\n",
    "\n",
    "# 3. Base Models 학습 및 예측\n",
    "# KNN\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred_train = knn.predict_proba(X_train)\n",
    "knn_pred_test = knn.predict_proba(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn.predict(X_test))  # 정확도 계산\n",
    "\n",
    "# Logistic Regression\n",
    "logistic.fit(X_train, y_train)\n",
    "logistic_pred_train = logistic.predict_proba(X_train)\n",
    "logistic_pred_test = logistic.predict_proba(X_test)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic.predict(X_test))  # 정확도 계산\n",
    "\n",
    "# Random Forest\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred_train = rf.predict_proba(X_train)\n",
    "rf_pred_test = rf.predict_proba(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf.predict(X_test))  # 정확도 계산\n",
    "\n",
    "# XGBoost\n",
    "xgboost.fit(X_train, y_train)\n",
    "xgb_pred_train = xgboost.predict_proba(X_train)\n",
    "xgb_pred_test = xgboost.predict_proba(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgboost.predict(X_test))  # 정확도 계산\n",
    "\n",
    "# 4. Base Models 예측값을 하나의 데이터프레임으로 합침\n",
    "# 학습 데이터\n",
    "stacked_train = pd.DataFrame({\n",
    "    \"knn_0\": knn_pred_train[:, 0], \"knn_1\": knn_pred_train[:, 1], \"knn_2\": knn_pred_train[:, 2],\n",
    "    \"logistic_0\": logistic_pred_train[:, 0], \"logistic_1\": logistic_pred_train[:, 1], \"logistic_2\": logistic_pred_train[:, 2],\n",
    "    \"rf_0\": rf_pred_train[:, 0], \"rf_1\": rf_pred_train[:, 1], \"rf_2\": rf_pred_train[:, 2],\n",
    "    \"xgb_0\": xgb_pred_train[:, 0], \"xgb_1\": xgb_pred_train[:, 1], \"xgb_2\": xgb_pred_train[:, 2],\n",
    "})\n",
    "\n",
    "# 테스트 데이터\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"knn_0\": knn_pred_test[:, 0], \"knn_1\": knn_pred_test[:, 1], \"knn_2\": knn_pred_test[:, 2],\n",
    "    \"logistic_0\": logistic_pred_test[:, 0], \"logistic_1\": logistic_pred_test[:, 1], \"logistic_2\": logistic_pred_test[:, 2],\n",
    "    \"rf_0\": rf_pred_test[:, 0], \"rf_1\": rf_pred_test[:, 1], \"rf_2\": rf_pred_test[:, 2],\n",
    "    \"xgb_0\": xgb_pred_test[:, 0], \"xgb_1\": xgb_pred_test[:, 1], \"xgb_2\": xgb_pred_test[:, 2],\n",
    "})\n",
    "\n",
    "# 데이터 정규화 (스케일링)\n",
    "scaler = StandardScaler()\n",
    "stacked_train_scaled = scaler.fit_transform(stacked_train)\n",
    "stacked_test_scaled = scaler.transform(stacked_test)\n",
    "\n",
    "# 5. Final Model (LightGBM) 학습 및 예측\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    min_data_in_leaf=30,        # 리프 노드 최소 데이터 수를 낮춤\n",
    "    min_gain_to_split=0.1,      # 분할 수행 최소 정보 이득을 낮춤\n",
    "    max_depth=7,                # 트리 최대 깊이를 늘림\n",
    "    num_leaves=31,              # 리프 노드 개수를 늘림\n",
    "    feature_fraction=0.9,       # 학습에 사용할 피처 비율 증가\n",
    "    bagging_fraction=0.9        # 학습에 사용할 데이터 비율 증가\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "lgb_model.fit(stacked_train_scaled, y_train)\n",
    "\n",
    "# 예측\n",
    "lgb_pred = lgb_model.predict(stacked_test_scaled)\n",
    "\n",
    "# 최종 모델 정확도\n",
    "lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "\n",
    "# 6. Base Model 및 최종 모델 정확도 출력\n",
    "print(f\"KNN Accuracy: {knn_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {logistic_accuracy:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"Final Model (LightGBM) Accuracy: {lgb_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9baa50b3-7a29-4018-8ed2-3eb4391922a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.9333\n",
      "Bagging Cross-Validation Accuracy: 0.9667\n",
      "Single Decision Tree Accuracy: 0.9333\n",
      "Single Decision Tree Cross-Validation Accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# 배깅(Bagging)\n",
    "#######################################################\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 1. 데이터 로드\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 데이터 분할 (Train, Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Base Estimator 설정\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# 3. Bagging Classifier 설정\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,  # 기본 모델 개수 증가\n",
    "    max_samples=1.0,  # 전체 데이터를 사용\n",
    "    max_features=1.0,  # 모든 피처를 사용\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 배깅 모델 학습 및 평가\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "bagging_pred = bagging_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
    "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
    "\n",
    "# 교차 검증 (배깅)\n",
    "bagging_cv_scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
    "print(f\"Bagging Cross-Validation Accuracy: {bagging_cv_scores.mean():.4f}\")\n",
    "\n",
    "# 4. 단일 DecisionTreeClassifier 학습 및 평가\n",
    "single_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_tree_pred = single_tree.predict(X_test)\n",
    "single_tree_accuracy = accuracy_score(y_test, single_tree_pred)\n",
    "print(f\"Single Decision Tree Accuracy: {single_tree_accuracy:.4f}\")\n",
    "\n",
    "# 교차 검증 (단일 Decision Tree)\n",
    "single_tree_cv_scores = cross_val_score(single_tree, X, y, cv=5)\n",
    "print(f\"Single Decision Tree Cross-Validation Accuracy: {single_tree_cv_scores.mean():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a79a9d08-f4a7-4442-a29c-539fa3375b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Accuracy: 0.9333\n",
      "Cross-Validation Accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# 부스팅(Boosting) - Gradient Boosting Classifier\n",
    "#######################################################\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 1. 데이터 로드\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 데이터 분할 (Train, Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Gradient Boosting Classifier 설정 (하이퍼파라미터 조정)\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=200,      # 부스팅 스테이지 개수 증가\n",
    "    learning_rate=0.05,    # 학습률 감소\n",
    "    max_depth=5,           # 개별 트리의 최대 깊이 증가\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. 모델 학습\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# 5. 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Gradient Boosting Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 6. 교차 검증 (추가적인 평가)\n",
    "cv_scores = cross_val_score(gb_clf, X, y, cv=5)\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a28086bb-823c-434d-8f0a-f3073a597b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\python\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\python\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGBoost Classifier Accuracy: 0.9333\n",
      "Tuned XGBoost Cross-Validation Accuracy: 0.9467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\python\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\python\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# 부스팅(Boosting) - xgboost\n",
    "#######################################################\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 로드 및 분할\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 스케일링 (선택 사항)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. XGBoost 설정 (최적화된 설정 유지)\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=300,         # 부스팅 스테이지 증가\n",
    "    learning_rate=0.1,        # 기본 학습률 사용\n",
    "    max_depth=5,              # 적당한 깊이로 조정\n",
    "    min_child_weight=3,       # 분할의 최소 조건 강화\n",
    "    subsample=0.8,            # 샘플링 비율\n",
    "    colsample_bytree=0.8,     # 피처 샘플링 비율\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,  # 경고 제거\n",
    "    eval_metric=\"mlogloss\"    # 다중 클래스 손실 함수\n",
    ")\n",
    "\n",
    "# 3. 모델 학습\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. 예측\n",
    "y_pred = xgb_clf.predict(X_test_scaled)\n",
    "\n",
    "# 5. 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Tuned XGBoost Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 6. 교차 검증\n",
    "cv_scores = cross_val_score(xgb_clf, X, y, cv=5)\n",
    "print(f\"Tuned XGBoost Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750ab7f-ceee-427c-baec-264557c157b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
