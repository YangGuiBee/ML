{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fdfe055a-1df9-46c0-bd15-607f29753c42",
      "metadata": {
        "id": "fdfe055a-1df9-46c0-bd15-607f29753c42"
      },
      "source": [
        "[1] Model-free RL : Value Iteration\n",
        "\n",
        "테이블 기반<br>\n",
        "\t(1-1) Q-Learning: 상태 공간에서 사용되는 표(Q-Table) 기반<br>\n",
        "\t(1-2) SARSA(State Action Reward State Action)<br>\n",
        "심층 기반<br>\n",
        "\t(1-3) Q-Network: 신경망을 사용하여 Q-값을 근사화하는 모델<br>\n",
        "\t(1-4) DQN(Deep Q-Network): 딥러닝을 활용한 Q-Learning의 발전된 형태<br>\n",
        "\t(1-5) Double DQN: Q-Learning의 과대 평가 문제를 완화하기 위한 개선된 버전<br>\n",
        "\t(1-6) Dueling DQN: 상태 가치와 행동의 중요도를 분리하여 Q-값을 계산하는 모델<br>\n",
        "\t(1-7) DRQN(Deep Recurrent Q-Network): 순차적인 경험을 학습하기 위해 RNN 구조를 포함한 DQN<br>\n",
        "분포 기반<br>\n",
        "\t(1-8) C51(Categorical DQN 51): Q-값의 분포를 학습하는 DQN 확장<br>\n",
        "\t(1-9) IQN(Implicit Quantile Networks): Q-값의 분포를 세밀하게 조정하는 방식<br>\n",
        "\t(1-10) Rainbow: 여러 DQN 확장(PER, Double DQN, C51 등)을 결합한 통합 알고리즘<br>\n",
        "소프트 기반<br>\n",
        "\t(1-11) SQL(Soft Q-Learning): 엔트로피를 추가하여 Q값 학습을 안정화하는 방식<br>\n",
        "리플레이/탐색 기반<br>\n",
        "\t(1-12) PER(Prioritized Experience Replay): 중요한 경험을 우선적으로 학습하는 경험 리플레이 전략<br>\n",
        "\t(1-13) HER(Hindsight Experience Replay): 목표 달성을 학습할 수 있도록 과거 경험을 재사용하는 기법<br>\n",
        "\t(1-14) NoisyNet: 신경망 가중치에 노이즈를 추가해 탐색 효율성을 높이는 방식<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "798ebbd8-9766-4c61-bda5-e46bc73bf1ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "798ebbd8-9766-4c61-bda5-e46bc73bf1ec",
        "outputId": "0a4d5dae-a9c8-47a2-ccfb-bb1188c5229f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Q-Learning 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.601,  epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.860,  epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.918,  epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.946,  epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.953,  epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.956,  epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.961,  epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.965,  epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.964,  epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.965,  epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 최종 Q-테이블 (행: 상태, 열: 행동[왼쪽, 오른쪽])\n",
            "상태 0: [0.62170412 0.7019    ]\n",
            "상태 1: [0.62170963 0.791     ]\n",
            "상태 2: [0.70189422 0.89      ]\n",
            "상태 3: [0.79099209 1.        ]\n",
            "상태 4: [0. 0.]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "########################################################################################################\n",
        "## (1-1) Q-Learning : 상태 공간에서 사용되는 표(Q-Table) 기반\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ======================================\n",
        "# 0. 난수 시드 고정 (항상 동일한 결과 보장)\n",
        "# ======================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태(State) 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동(Action) 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "# 상태 전이 및 보상 함수\n",
        "def step(state, action):\n",
        "    # 행동이 0이면 왼쪽으로 이동, 1이면 오른쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 이하로 내려가지 않게 처리\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 이상으로 올라가지 않게 처리\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우 보상 +1, 그 외에는 -0.01 패널티\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0\n",
        "        done = True                                 # 목표 도달 → 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                              # 빨리 도달하도록 작은 음수 보상\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "\n",
        "# 초기 상태 반환 함수\n",
        "def reset():\n",
        "    return 0                                        # 항상 state 0에서 에피소드 시작\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 2. Q-Learning 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "alpha = 0.1         # 학습률 (Learning Rate)\n",
        "gamma = 0.9         # 할인율 (Discount Factor)\n",
        "epsilon = 1.0       # 초기 탐험 확률 (ε-greedy에서 ε)\n",
        "epsilon_min = 0.05  # 탐험 최소값\n",
        "epsilon_decay = 0.995  # 에피소드마다 ε 감소율\n",
        "\n",
        "n_episodes = 500    # 총 학습 반복(에피소드) 횟수\n",
        "max_steps = 20      # 한 에피소드에서 최대 step (무한 루프 방지용)\n",
        "\n",
        "# Q-테이블 초기화: 모든 상태-행동 쌍을 0으로 시작\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 3. ε-greedy 행동 선택 함수\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # 일정 확률 ε로 탐험(랜덤 행동 선택)\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    # 나머지 확률로 현재 Q값이 가장 큰 행동 선택 (exploitation)\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 4. Q-Learning 학습 루프\n",
        "# ======================================\n",
        "reward_history = []     # 에피소드별 총 보상을 저장할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Q-Learning 학습 시작 ===\")\n",
        "\n",
        "# 전체 에피소드 반복\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()     # 매 에피소드마다 초기 상태로 리셋\n",
        "    total_reward = 0.0  # 에피소드 누적 보상 초기화\n",
        "\n",
        "    # 한 에피소드 안에서 반복\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        # 1) ε-greedy 정책으로 행동 선택\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # 2) 환경에 행동 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) Q(s,a) 업데이트\n",
        "        #    TD Target = r + γ * max(Q(s', a'))\n",
        "        best_next_Q = np.max(Q[next_state])                 # 다음 상태에서의 최대 Q\n",
        "        td_target = reward + gamma * best_next_Q            # TD Target 계산\n",
        "        td_error = td_target - Q[state, action]             # TD Error 계산\n",
        "        Q[state, action] += alpha * td_error                # 학습률 α 반영하여 업데이트\n",
        "\n",
        "        # 4) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # 5) 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # 종료 상태이면 반복 중단\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # ε 감소 (탐험 → 이용 비중 증가)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # 50에피소드마다 최근 50개 평균 보상 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f},  epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 5. 학습된 Q테이블 출력\n",
        "# ======================================\n",
        "print(\"▶ 최종 Q-테이블 (행: 상태, 열: 행동[왼쪽, 오른쪽])\")\n",
        "for s in range(n_states):\n",
        "    print(f\"상태 {s}: {Q[s]}\")\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 6. 학습된 최적 정책 출력\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}    # 행동을 화살표로 표시\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:            # 마지막 상태는 Goal\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        best_a = np.argmax(Q[s])     # 각 상태에서 Q값이 가장 큰 행동 선택\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 7. 학습 결과 테스트 실행 (탐험 없이 greedy만)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()               # 초기 상태\n",
        "trajectory = [state]          # 방문한 상태 기록\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    action = np.argmax(Q[state])               # 탐험 없이 항상 최적 행동\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory)-1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e7327ef-cc3c-4f65-bac0-878aa87c54e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e7327ef-cc3c-4f65-bac0-878aa87c54e8",
        "outputId": "bc0077d2-6d54-4a6d-987c-c5f2b30f80cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 SARSA 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.518, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.884, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.913, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.949, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.953, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.958, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.961, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.965, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.965, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 최종 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.56804536 0.67414501]\n",
            "상태 1: [0.55285797 0.77460095]\n",
            "상태 2: [0.61751656 0.88120465]\n",
            "상태 3: [0.7167551 1.       ]\n",
            "상태 4: [0. 0.]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "########################################################################################################\n",
        "## (1-2) SARSA(State-Action-Reward-State-Action)\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 난수 시드 고정 (결과 재현성 보장)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 행동이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝 이하로 못 가게 처리\n",
        "    # 행동이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝 이상으로 못 가게 처리\n",
        "\n",
        "    # 목표 상태에 도달하면 보상 +1\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0\n",
        "        done = True                                 # 목표 도달 → 종료\n",
        "    # 그 외에는 작은 패널티\n",
        "    else:\n",
        "        reward = -0.01\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 매 에피소드 시작 상태는 항상 0\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 2. SARSA 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "alpha = 0.1         # 학습률\n",
        "gamma = 0.9         # 할인율\n",
        "epsilon = 1.0       # 탐험 비율 시작값 (ε-greedy)\n",
        "epsilon_min = 0.05  # 최소 탐험값\n",
        "epsilon_decay = 0.995  # 탐험 감소 비율\n",
        "\n",
        "n_episodes = 500    # 학습 에피소드 수\n",
        "max_steps = 20      # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "# Q 테이블 초기화 (상태 × 행동)\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 3. ε-greedy 행동 선택 함수\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    # 1-ε 확률로 현재 Q가 가장 큰 행동 선택\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 4. SARSA 학습 루프\n",
        "#    (On-policy: TD Target에 실제 다음 행동 a' 를 사용)\n",
        "# ======================================\n",
        "reward_history = []\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 SARSA 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    # (1) 에피소드 시작: 상태 초기화\n",
        "    state = reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    # (2) 초기 상태에서 첫 행동 선택 (SARSA는 s,a 쌍으로 시작)\n",
        "    action = choose_action(state, epsilon)\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        # (3) 현재 상태 s 에서 행동 a 수행\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (4) 다음 상태 s' 에서 다음 행동 a' 선택 (ε-greedy)\n",
        "        #     SARSA의 핵심: 여기서도 같은 정책(ε-greedy)을 사용\n",
        "        if not done:\n",
        "            next_action = choose_action(next_state, epsilon)\n",
        "        else:\n",
        "            next_action = None  # 종료 상태에서는 의미 없음\n",
        "\n",
        "        # (5) SARSA TD Target 계산\n",
        "        #     - done 이면 다음 상태의 Q는 0\n",
        "        #     - 아니면 Q(s', a') 사용\n",
        "        if done:\n",
        "            td_target = reward                         # 마지막 상태 → 미래 보상 없음\n",
        "        else:\n",
        "            td_target = reward + gamma * Q[next_state, next_action]\n",
        "\n",
        "        # (6) TD Error 및 Q 업데이트\n",
        "        td_error = td_target - Q[state, action]\n",
        "        Q[state, action] += alpha * td_error\n",
        "\n",
        "        # (7) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (8) 상태와 행동을 다음 시점으로 이동\n",
        "        state = next_state\n",
        "        action = next_action if not done else action  # done이면 action은 더 안 쓰지만 형식상 유지\n",
        "\n",
        "        # (9) 종료 상태면 에피소드 정리\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (10) 에피소드 종료 후 ε 감소\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # 50 에피소드마다 로그 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 5. 학습된 Q-테이블 출력\n",
        "# ======================================\n",
        "print(\"▶ 최종 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    print(f\"상태 {s}: {Q[s]}\")\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 6. 학습된 정책 출력\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        best_a = np.argmax(Q[s])\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 7. 학습된 정책으로 1회 테스트 실행 (탐험 없이 greedy만)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    # 테스트에서는 탐험 없이 항상 greedy 정책 사용\n",
        "    action = np.argmax(Q[state])\n",
        "    next_state, reward, done = step(state, action)\n",
        "\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory)-1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a682e26d-6030-4b9d-b473-052b2c5f1879",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a682e26d-6030-4b9d-b473-052b2c5f1879",
        "outputId": "e02404f9-9405-456b-f43a-138bf48b401b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Q-Network(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.634, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.817, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.939, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.953, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.951, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.956, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.962, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.963, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.965, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.61391433 0.71010196]\n",
            "상태 1: [0.58996149 0.71413935]\n",
            "상태 2: [0.57754957 0.76342132]\n",
            "상태 3: [0.59451489 0.96636278]\n",
            "상태 4: [0.63171271 0.72202013]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-3) Q-Network : 신경망을 사용하여 Q-값을 근사화하는 모델\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (실행할 때마다 같은 결과를 얻기 위함)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 이동마다 작은 패널티 부여\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 0 벡터 생성\n",
        "    x[0, state] = 1.0                               # 해당 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. Q-Network 파라미터 설정 (순수 NumPy로 신경망 구성)\n",
        "# ======================================\n",
        "input_dim = n_states      # 입력 차원: 상태를 one-hot으로 표현하므로 n_states\n",
        "hidden_dim = 16           # 은닉층 노드 수 (임의로 16으로 설정)\n",
        "output_dim = n_actions    # 출력 차원: 각 행동에 대한 Q값 2개\n",
        "\n",
        "# 가중치와 편향을 작은 값으로 초기화\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)   # 1층 가중치 (입력 → 은닉)\n",
        "b1 = np.zeros((1, hidden_dim))                      # 1층 편향\n",
        "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)  # 2층 가중치 (은닉 → 출력)\n",
        "b2 = np.zeros((1, output_dim))                      # 2층 편향\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 0보다 작으면 0, 크면 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU의 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward(x):\n",
        "    # 신경망 순전파: 입력 x에 대해 Q값을 계산\n",
        "    # x: (1, input_dim) 형태의 one-hot 상태 벡터\n",
        "    z1 = x @ W1 + b1                # 1층 선형 결합 (1 x hidden_dim)\n",
        "    a1 = relu(z1)                   # ReLU 활성화 (1 x hidden_dim)\n",
        "    z2 = a1 @ W2 + b2               # 2층 선형 결합 (1 x output_dim)\n",
        "    q_values = z2                   # 출력층: 각 행동에 대한 Q값\n",
        "    return z1, a1, q_values         # 역전파 위해 중간값도 반환\n",
        "\n",
        "# ======================================\n",
        "# 3. 하이퍼파라미터 설정 (Q-Learning과 동일 구조)\n",
        "# ======================================\n",
        "gamma = 0.9         # 할인율\n",
        "epsilon = 1.0       # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05  # ε의 최소값\n",
        "epsilon_decay = 0.995  # 에피소드마다 ε 감소 비율\n",
        "learning_rate = 0.01   # 신경망 파라미터 학습률 (gradient descent step 크기)\n",
        "\n",
        "n_episodes = 500    # 총 학습 에피소드 수\n",
        "max_steps = 20      # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "# ======================================\n",
        "# 4. ε-greedy 정책으로 행동 선택 함수\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 랜덤 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)         # 0 또는 1 중 랜덤 선택\n",
        "\n",
        "    # 1-ε 확률로 Q값이 최대인 행동 선택\n",
        "    x = state_to_onehot(state)                      # 상태를 one-hot으로 변환\n",
        "    _, _, q_values = forward(x)                     # Q값 계산\n",
        "    action = int(np.argmax(q_values, axis=1)[0])    # 가장 큰 Q값을 주는 행동 인덱스 선택\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 5. Q-Network 기반 Q-Learning 학습 루프\n",
        "# ======================================\n",
        "reward_history = []  # 에피소드별 총 보상을 저장할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Q-Network(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()          # 에피소드 시작 상태 초기화\n",
        "    total_reward = 0.0       # 에피소드 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        # (1) ε-greedy 정책으로 행동 선택\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 현재 상태와 다음 상태를 one-hot 벡터로 변환\n",
        "        x = state_to_onehot(state)                    # 현재 상태 (1 x n_states)\n",
        "        x_next = state_to_onehot(next_state)          # 다음 상태 (1 x n_states)\n",
        "\n",
        "        # (4) 현재 상태에서의 Q값 계산 (순전파)\n",
        "        z1, a1, q_values = forward(x)                 # q_values: (1 x n_actions)\n",
        "        q_value = q_values[0, action]                 # 선택한 행동에 대한 Q값 (스칼라)\n",
        "\n",
        "        # (5) 다음 상태에서의 최대 Q값 계산 (Q-Learning 방식)\n",
        "        _, _, q_values_next = forward(x_next)         # 다음 상태에서의 Q값들\n",
        "        max_next_q = float(np.max(q_values_next, axis=1)[0])  # 그 중 최대값\n",
        "\n",
        "        # (6) TD Target 계산\n",
        "        if done:\n",
        "            target = reward                           # 종료 상태면 미래 보상 없음\n",
        "        else:\n",
        "            target = reward + gamma * max_next_q      # r + γ * max_a' Q(s', a')\n",
        "\n",
        "        # (7) TD Error 계산 (예측 - 타깃)\n",
        "        #     손실 L = 0.5 * (q_value - target)^2 라고 두면\n",
        "        #     dL/d(q_value) = (q_value - target)\n",
        "        td_error = q_value - target                   # 스칼라\n",
        "\n",
        "        # (8) 출력층(z2)에서의 gradient 계산\n",
        "        #     z2: (1 x n_actions), 그 중 action 인덱스만 영향을 받음\n",
        "        dL_dz2 = np.zeros_like(q_values)              # (1 x n_actions) 0으로 초기화\n",
        "        dL_dz2[0, action] = td_error                  # 선택한 행동 위치에만 td_error 반영\n",
        "\n",
        "        # (9) 2층 가중치와 편향에 대한 gradient\n",
        "        #     dL/dW2 = a1^T @ dL_dz2  (hidden_dim x 1) x (1 x n_actions) = (hidden_dim x n_actions)\n",
        "        dW2 = a1.T @ dL_dz2                           # (hidden_dim x n_actions)\n",
        "        db2 = dL_dz2                                  # (1 x n_actions)\n",
        "\n",
        "        # (10) 1층으로 gradient 전파\n",
        "        #      dL/da1 = dL_dz2 @ W2^T  → (1 x n_actions) @ (n_actions x hidden_dim) = (1 x hidden_dim)\n",
        "        dL_da1 = dL_dz2 @ W2.T                        # (1 x hidden_dim)\n",
        "        #      ReLU 미분: dz1 = dL/da1 * ReLU'(z1)\n",
        "        dL_dz1 = dL_da1 * relu_deriv(z1)              # (1 x hidden_dim)\n",
        "\n",
        "        # (11) 1층 가중치와 편향에 대한 gradient\n",
        "        #      dL/dW1 = x^T @ dL_dz1  (input_dim x 1) x (1 x hidden_dim) = (input_dim x hidden_dim)\n",
        "        dW1 = x.T @ dL_dz1                            # (input_dim x hidden_dim)\n",
        "        db1 = dL_dz1                                  # (1 x hidden_dim)\n",
        "\n",
        "        # (12) 파라미터 업데이트 (경사하강법: W ← W - η * dW)\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "\n",
        "        # (13) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (14) 상태를 다음 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # (15) 종료 상태면 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (16) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (17) 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (18) 50 에피소드마다 최근 50개 평균 리워드와 현재 epsilon 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 6. 학습된 Q-Network로부터 '근사 Q-테이블' 출력\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                # 상태 s를 one-hot으로 변환\n",
        "    _, _, q_vals = forward(x)             # Q값 계산\n",
        "    q_vals_row = q_vals[0]                # (1 x n_actions) → (n_actions,)\n",
        "    print(f\"상태 {s}: {q_vals_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 7. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}         # 행동 인덱스를 화살표로 표현하기 위한 매핑\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"               # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, q_vals = forward(x)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 8. 학습된 정책으로 1회 테스트 실행 (탐험 없이 greedy만 사용)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                       # 초기 상태 0\n",
        "trajectory = [state]                  # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)        # 현재 상태를 one-hot으로 변환\n",
        "    _, _, q_vals = forward(x)         # Q값 계산\n",
        "    action = int(np.argmax(q_vals))   # 탐험 없이 항상 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "    trajectory.append(next_state)     # 방문한 상태 기록\n",
        "    state = next_state                # 상태 업데이트\n",
        "\n",
        "    if done:\n",
        "        break                         # 목표 도달 시 종료\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d1064385-818c-44e5-a3da-b497a09d9af3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1064385-818c-44e5-a3da-b497a09d9af3",
        "outputId": "2d470a22-789f-47a1-886e-1b5986241a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 DQN(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.691, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.859, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.916, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.951, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.955, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.954, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.960, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.967, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.966, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.55878423 0.68169877]\n",
            "상태 1: [0.53764078 0.68297671]\n",
            "상태 2: [0.52598826 0.73224926]\n",
            "상태 3: [0.54513745 0.96248031]\n",
            "상태 4: [0.57992423 0.69895181]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-4) DQN(Deep Q-Network) : 딥러닝을 활용한 Q-Learning의 발전된 형태 (2013, 2015)\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (재현 가능한 결과를 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 이동마다 작은 패널티 부여\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 0 벡터 생성\n",
        "    x[0, state] = 1.0                               # 해당 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. DQN용 Q-Network / Target Network 파라미터 정의\n",
        "#    - 간단한 2층 완전연결 신경망 사용\n",
        "# ======================================\n",
        "input_dim = n_states      # 입력 차원: 상태를 one-hot으로 표현하므로 n_states\n",
        "hidden_dim = 16           # 은닉층 노드 수 (임의로 16으로 설정)\n",
        "output_dim = n_actions    # 출력 차원: 각 행동에 대한 Q값 2개\n",
        "\n",
        "# 온라인 네트워크(Online Q-Network) 파라미터\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)   # 1층 가중치 (입력 → 은닉)\n",
        "b1 = np.zeros((1, hidden_dim))                      # 1층 편향\n",
        "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)  # 2층 가중치 (은닉 → 출력)\n",
        "b2 = np.zeros((1, output_dim))                      # 2층 편향\n",
        "\n",
        "# 타깃 네트워크(Target Q-Network) 파라미터 (초기에는 동일하게 복사)\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "W2_tgt = W2.copy()\n",
        "b2_tgt = b2.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 0보다 작으면 0, 크면 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU의 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward(x, W1, b1, W2, b2):\n",
        "    # 신경망 순전파: 입력 x에 대해 Q값을 계산\n",
        "    # x: (배치크기, input_dim) 형태의 상태(one-hot) 벡터\n",
        "    z1 = x @ W1 + b1                # 1층 선형 결합 (배치 x hidden_dim)\n",
        "    a1 = relu(z1)                   # ReLU 활성화 (배치 x hidden_dim)\n",
        "    z2 = a1 @ W2 + b2               # 2층 선형 결합 (배치 x output_dim)\n",
        "    q_values = z2                   # 출력층: 각 행동에 대한 Q값\n",
        "    return z1, a1, q_values         # 역전파를 위해 중간값도 함께 반환\n",
        "\n",
        "# ======================================\n",
        "# 3. DQN 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율\n",
        "epsilon = 1.0           # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05      # ε의 최소값\n",
        "epsilon_decay = 0.995   # 에피소드마다 ε 감소 비율\n",
        "learning_rate = 0.01    # 신경망 파라미터 학습률(경사하강 step 크기)\n",
        "\n",
        "n_episodes = 500        # 총 학습 에피소드 수\n",
        "max_steps = 20          # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 1000  # 리플레이 버퍼 최대 크기\n",
        "batch_size = 32         # 미니배치 크기\n",
        "warmup_steps = 100      # 최소 이 정도 샘플이 쌓인 후부터 학습 시작\n",
        "target_update_freq = 20 # 타깃 네트워크를 몇 에피소드마다 한 번씩 갱신할지\n",
        "\n",
        "# ======================================\n",
        "# 4. 리플레이 버퍼 구현 (간단한 리스트 버퍼)\n",
        "# ======================================\n",
        "replay_buffer = []  # (state, action, reward, next_state, done) 튜플을 저장\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 버퍼에 새 transition 추가\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)  # 가장 오래된 데이터 삭제 (FIFO)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    # 버퍼에서 랜덤하게 batch_size 개 샘플 추출\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, replace=False)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    return batch\n",
        "\n",
        "# ======================================\n",
        "# 5. ε-greedy 정책으로 행동 선택 함수 (Online Network 사용)\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 랜덤 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)         # 0 또는 1 중 랜덤 선택\n",
        "\n",
        "    # 1-ε 확률로 Q값이 최대인 행동 선택 (Online Network 기준)\n",
        "    x = state_to_onehot(state)                      # 상태를 one-hot으로 변환\n",
        "    _, _, q_values = forward(x, W1, b1, W2, b2)     # Q값 계산\n",
        "    action = int(np.argmax(q_values, axis=1)[0])    # 가장 큰 Q값을 주는 행동 인덱스 선택\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 6. DQN 학습 함수: 리플레이 버퍼에서 미니배치 샘플 → 경사하강\n",
        "# ======================================\n",
        "def train_dqn(batch_size):\n",
        "    global W1, b1, W2, b2   # 전역 파라미터 사용\n",
        "\n",
        "    # 버퍼에서 미니배치 샘플 추출\n",
        "    batch = sample_from_buffer(batch_size)\n",
        "\n",
        "    # 배치를 각 성분별로 나누어 numpy 배열로 변환\n",
        "    states      = np.array([s for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d for (s, a, r, ns, d) in batch], dtype=np.bool_)\n",
        "\n",
        "    # 상태, 다음 상태를 one-hot 벡터로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "\n",
        "    # 온라인 네트워크로 현재 상태의 Q값 계산\n",
        "    z1, a1, q_values = forward(X, W1, b1, W2, b2)                  # q_values: (B, n_actions)\n",
        "\n",
        "    # 타깃 네트워크로 다음 상태의 Q값 계산\n",
        "    _, _, q_values_next_tgt = forward(X_next, W1_tgt, b1_tgt, W2_tgt, b2_tgt)  # (B, n_actions)\n",
        "\n",
        "    # 다음 상태에서의 최대 Q값(max_a' Q_target(s', a')) 추출\n",
        "    max_next_q = np.max(q_values_next_tgt, axis=1)                  # (B,)\n",
        "\n",
        "    # TD Target 계산: done이면 미래 보상 없음\n",
        "    targets = rewards.copy()\n",
        "    not_dones = (~dones)\n",
        "    targets[not_dones] += gamma * max_next_q[not_dones]\n",
        "\n",
        "    # 예측 Q 중에서 실제로 선택한 행동의 Q값만 사용\n",
        "    # dL/d(q_pred) = (q_pred - target) / B  (MSE 기준)\n",
        "    B = batch_size\n",
        "    dL_dz2 = np.zeros_like(q_values)                                # (B, n_actions)\n",
        "    q_pred_selected = q_values[np.arange(B), actions]               # (B,)\n",
        "    td_errors = (q_pred_selected - targets) / B                     # (B,)\n",
        "\n",
        "    # 선택한 행동 위치에만 gradient 반영\n",
        "    dL_dz2[np.arange(B), actions] = td_errors                       # (B, n_actions)\n",
        "\n",
        "    # 2층(출력층) 가중치/편향에 대한 gradient\n",
        "    dW2 = a1.T @ dL_dz2                                             # (hidden_dim x n_actions)\n",
        "    db2 = np.sum(dL_dz2, axis=0, keepdims=True)                     # (1 x n_actions)\n",
        "\n",
        "    # 1층으로 gradient 전파\n",
        "    dL_da1 = dL_dz2 @ W2.T                                          # (B x hidden_dim)\n",
        "    dL_dz1 = dL_da1 * relu_deriv(z1)                                # (B x hidden_dim)\n",
        "\n",
        "    # 1층 가중치/편향에 대한 gradient\n",
        "    dW1 = X.T @ dL_dz1                                              # (input_dim x hidden_dim)\n",
        "    db1 = np.sum(dL_dz1, axis=0, keepdims=True)                     # (1 x hidden_dim)\n",
        "\n",
        "    # 파라미터 업데이트 (경사하강법)\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "# ======================================\n",
        "# 7. DQN 학습 루프\n",
        "# ======================================\n",
        "reward_history = []  # 에피소드별 총 보상을 저장할 리스트\n",
        "total_steps = 0      # 전체 스텝 수(옵션, 여기서는 로그용으로만 사용)\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 DQN(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()          # 에피소드 시작 상태 초기화\n",
        "    total_reward = 0.0       # 에피소드 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "        # (1) ε-greedy 정책으로 행동 선택 (Online Network 기준)\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 리플레이 버퍼에 transition 저장\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # (4) 일정 step 이상 쌓여야 학습 시작 (warmup_steps 이후)\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_dqn(batch_size)\n",
        "\n",
        "        # (5) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (6) 상태를 다음 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # (7) 종료 상태면 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (8) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (9) 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (10) 일정 에피소드마다 타깃 네트워크 파라미터 동기화\n",
        "    if episode % target_update_freq == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        W2_tgt = W2.copy()\n",
        "        b2_tgt = b2.copy()\n",
        "\n",
        "    # (11) 50 에피소드마다 최근 50개 평균 리워드와 현재 epsilon 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 8. 학습된 Online Q-Network로부터 '근사 Q-테이블' 출력\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                        # 상태 s를 one-hot으로 변환\n",
        "    _, _, q_vals = forward(x, W1, b1, W2, b2)     # Q값 계산\n",
        "    q_vals_row = q_vals[0]                        # (1 x n_actions) → (n_actions,)\n",
        "    print(f\"상태 {s}: {q_vals_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 9. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}                 # 행동 인덱스를 화살표로 표현하기 위한 매핑\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                       # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, q_vals = forward(x, W1, b1, W2, b2)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 10. 학습된 정책으로 1회 테스트 실행 (탐험 없이 greedy만 사용)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                       # 초기 상태 0\n",
        "trajectory = [state]                  # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)        # 현재 상태를 one-hot으로 변환\n",
        "    _, _, q_vals = forward(x, W1, b1, W2, b2)  # Q값 계산\n",
        "    action = int(np.argmax(q_vals))   # 탐험 없이 항상 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "    trajectory.append(next_state)     # 방문한 상태 기록\n",
        "    state = next_state                # 상태 업데이트\n",
        "\n",
        "    if done:\n",
        "        break                         # 목표 도달 시 종료\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9cf86cf8-93a3-491d-929d-c09d7962d907",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cf86cf8-93a3-491d-929d-c09d7962d907",
        "outputId": "906e6a05-891c-4ef5-b90a-97959c3abbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Double DQN(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.691, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.859, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.916, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.951, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.955, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.954, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.960, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.967, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.966, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.55874722 0.6816946 ]\n",
            "상태 1: [0.53761728 0.68297569]\n",
            "상태 2: [0.52602922 0.73223193]\n",
            "상태 3: [0.54516807 0.96246224]\n",
            "상태 4: [0.57990547 0.69895713]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-5) Double DQN : Q-Learning의 과대 평가 문제를 완화하기 위한 개선된 버전\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (재현 가능한 결과를 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 이동마다 작은 패널티 부여\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 0 벡터 생성\n",
        "    x[0, state] = 1.0                               # 해당 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. Double DQN용 Q-Network / Target Network 파라미터 정의\n",
        "#    - 간단한 2층 완전연결 신경망 사용\n",
        "# ======================================\n",
        "input_dim = n_states      # 입력 차원: one-hot 상태\n",
        "hidden_dim = 16           # 은닉층 노드 수\n",
        "output_dim = n_actions    # 출력 차원: 각 행동에 대한 Q값 2개\n",
        "\n",
        "# 온라인 네트워크(Online Q-Network) 파라미터\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)   # 1층 가중치 (입력 → 은닉)\n",
        "b1 = np.zeros((1, hidden_dim))                      # 1층 편향\n",
        "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)  # 2층 가중치 (은닉 → 출력)\n",
        "b2 = np.zeros((1, output_dim))                      # 2층 편향\n",
        "\n",
        "# 타깃 네트워크(Target Q-Network) 파라미터 (초기에는 동일하게 복사)\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "W2_tgt = W2.copy()\n",
        "b2_tgt = b2.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 0보다 작으면 0, 크면 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU의 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward(x, W1, b1, W2, b2):\n",
        "    # 신경망 순전파: 입력 x에 대해 Q값을 계산\n",
        "    # x: (배치크기, input_dim) 형태의 상태(one-hot) 벡터\n",
        "    z1 = x @ W1 + b1                # 1층 선형 결합 (배치 x hidden_dim)\n",
        "    a1 = relu(z1)                   # ReLU 활성화 (배치 x hidden_dim)\n",
        "    z2 = a1 @ W2 + b2               # 2층 선형 결합 (배치 x output_dim)\n",
        "    q_values = z2                   # 출력층: 각 행동에 대한 Q값\n",
        "    return z1, a1, q_values         # 역전파를 위해 중간값도 함께 반환\n",
        "\n",
        "# ======================================\n",
        "# 3. Double DQN 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율\n",
        "epsilon = 1.0           # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05      # ε의 최소값\n",
        "epsilon_decay = 0.995   # 에피소드마다 ε 감소 비율\n",
        "learning_rate = 0.01    # 신경망 파라미터 학습률(경사하강 step 크기)\n",
        "\n",
        "n_episodes = 500        # 총 학습 에피소드 수\n",
        "max_steps = 20          # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 1000  # 리플레이 버퍼 최대 크기\n",
        "batch_size = 32         # 미니배치 크기\n",
        "warmup_steps = 100      # 최소 이 정도 샘플이 쌓인 후부터 학습 시작\n",
        "target_update_freq = 20 # 타깃 네트워크를 몇 에피소드마다 한 번씩 갱신할지\n",
        "\n",
        "# ======================================\n",
        "# 4. 리플레이 버퍼 구현 (간단한 리스트 버퍼)\n",
        "# ======================================\n",
        "replay_buffer = []  # (state, action, reward, next_state, done) 튜플을 저장\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 버퍼에 새 transition 추가\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)  # 가장 오래된 데이터 삭제 (FIFO)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    # 버퍼에서 랜덤하게 batch_size 개 샘플 추출\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, replace=False)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    return batch\n",
        "\n",
        "# ======================================\n",
        "# 5. ε-greedy 정책으로 행동 선택 함수 (Online Network 사용)\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 랜덤 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)         # 0 또는 1 중 랜덤 선택\n",
        "\n",
        "    # 1-ε 확률로 Q값이 최대인 행동 선택 (Online Network 기준)\n",
        "    x = state_to_onehot(state)                      # 상태를 one-hot으로 변환\n",
        "    _, _, q_values = forward(x, W1, b1, W2, b2)     # Q값 계산\n",
        "    action = int(np.argmax(q_values, axis=1)[0])    # 가장 큰 Q값을 주는 행동 인덱스 선택\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 6. Double DQN 학습 함수\n",
        "#    - 다음 상태에서 행동 선택은 Online Network\n",
        "#    - 그 행동의 Q값 평가는 Target Network\n",
        "# ======================================\n",
        "def train_double_dqn(batch_size):\n",
        "    global W1, b1, W2, b2   # 전역 파라미터 사용\n",
        "\n",
        "    # 버퍼에서 미니배치 샘플 추출\n",
        "    batch = sample_from_buffer(batch_size)\n",
        "\n",
        "    # 배치를 각 성분별로 나누어 numpy 배열로 변환\n",
        "    states      = np.array([s for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태, 다음 상태를 one-hot 벡터로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "\n",
        "    # 온라인 네트워크로 현재 상태의 Q값 계산\n",
        "    z1, a1, q_values = forward(X, W1, b1, W2, b2)                  # q_values: (B, n_actions)\n",
        "\n",
        "    # 다음 상태에서의 Q값 (Online Network) 계산 → argmax용\n",
        "    _, _, q_values_next_online = forward(X_next, W1, b1, W2, b2)   # (B, n_actions)\n",
        "    best_next_actions = np.argmax(q_values_next_online, axis=1)    # (B,)\n",
        "\n",
        "    # 타깃 네트워크로 다음 상태의 Q값 계산 → 선택된 행동의 값만 사용\n",
        "    _, _, q_values_next_tgt = forward(X_next, W1_tgt, b1_tgt, W2_tgt, b2_tgt)  # (B, n_actions)\n",
        "    next_q_selected = q_values_next_tgt[np.arange(len(batch)), best_next_actions]  # (B,)\n",
        "\n",
        "    # TD Target 계산: done이면 미래 보상 없음\n",
        "    targets = rewards.copy()\n",
        "    not_dones = (~dones)\n",
        "    targets[not_dones] += gamma * next_q_selected[not_dones]\n",
        "\n",
        "    # 예측 Q 중에서 실제로 선택한 행동의 Q값만 사용\n",
        "    B = batch_size\n",
        "    dL_dz2 = np.zeros_like(q_values)                                # (B, n_actions)\n",
        "    q_pred_selected = q_values[np.arange(B), actions]               # (B,)\n",
        "    td_errors = (q_pred_selected - targets) / B                     # (B,)\n",
        "\n",
        "    # 선택한 행동 위치에만 gradient 반영\n",
        "    dL_dz2[np.arange(B), actions] = td_errors                       # (B, n_actions)\n",
        "\n",
        "    # 2층(출력층) 가중치/편향에 대한 gradient\n",
        "    dW2 = a1.T @ dL_dz2                                             # (hidden_dim x n_actions)\n",
        "    db2 = np.sum(dL_dz2, axis=0, keepdims=True)                     # (1 x n_actions)\n",
        "\n",
        "    # 1층으로 gradient 전파\n",
        "    dL_da1 = dL_dz2 @ W2.T                                          # (B x hidden_dim)\n",
        "    dL_dz1 = dL_da1 * relu_deriv(z1)                                # (B x hidden_dim)\n",
        "\n",
        "    # 1층 가중치/편향에 대한 gradient\n",
        "    dW1 = X.T @ dL_dz1                                              # (input_dim x hidden_dim)\n",
        "    db1 = np.sum(dL_dz1, axis=0, keepdims=True)                     # (1 x hidden_dim)\n",
        "\n",
        "    # 파라미터 업데이트 (경사하강법)\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "# ======================================\n",
        "# 7. Double DQN 학습 루프\n",
        "# ======================================\n",
        "reward_history = []  # 에피소드별 총 보상을 저장할 리스트\n",
        "total_steps = 0      # 전체 스텝 수(옵션)\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Double DQN(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()          # 에피소드 시작 상태 초기화\n",
        "    total_reward = 0.0       # 에피소드 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "        # (1) ε-greedy 정책으로 행동 선택 (Online Network 기준)\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 리플레이 버퍼에 transition 저장\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # (4) 일정 step 이상 쌓여야 학습 시작 (warmup_steps 이후)\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_double_dqn(batch_size)\n",
        "\n",
        "        # (5) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (6) 상태를 다음 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # (7) 종료 상태면 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (8) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (9) 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (10) 일정 에피소드마다 타깃 네트워크 파라미터 동기화\n",
        "    if episode % target_update_freq == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        W2_tgt = W2.copy()\n",
        "        b2_tgt = b2.copy()\n",
        "\n",
        "    # (11) 50 에피소드마다 최근 50개 평균 리워드와 현재 epsilon 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 8. 학습된 Online Q-Network로부터 '근사 Q-테이블' 출력\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                        # 상태 s를 one-hot으로 변환\n",
        "    _, _, q_vals = forward(x, W1, b1, W2, b2)     # Q값 계산\n",
        "    q_vals_row = q_vals[0]                        # (1 x n_actions) → (n_actions,)\n",
        "    print(f\"상태 {s}: {q_vals_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 9. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}                 # 행동 인덱스를 화살표로 표현하기 위한 매핑\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                       # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, q_vals = forward(x, W1, b1, W2, b2)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 10. 학습된 정책으로 1회 테스트 실행 (탐험 없이 greedy만 사용)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                       # 초기 상태 0\n",
        "trajectory = [state]                  # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)        # 현재 상태를 one-hot으로 변환\n",
        "    _, _, q_vals = forward(x, W1, b1, W2, b2)  # Q값 계산\n",
        "    action = int(np.argmax(q_vals))   # 탐험 없이 항상 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "    trajectory.append(next_state)     # 방문한 상태 기록\n",
        "    state = next_state                # 상태 업데이트\n",
        "\n",
        "    if done:\n",
        "        break                         # 목표 도달 시 종료\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8a6ff12b-c130-4ef4-b594-37a1ff55fbe2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a6ff12b-c130-4ef4-b594-37a1ff55fbe2",
        "outputId": "32f72fc6-308b-41c8-bdb6-edf3c64180d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Dueling DQN(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.691, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.859, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.916, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.951, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.955, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.954, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.960, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.967, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.966, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.61058338 0.69467441]\n",
            "상태 1: [0.62574346 0.79322378]\n",
            "상태 2: [0.65669643 0.92230427]\n",
            "상태 3: [0.75054659 1.03802841]\n",
            "상태 4: [0.66478158 0.86254989]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-6) Dueling DQN : 상태 가치와 행동의 중요도를 분리하여 Q-값을 계산하는 모델\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (재현 가능한 결과를 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 이동마다 작은 패널티 부여\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 0 벡터 생성\n",
        "    x[0, state] = 1.0                               # 해당 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. Dueling DQN용 Q-Network / Target Network 파라미터 정의\n",
        "#    - 공통 feature 층 + Value Stream + Advantage Stream\n",
        "# ======================================\n",
        "input_dim = n_states      # 입력 차원: one-hot 상태\n",
        "hidden_dim = 16           # 은닉층 노드 수\n",
        "output_dim = n_actions    # 출력 차원: 각 행동에 대한 Q값 2개\n",
        "\n",
        "# 온라인 네트워크(Online Dueling Q-Network) 파라미터\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)    # 1층 가중치 (입력 → 은닉)\n",
        "b1 = np.zeros((1, hidden_dim))                       # 1층 편향\n",
        "\n",
        "# Value Stream: V(s) 출력 (스칼라)\n",
        "Wv = 0.1 * np.random.randn(hidden_dim, 1)            # 은닉 → Value\n",
        "bv = np.zeros((1, 1))                                # Value 편향\n",
        "\n",
        "# Advantage Stream: A(s,a) 출력 (각 행동별)\n",
        "Wa = 0.1 * np.random.randn(hidden_dim, output_dim)   # 은닉 → Advantage\n",
        "ba = np.zeros((1, output_dim))                       # Advantage 편향\n",
        "\n",
        "# 타깃 네트워크(Target Dueling Q-Network) 파라미터 (초기에는 동일하게 복사)\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "Wv_tgt = Wv.copy()\n",
        "bv_tgt = bv.copy()\n",
        "Wa_tgt = Wa.copy()\n",
        "ba_tgt = ba.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 0보다 작으면 0, 크면 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU의 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward(x, W1, b1, Wv, bv, Wa, ba):\n",
        "    # 신경망 순전파: 입력 x에 대해 Q값을 계산\n",
        "    # x: (배치크기, input_dim) 형태의 상태(one-hot) 벡터\n",
        "\n",
        "    # 공통 feature 층 (shared layer)\n",
        "    z1 = x @ W1 + b1                # 1층 선형 결합 (배치 x hidden_dim)\n",
        "    a1 = relu(z1)                   # ReLU 활성화 (배치 x hidden_dim)\n",
        "\n",
        "    # Value Stream: V(s)\n",
        "    z_v = a1 @ Wv + bv              # (배치 x 1)\n",
        "\n",
        "    # Advantage Stream: A(s,a)\n",
        "    z_a = a1 @ Wa + ba              # (배치 x n_actions)\n",
        "\n",
        "    # Advantage의 평균을 빼서 식: Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))\n",
        "    mean_a = np.mean(z_a, axis=1, keepdims=True)  # (배치 x 1)\n",
        "    q_values = z_v + (z_a - mean_a)               # (배치 x n_actions)\n",
        "\n",
        "    # 역전파를 위해 중간값들까지 함께 반환\n",
        "    return z1, a1, z_v, z_a, q_values\n",
        "\n",
        "# ======================================\n",
        "# 3. Dueling DQN 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율\n",
        "epsilon = 1.0           # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05      # ε의 최소값\n",
        "epsilon_decay = 0.995   # 에피소드마다 ε 감소 비율\n",
        "learning_rate = 0.01    # 신경망 파라미터 학습률(경사하강 step 크기)\n",
        "\n",
        "n_episodes = 500        # 총 학습 에피소드 수\n",
        "max_steps = 20          # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 1000  # 리플레이 버퍼 최대 크기\n",
        "batch_size = 32         # 미니배치 크기\n",
        "warmup_steps = 100      # 최소 이 정도 샘플이 쌓인 후부터 학습 시작\n",
        "target_update_freq = 20 # 타깃 네트워크를 몇 에피소드마다 한 번씩 갱신할지\n",
        "\n",
        "# ======================================\n",
        "# 4. 리플레이 버퍼 구현 (간단한 리스트 버퍼)\n",
        "# ======================================\n",
        "replay_buffer = []  # (state, action, reward, next_state, done) 튜플을 저장\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 버퍼에 새 transition 추가\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)  # 가장 오래된 데이터 삭제 (FIFO)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    # 버퍼에서 랜덤하게 batch_size 개 샘플 추출\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, replace=False)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    return batch\n",
        "\n",
        "# ======================================\n",
        "# 5. ε-greedy 정책으로 행동 선택 함수 (Online Network 사용)\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 랜덤 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)         # 0 또는 1 중 랜덤 선택\n",
        "\n",
        "    # 1-ε 확률로 Q값이 최대인 행동 선택 (Online Dueling Network 기준)\n",
        "    x = state_to_onehot(state)                      # 상태를 one-hot으로 변환\n",
        "    _, _, _, _, q_values = forward(x, W1, b1, Wv, bv, Wa, ba)  # Q값 계산\n",
        "    action = int(np.argmax(q_values, axis=1)[0])    # 가장 큰 Q값을 주는 행동 인덱스 선택\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 6. Dueling DQN 학습 함수 (표준 DQN 타깃: max_a Q_target(s', a))\n",
        "# ======================================\n",
        "def train_dueling_dqn(batch_size):\n",
        "    global W1, b1, Wv, bv, Wa, ba   # 전역 파라미터 사용\n",
        "\n",
        "    # 버퍼에서 미니배치 샘플 추출\n",
        "    batch = sample_from_buffer(batch_size)\n",
        "\n",
        "    # 배치를 각 성분별로 나누어 numpy 배열로 변환\n",
        "    states      = np.array([s  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r  for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d  for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태, 다음 상태를 one-hot 벡터로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "\n",
        "    # 온라인 네트워크로 현재 상태의 Q값 계산\n",
        "    z1, a1, z_v, z_a, q_values = forward(X, W1, b1, Wv, bv, Wa, ba)  # q_values: (B, n_actions)\n",
        "\n",
        "    # 타깃 네트워크로 다음 상태의 Q값 계산\n",
        "    _, _, _, _, q_values_next_tgt = forward(X_next, W1_tgt, b1_tgt, Wv_tgt, bv_tgt, Wa_tgt, ba_tgt)  # (B, n_actions)\n",
        "\n",
        "    # 다음 상태에서의 최대 Q값(max_a' Q_target(s', a')) 추출\n",
        "    max_next_q = np.max(q_values_next_tgt, axis=1)                  # (B,)\n",
        "\n",
        "    # TD Target 계산: done이면 미래 보상 없음\n",
        "    targets = rewards.copy()\n",
        "    not_dones = (~dones)\n",
        "    targets[not_dones] += gamma * max_next_q[not_dones]\n",
        "\n",
        "    # 예측 Q 중에서 실제로 선택한 행동의 Q값만 사용\n",
        "    B = batch_size\n",
        "    D = np.zeros_like(q_values)                                     # (B, n_actions)\n",
        "    q_pred_selected = q_values[np.arange(B), actions]               # (B,)\n",
        "    td_errors = (q_pred_selected - targets) / B                     # (B,)\n",
        "\n",
        "    # 선택한 행동 위치에만 gradient 반영\n",
        "    D[np.arange(B), actions] = td_errors                            # dL/dQ (B, n_actions)\n",
        "\n",
        "    # Dueling 구조에 맞는 gradient 분해\n",
        "    # dL/dz_v = Σ_k dL/dQ_k (row-wise sum)\n",
        "    row_sum = np.sum(D, axis=1, keepdims=True)                      # (B, 1)\n",
        "    dL_dz_v = row_sum                                               # (B, 1)\n",
        "\n",
        "    # dL/dz_a = D - (1/|A|) * row_sum  (A 평균을 빼는 부분 반영)\n",
        "    dL_dz_a = D - row_sum / n_actions                               # (B, n_actions)\n",
        "\n",
        "    # Value Stream 파라미터 gradient\n",
        "    dWv = a1.T @ dL_dz_v                                            # (hidden_dim x 1)\n",
        "    dbv = np.sum(dL_dz_v, axis=0, keepdims=True)                    # (1 x 1)\n",
        "\n",
        "    # Advantage Stream 파라미터 gradient\n",
        "    dWa = a1.T @ dL_dz_a                                            # (hidden_dim x n_actions)\n",
        "    dba = np.sum(dL_dz_a, axis=0, keepdims=True)                    # (1 x n_actions)\n",
        "\n",
        "    # 공통 은닉층으로 gradient 전파\n",
        "    # dL/da1 = dL/dz_v * Wv^T + dL/dz_a * Wa^T\n",
        "    dL_da1 = dL_dz_v @ Wv.T + dL_dz_a @ Wa.T                        # (B x hidden_dim)\n",
        "\n",
        "    # ReLU 이전 z1에 대한 gradient\n",
        "    dL_dz1 = dL_da1 * relu_deriv(z1)                                # (B x hidden_dim)\n",
        "\n",
        "    # 1층(공통층) 파라미터 gradient\n",
        "    dW1 = X.T @ dL_dz1                                              # (input_dim x hidden_dim)\n",
        "    db1 = np.sum(dL_dz1, axis=0, keepdims=True)                     # (1 x hidden_dim)\n",
        "\n",
        "    # 파라미터 업데이트 (경사하강법)\n",
        "    Wv -= learning_rate * dWv\n",
        "    bv -= learning_rate * dbv\n",
        "    Wa -= learning_rate * dWa\n",
        "    ba -= learning_rate * dba\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "# ======================================\n",
        "# 7. Dueling DQN 학습 루프\n",
        "# ======================================\n",
        "reward_history = []  # 에피소드별 총 보상을 저장할 리스트\n",
        "total_steps = 0      # 전체 스텝 수(옵션)\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Dueling DQN(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()          # 에피소드 시작 상태 초기화\n",
        "    total_reward = 0.0       # 에피소드 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "        # (1) ε-greedy 정책으로 행동 선택 (Online Dueling Network 기준)\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 리플레이 버퍼에 transition 저장\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # (4) 일정 step 이상 쌓여야 학습 시작 (warmup_steps 이후)\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_dueling_dqn(batch_size)\n",
        "\n",
        "        # (5) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (6) 상태를 다음 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # (7) 종료 상태면 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (8) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (9) 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (10) 일정 에피소드마다 타깃 네트워크 파라미터 동기화\n",
        "    if episode % target_update_freq == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        Wv_tgt = Wv.copy()\n",
        "        bv_tgt = bv.copy()\n",
        "        Wa_tgt = Wa.copy()\n",
        "        ba_tgt = ba.copy()\n",
        "\n",
        "    # (11) 50 에피소드마다 최근 50개 평균 리워드와 현재 epsilon 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 8. 학습된 Online Dueling Q-Network로부터 '근사 Q-테이블' 출력\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                                    # 상태 s를 one-hot으로 변환\n",
        "    _, _, _, _, q_vals = forward(x, W1, b1, Wv, bv, Wa, ba)   # Q값 계산\n",
        "    q_vals_row = q_vals[0]                                    # (1 x n_actions) → (n_actions,)\n",
        "    print(f\"상태 {s}: {q_vals_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 9. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}                             # 행동 인덱스를 화살표로 표현하기 위한 매핑\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                                   # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, _, _, q_vals = forward(x, W1, b1, Wv, bv, Wa, ba)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 10. 학습된 정책으로 1회 테스트 실행 (탐험 없이 greedy만 사용)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                       # 초기 상태 0\n",
        "trajectory = [state]                  # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)        # 현재 상태를 one-hot으로 변환\n",
        "    _, _, _, _, q_vals = forward(x, W1, b1, Wv, bv, Wa, ba)  # Q값 계산\n",
        "    action = int(np.argmax(q_vals))   # 탐험 없이 항상 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "    trajectory.append(next_state)     # 방문한 상태 기록\n",
        "    state = next_state                # 상태 업데이트\n",
        "\n",
        "    if done:\n",
        "        break                         # 목표 도달 시 종료\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "aabb7067-3fe9-4484-83e7-8602141383b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aabb7067-3fe9-4484-83e7-8602141383b9",
        "outputId": "0c15928e-73bf-4a0d-a774-38549dc4c0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 DRQN(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.646, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.815, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.934, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.946, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.953, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.961, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.962, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.963, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.967, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.59828139 0.69852075]\n",
            "상태 1: [0.62906271 0.79959098]\n",
            "상태 2: [0.66495218 0.90457089]\n",
            "상태 3: [0.63253563 0.99507243]\n",
            "상태 4: [0.6295555 0.7800156]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "########################################################################################################\n",
        "## (1-7) DRQN(Deep Recurrent Q-Network) : 순차적인 경험을 학습하기 위해 RNN 구조를 포함한 DQN\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (실행마다 동일한 결과를 얻기 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 한 스텝마다 작은 패널티 부여(빨리 도달하도록 유도)\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 영벡터 생성\n",
        "    x[0, state] = 1.0                               # 현재 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. DRQN용 RNN-Q 네트워크 파라미터 정의\n",
        "#    - 단일 은닉층 RNN + 선형 출력층\n",
        "#    - h_t = tanh(x_t Wxh + h_{t-1} Whh + bh)\n",
        "#    - Q(s_t, a) = h_t Wout + bout\n",
        "# ======================================\n",
        "input_dim  = n_states   # 입력 차원: 상태를 one-hot으로 표시하므로 n_states\n",
        "hidden_dim = 16         # RNN 은닉 상태 차원\n",
        "output_dim = n_actions  # 출력 차원: 각 행동에 대한 Q값 2개\n",
        "\n",
        "# 입력 → 은닉 가중치 (Wxh)\n",
        "Wxh = 0.1 * np.random.randn(input_dim, hidden_dim)   # (input_dim x hidden_dim)\n",
        "# 은닉 → 은닉 가중치 (Whh)\n",
        "Whh = 0.1 * np.random.randn(hidden_dim, hidden_dim)  # (hidden_dim x hidden_dim)\n",
        "# 은닉 편향 (bh)\n",
        "bh  = np.zeros((1, hidden_dim))                      # (1 x hidden_dim)\n",
        "\n",
        "# 은닉 → 출력(Q값) 가중치 (Wout)\n",
        "Wout = 0.1 * np.random.randn(hidden_dim, output_dim) # (hidden_dim x output_dim)\n",
        "# 출력 편향 (bout)\n",
        "bout = np.zeros((1, output_dim))                     # (1 x output_dim)\n",
        "\n",
        "def rnn_step(x, h_prev):\n",
        "    # RNN 한 스텝 순전파\n",
        "    # x      : (1 x input_dim)  현재 상태의 one-hot 벡터\n",
        "    # h_prev : (1 x hidden_dim) 직전 시점의 은닉 상태\n",
        "    # 반환값 : h_next(다음 은닉 상태), q_values(현재 상태에서의 각 행동 Q값)\n",
        "\n",
        "    # 1층 선형 결합: z = x Wxh + h_prev Whh + bh\n",
        "    z = x @ Wxh + h_prev @ Whh + bh      # (1 x hidden_dim)\n",
        "\n",
        "    # tanh 활성화: h_next = tanh(z)\n",
        "    h_next = np.tanh(z)                  # (1 x hidden_dim)\n",
        "\n",
        "    # 출력층: q_values = h_next Wout + bout\n",
        "    q_values = h_next @ Wout + bout      # (1 x output_dim)\n",
        "\n",
        "    return z, h_next, q_values\n",
        "\n",
        "# ======================================\n",
        "# 3. DRQN 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율(미래 보상 반영 비율)\n",
        "epsilon = 1.0           # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05      # ε의 최소값\n",
        "epsilon_decay = 0.995   # 에피소드마다 ε 감소 비율\n",
        "learning_rate = 0.01    # 신경망 파라미터 학습률(경사하강 step 크기)\n",
        "\n",
        "n_episodes = 500        # 총 학습 에피소드 수\n",
        "max_steps  = 20         # 한 에피소드에서 최대 스텝 수 (무한 루프 방지)\n",
        "\n",
        "# ======================================\n",
        "# 4. ε-greedy 정책으로 행동 선택\n",
        "#    - 입력: 현재 상태, 현재 은닉 상태\n",
        "#    - 출력: 선택된 행동, 그때의 Q값, 업데이트된 은닉 상태\n",
        "# ======================================\n",
        "def choose_action_with_rnn(state, h_prev, epsilon):\n",
        "    # 1) 상태를 one-hot 벡터로 변환\n",
        "    x = state_to_onehot(state)                   # (1 x n_states)\n",
        "\n",
        "    # 2) RNN 한 스텝 순전파\n",
        "    z, h_next, q_values = rnn_step(x, h_prev)   # h_next: 업데이트된 은닉 상태\n",
        "\n",
        "    # 3) ε-greedy로 행동 선택\n",
        "    if np.random.rand() < epsilon:\n",
        "        action = np.random.randint(n_actions)    # 탐험: 무작위 행동 선택\n",
        "    else:\n",
        "        action = int(np.argmax(q_values))        # 이용: Q값이 최대인 행동 선택\n",
        "\n",
        "    return action, z, h_next, q_values\n",
        "\n",
        "# ======================================\n",
        "# 5. DRQN 학습 루프\n",
        "#    - 각 스텝마다 TD(0) 손실에 대해 단일 스텝 BPTT(역전파)\n",
        "#    - h_prev까지는 gradient를 전파하지 않는 truncation(1-step BPTT)\n",
        "# ======================================\n",
        "reward_history = []  # 에피소드별 총 보상 기록용 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 DRQN(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()                              # 에피소드 시작 시 상태 초기화 (0)\n",
        "    h_prev = np.zeros((1, hidden_dim))          # 에피소드 시작 시 은닉 상태 0으로 초기화\n",
        "    total_reward = 0.0                          # 에피소드별 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        # (1) 현재 상태와 은닉 상태를 이용해 ε-greedy로 행동 선택\n",
        "        action, z, h, q_values = choose_action_with_rnn(state, h_prev, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 다음 상태에서의 Q값을 계산하여 TD Target 구성\n",
        "        #     여기서는 target용 RNN 한 스텝을 별도로 수행 (탐험 없이 greedy 기준)\n",
        "        x_next = state_to_onehot(next_state)                   # 다음 상태 one-hot\n",
        "        z_next, h_next, q_next = rnn_step(x_next, h)           # 다음 시점 은닉 상태, Q값\n",
        "\n",
        "        # done이면 미래 보상 없음, 아니면 max_a' Q(s', a') 반영\n",
        "        if done:\n",
        "            td_target = reward                                 # 목표 상태 도달 시\n",
        "        else:\n",
        "            td_target = reward + gamma * np.max(q_next)        # TD Target = r + γ max Q'\n",
        "\n",
        "        # (4) 현재 시점에서 선택한 행동의 Q값 추출\n",
        "        q_pred = q_values[0, action]                           # 스칼라 값\n",
        "\n",
        "        # (5) 손실: L = 0.5 * (q_pred - td_target)^2\n",
        "        #     dL/dq_pred = (q_pred - td_target)\n",
        "        dL_dq = q_pred - td_target                             # 스칼라 gradient\n",
        "\n",
        "        # (6) 출력층(Wout, bout)에 대한 gradient\n",
        "        #     q_values = h @ Wout + bout\n",
        "        #     dL/dWout = h^T @ dL/dq (단, 선택한 action에만 반영)\n",
        "        dQ = np.zeros_like(q_values)                           # (1 x output_dim)\n",
        "        dQ[0, action] = dL_dq                                  # 선택한 action 위치에만 gradient\n",
        "\n",
        "        dWout = h.T @ dQ                                       # (hidden_dim x output_dim)\n",
        "        dbout = dQ                                             # (1 x output_dim)\n",
        "\n",
        "        # (7) 은닉 상태 h에 대한 gradient\n",
        "        #     dL/dh = dQ @ Wout^T\n",
        "        dL_dh = dQ @ Wout.T                                    # (1 x hidden_dim)\n",
        "\n",
        "        # (8) tanh 이전 z에 대한 gradient\n",
        "        #     h = tanh(z) 이므로 dh/dz = (1 - tanh(z)^2)\n",
        "        dh_dz = 1.0 - np.tanh(z) ** 2                          # (1 x hidden_dim)\n",
        "        dL_dz = dL_dh * dh_dz                                  # (1 x hidden_dim)\n",
        "\n",
        "        # (9) 입력층(Wxh), 순환층(Whh), 편향(bh)에 대한 gradient\n",
        "        x = state_to_onehot(state)                             # 현재 상태 one-hot (1 x input_dim)\n",
        "\n",
        "        dWxh = x.T @ dL_dz                                     # (input_dim x hidden_dim)\n",
        "        dWhh = h_prev.T @ dL_dz                                # (hidden_dim x hidden_dim)\n",
        "        dbh  = dL_dz                                           # (1 x hidden_dim)\n",
        "\n",
        "        # (10) 경사하강법으로 파라미터 업데이트\n",
        "        Wout -= learning_rate * dWout\n",
        "        bout -= learning_rate * dbout\n",
        "        Wxh  -= learning_rate * dWxh\n",
        "        Whh  -= learning_rate * dWhh\n",
        "        bh   -= learning_rate * dbh\n",
        "\n",
        "        # (11) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (12) 다음 스텝을 위해 상태와 은닉 상태를 업데이트\n",
        "        state  = next_state\n",
        "        h_prev = h                                             # 바로 이전 은닉 상태를 현재 h로 설정\n",
        "\n",
        "        # (13) 목표에 도달하면 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (14) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (15) 에피소드별 총 보상 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (16) 50 에피소드마다 최근 50개 평균 리워드와 현재 epsilon 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 6. 학습된 DRQN에서 '근사 Q-테이블' 출력\n",
        "#    - 은닉 상태를 0으로 두고, 각 상태를 독립적으로 넣어서 Q값을 본다\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    # 각 상태별로 은닉 상태를 0에서 시작한다고 가정\n",
        "    h0 = np.zeros((1, hidden_dim))                      # 초기 은닉 상태\n",
        "    x  = state_to_onehot(s)                             # 상태 s의 one-hot\n",
        "    z, h, q_vals = rnn_step(x, h0)                      # 한 스텝 순전파\n",
        "    q_vals_row = q_vals[0]                              # (1 x output_dim) → (output_dim,)\n",
        "    print(f\"상태 {s}: {q_vals_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 7. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "#    - 에피소드 실행 시에는 은닉 상태가 계속 이어진다는 점이 DRQN의 특징\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}                       # 행동 인덱스를 화살표로 표현하기 위한 매핑\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                             # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        h0 = np.zeros((1, hidden_dim))                  # 정책만 볼 때는 h=0 기준으로 계산\n",
        "        x  = state_to_onehot(s)\n",
        "        z, h, q_vals = rnn_step(x, h0)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 8. 학습된 정책으로 1회 에피소드 실행 (탐험 없이 greedy만 사용)\n",
        "#    - 여기서는 은닉 상태를 실제로 시퀀스 전체에 걸쳐 사용\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                               # 초기 상태 0\n",
        "h_prev = np.zeros((1, hidden_dim))           # 은닉 상태 0으로 초기화\n",
        "trajectory = [state]                          # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)               # 현재 상태를 one-hot으로 변환\n",
        "    z, h, q_vals = rnn_step(x, h_prev)       # RNN 한 스텝 순전파\n",
        "    action = int(np.argmax(q_vals))          # 탐험 없이 항상 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "    trajectory.append(next_state)            # 방문한 상태 기록\n",
        "\n",
        "    # 다음 스텝을 위해 상태와 은닉 상태 업데이트\n",
        "    state  = next_state\n",
        "    h_prev = h\n",
        "\n",
        "    if done:\n",
        "        break                                # 목표 도달 시 종료\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "40b5fadf-d238-4972-91f7-e8d51423b788",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40b5fadf-d238-4972-91f7-e8d51423b788",
        "outputId": "57f87050-1a23-4c8b-ba03-3c43795bdbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 C51 (Categorical DQN, NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.781, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.858, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.690, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.300, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = -0.029, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = -0.076, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = -0.179, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = -0.200, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = -0.116, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.362, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [-0.04470164 -0.04139259]\n",
            "상태 1: [-0.0460864  -0.04379602]\n",
            "상태 2: [-0.04630476 -0.04427904]\n",
            "상태 3: [-0.04213484 -0.04170734]\n",
            "상태 4: [-0.03890941 -0.03502329]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-8) C51(Categorical DQN 51) : Q-값의 분포를 학습하는 DQN 확장\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (재현 가능한 결과를 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5     # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2    # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 한 스텝마다 작은 패널티 부여\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 0 벡터 생성\n",
        "    x[0, state] = 1.0                               # 해당 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. C51(51-atom Categorical DQN) 설정\n",
        "#    - Vmin, Vmax: 가치 분포의 최소/최대 값\n",
        "#    - n_atoms   : 원자(atom) 개수\n",
        "# ======================================\n",
        "n_atoms = 51                    # C51 논문에서 제안한 atom 개수\n",
        "Vmin = -1.0                     # 최소 가치 (이 환경에 맞춰 대략 설정)\n",
        "Vmax =  1.0                     # 최대 가치\n",
        "delta_z = (Vmax - Vmin) / (n_atoms - 1)          # 인접 atom 간 간격\n",
        "z_support = np.linspace(Vmin, Vmax, n_atoms)     # shape: (n_atoms,)\n",
        "\n",
        "# ======================================\n",
        "# 3. C51용 Q-Network / Target Network 정의\n",
        "#    - 입력: 상태(one-hot)\n",
        "#    - 출력: 각 행동별 원자 분포의 로짓 (logits) → softmax로 확률 분포\n",
        "# ======================================\n",
        "input_dim = n_states                    # 상태 차원 (one-hot)\n",
        "hidden_dim = 16                         # 은닉층 크기\n",
        "output_dim = n_actions * n_atoms        # 각 행동마다 n_atoms개의 확률 → 총 출력차원\n",
        "\n",
        "# 온라인 네트워크(Online C51 Q-Network) 파라미터\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)   # 1층 가중치 (입력 → 은닉)\n",
        "b1 = np.zeros((1, hidden_dim))                      # 1층 편향\n",
        "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)  # 2층 가중치 (은닉 → 행동×원자)\n",
        "b2 = np.zeros((1, output_dim))                      # 2층 편향\n",
        "\n",
        "# 타깃 네트워크(Target C51 Q-Network) 파라미터 (초기에는 동일하게 복사)\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "W2_tgt = W2.copy()\n",
        "b2_tgt = b2.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 0보다 작으면 0, 크면 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU의 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    # Softmax 함수: 숫자 안정성 확보 위해 최대값을 빼고 exp 계산\n",
        "    x_shifted = x - np.max(x, axis=axis, keepdims=True)  # overflow 방지\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n",
        "    return exp_x / (sum_exp + 1e-8)                      # 0으로 나눌 위험 방지용 작은 값 추가\n",
        "\n",
        "def forward(x, W1, b1, W2, b2):\n",
        "    # C51 네트워크 순전파\n",
        "    # x           : (배치크기, input_dim)\n",
        "    # 반환값      : z1, h, logits(배치 x 행동 x 원자), probs(동일 크기)\n",
        "    z1 = x @ W1 + b1                             # 1층 선형 결합 (배치 x hidden_dim)\n",
        "    h  = relu(z1)                                # ReLU 활성화 (배치 x hidden_dim)\n",
        "\n",
        "    logits_flat = h @ W2 + b2                    # (배치 x (n_actions * n_atoms))\n",
        "    batch_size = x.shape[0]\n",
        "    logits = logits_flat.reshape(batch_size, n_actions, n_atoms)  # (배치 x 행동 x 원자)\n",
        "\n",
        "    probs = softmax(logits, axis=2)              # atom 차원(마지막 축)에 대해 softmax → 확률 분포\n",
        "    return z1, h, logits, probs\n",
        "\n",
        "# ======================================\n",
        "# 4. C51 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율\n",
        "epsilon = 1.0           # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05      # ε의 최소값\n",
        "epsilon_decay = 0.995   # 에피소드마다 ε 감소 비율\n",
        "learning_rate = 0.001   # 신경망 파라미터 학습률 (C51이므로 조금 작게 설정)\n",
        "\n",
        "n_episodes = 500        # 총 학습 에피소드 수\n",
        "max_steps = 20          # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 1000  # 리플레이 버퍼 최대 크기\n",
        "batch_size = 32         # 미니배치 크기\n",
        "warmup_steps = 100      # 최소 이 정도 샘플이 쌓인 후부터 학습 시작\n",
        "target_update_freq = 20 # 타깃 네트워크를 몇 에피소드마다 한 번씩 갱신할지\n",
        "\n",
        "# ======================================\n",
        "# 5. 리플레이 버퍼 구현 (간단한 리스트 버퍼)\n",
        "# ======================================\n",
        "replay_buffer = []  # (state, action, reward, next_state, done) 튜플을 저장\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 버퍼에 새 transition 추가\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)  # 가장 오래된 데이터 삭제 (FIFO)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    # 버퍼에서 랜덤하게 batch_size 개 샘플 추출\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, replace=False)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    return batch\n",
        "\n",
        "# ======================================\n",
        "# 6. ε-greedy 정책으로 행동 선택 (C51 분포 → 기대값 Q로 변환 후 사용)\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 랜덤 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)         # 0 또는 1 중 랜덤 선택\n",
        "\n",
        "    # 1-ε 확률로 분포의 기대값(∑ z * p)을 기준으로 Q값을 계산하고, 그 중 최대 행동 선택\n",
        "    x = state_to_onehot(state)                      # 상태를 one-hot으로 변환\n",
        "    _, _, _, probs = forward(x, W1, b1, W2, b2)     # (1 x 행동 x 원자) 크기의 확률 분포\n",
        "\n",
        "    # 각 행동별 기대 Q값 계산: Q(a) = Σ_j z_j * p(a, j)\n",
        "    q_values = np.sum(z_support[None, None, :] * probs, axis=2)  # (1 x 행동)\n",
        "    action = int(np.argmax(q_values, axis=1)[0])                 # Q값이 최대인 행동 선택\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 7. C51의 핵심: 분포 프로젝션 함수\n",
        "#    - r + γ z_j 를 [Vmin, Vmax] 구간의 atom들로 분배하는 함수\n",
        "# ======================================\n",
        "def project_distribution(rewards, dones, next_probs):\n",
        "    # rewards    : (B,)        - 보상 r\n",
        "    # dones      : (B,) bool   - 종료 여부\n",
        "    # next_probs : (B, n_atoms) - 다음 상태에서의 선택된 행동 a*에 대한 분포 p(z'|s',a*)\n",
        "    # 반환값     : (B, n_atoms) - 현재 상태에서의 타깃 분포 m(z)\n",
        "    batch_size = rewards.shape[0]\n",
        "\n",
        "    # Tz = r + γ z (단, 종료 상태면 Tz = r)\n",
        "    # rewards[:, None] : (B x 1)\n",
        "    # z_support[None,:]: (1 x n_atoms)\n",
        "    Tz = rewards[:, None] + gamma * z_support[None, :] * (1.0 - dones[:, None].astype(np.float32))\n",
        "    Tz = np.clip(Tz, Vmin, Vmax)   # Vmin, Vmax 범위로 클리핑\n",
        "\n",
        "    # b = (Tz - Vmin) / Δz  (atom index 실수값)\n",
        "    b = (Tz - Vmin) / delta_z\n",
        "    l = np.floor(b).astype(int)\n",
        "    u = np.ceil(b).astype(int)\n",
        "\n",
        "    # 인덱스는 [0, n_atoms-1] 범위로 잘라줌\n",
        "    l = np.clip(l, 0, n_atoms - 1)\n",
        "    u = np.clip(u, 0, n_atoms - 1)\n",
        "\n",
        "    # 최종 타깃 분포 m(z) 초기화\n",
        "    m = np.zeros((batch_size, n_atoms), dtype=np.float32)\n",
        "\n",
        "    # 각 atom j에 대해 m의 해당 위치에 분배\n",
        "    for j in range(n_atoms):\n",
        "        # 각 배치에서 l_ij, u_ij 위치에 확률을 나누어 더함\n",
        "        lj = l[:, j]          # (B,)\n",
        "        uj = u[:, j]          # (B,)\n",
        "        bj = b[:, j]          # (B,)\n",
        "        pj = next_probs[:, j] # (B,)  현재 atom j의 확률\n",
        "\n",
        "        # m_{l} += p_j * (u - b)\n",
        "        m[np.arange(batch_size), lj] += pj * (uj - bj)\n",
        "        # m_{u} += p_j * (b - l)\n",
        "        m[np.arange(batch_size), uj] += pj * (bj - lj)\n",
        "\n",
        "    # 수치적 안정성을 위해 정규화(합이 1이 되도록)\n",
        "    m_sum = np.sum(m, axis=1, keepdims=True)\n",
        "    m = m / (m_sum + 1e-8)\n",
        "    return m\n",
        "\n",
        "# ======================================\n",
        "# 8. C51 학습 함수 (리플레이 버퍼에서 미니배치 샘플 → 분포 업데이트)\n",
        "# ======================================\n",
        "def train_c51(batch_size):\n",
        "    global W1, b1, W2, b2   # 전역 파라미터 사용\n",
        "\n",
        "    # 버퍼에서 미니배치 샘플 추출\n",
        "    batch = sample_from_buffer(batch_size)\n",
        "\n",
        "    # 배치를 각 성분별로 나누어 numpy 배열로 변환\n",
        "    states      = np.array([s  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r  for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d  for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태, 다음 상태를 one-hot 벡터로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "\n",
        "    batch_size = X.shape[0]\n",
        "\n",
        "    # 온라인 네트워크로 현재 상태의 로짓/분포 계산\n",
        "    z1, h, logits, probs = forward(X, W1, b1, W2, b2)              # probs: (B, 행동, 원자)\n",
        "\n",
        "    # 타깃 네트워크로 다음 상태의 분포 계산\n",
        "    _, _, _, next_probs_all = forward(X_next, W1_tgt, b1_tgt, W2_tgt, b2_tgt)  # (B, 행동, 원자)\n",
        "\n",
        "    # 다음 상태에서 각 행동별 기대 Q값 계산\n",
        "    # Q_next(a) = Σ z * p(a,z)\n",
        "    q_next = np.sum(z_support[None, None, :] * next_probs_all, axis=2)  # (B, 행동)\n",
        "\n",
        "    # Double DQN 스타일: 다음 상태에서 행동 선택은 온라인 네트워크로 해도 되지만,\n",
        "    # 여기서는 간단히 타깃 네트워크 기준으로 max Q 행동 선택\n",
        "    best_next_actions = np.argmax(q_next, axis=1)                  # (B,)\n",
        "\n",
        "    # 선택된 행동 a*에 대한 다음 상태의 분포 p(z|s',a*)\n",
        "    next_probs = next_probs_all[np.arange(batch_size), best_next_actions, :]  # (B, n_atoms)\n",
        "\n",
        "    # C51 분포 프로젝션으로 타깃 분포 m 계산\n",
        "    target_dist = project_distribution(rewards, dones, next_probs)           # (B, n_atoms)\n",
        "\n",
        "    # 현재 상태에서 선택된 행동 a에 대한 분포 p(z|s,a)\n",
        "    current_probs = probs[np.arange(batch_size), actions, :]                 # (B, n_atoms)\n",
        "\n",
        "    # cross-entropy 손실 L = - Σ m(z) * log p(z)\n",
        "    # softmax + cross-entropy의 gradient: dL/dlogits = p - m\n",
        "    # 따라서, 선택된 행동에 대해서만 gradient p - m을 반영\n",
        "    d_logits = np.zeros_like(logits)                                         # (B, 행동, 원자)\n",
        "    d_logits[np.arange(batch_size), actions, :] = (current_probs - target_dist)  # (B, 원자)\n",
        "\n",
        "    # 2층(출력층) gradient 계산\n",
        "    d_logits_flat = d_logits.reshape(batch_size, -1)                         # (B, 행동*원자)\n",
        "    dW2 = h.T @ d_logits_flat                                                # (hidden_dim x (행동*원자))\n",
        "    db2 = np.sum(d_logits_flat, axis=0, keepdims=True)                       # (1 x (행동*원자))\n",
        "\n",
        "    # 은닉층으로 gradient 전파\n",
        "    d_h = d_logits_flat @ W2.T                                               # (B x hidden_dim)\n",
        "    d_z1 = d_h * relu_deriv(z1)                                              # (B x hidden_dim)\n",
        "\n",
        "    # 1층(입력층) gradient 계산\n",
        "    dW1 = X.T @ d_z1                                                         # (input_dim x hidden_dim)\n",
        "    db1 = np.sum(d_z1, axis=0, keepdims=True)                                # (1 x hidden_dim)\n",
        "\n",
        "    # 파라미터 업데이트 (경사하강법)\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "# ======================================\n",
        "# 9. C51 학습 루프\n",
        "# ======================================\n",
        "reward_history = []  # 에피소드별 총 보상을 저장할 리스트\n",
        "total_steps = 0      # 전체 스텝 수(옵션)\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 C51 (Categorical DQN, NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()          # 에피소드 시작 상태 초기화\n",
        "    total_reward = 0.0       # 에피소드 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "        # (1) ε-greedy 정책으로 행동 선택 (분포 기대값 기준 Q 사용)\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 리플레이 버퍼에 transition 저장\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # (4) 일정 step 이상 쌓여야 학습 시작 (warmup_steps 이후)\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_c51(batch_size)\n",
        "\n",
        "        # (5) 보상 누적\n",
        "        total_reward += reward\n",
        "\n",
        "        # (6) 상태를 다음 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # (7) 종료 상태면 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (8) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (9) 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (10) 일정 에피소드마다 타깃 네트워크 파라미터 동기화\n",
        "    if episode % target_update_freq == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        W2_tgt = W2.copy()\n",
        "        b2_tgt = b2.copy()\n",
        "\n",
        "    # (11) 50 에피소드마다 최근 50개 평균 리워드와 현재 epsilon 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 10. 학습된 C51 분포로부터 '근사 Q-테이블' 출력\n",
        "#     - 각 상태별로 분포를 forward 후, 기대값 Q(s,a)을 계산\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                                         # 상태 s를 one-hot으로 변환\n",
        "    _, _, _, probs = forward(x, W1, b1, W2, b2)                    # (1 x 행동 x 원자)\n",
        "    q_vals = np.sum(z_support[None, None, :] * probs, axis=2)      # (1 x 행동)\n",
        "    q_row = q_vals[0]                                              # (행동,)\n",
        "    print(f\"상태 {s}: {q_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 11. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}                                   # 행동 인덱스를 화살표로 표현\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                                         # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, _, probs = forward(x, W1, b1, W2, b2)\n",
        "        q_vals = np.sum(z_support[None, None, :] * probs, axis=2)   # (1 x 행동)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 12. 학습된 정책으로 1회 에피소드 실행 (탐험 없이 greedy만 사용)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                               # 초기 상태 0\n",
        "trajectory = [state]                          # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)\n",
        "    _, _, _, probs = forward(x, W1, b1, W2, b2)\n",
        "    q_vals = np.sum(z_support[None, None, :] * probs, axis=2)  # (1 x 행동)\n",
        "    action = int(np.argmax(q_vals))                            # 탐험 없이 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fdd23fd3-2d98-4490-a183-70598235c130",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdd23fd3-2d98-4490-a183-70598235c130",
        "outputId": "ceacca51-4040-4edb-ede2-bd9deb4642ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 IQN(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.602, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.807, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.919, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.949, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.953, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.955, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.962, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.965, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.964, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.966, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [-0.03301757  0.13893495]\n",
            "상태 1: [-0.00705666  0.21028906]\n",
            "상태 2: [0.01188504 0.21389171]\n",
            "상태 3: [-0.03812291  0.23077639]\n",
            "상태 4: [0.04196164 0.20656024]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-9) IQN(Implicit Quantile Networks) : Q-값의 분포를 세밀하게 조정하는 방식\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (실행마다 동일한 결과를 얻기 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "# ======================================\n",
        "n_states = 5   # 상태 개수: 0,1,2,3,4  (4가 목표 상태)\n",
        "n_actions = 2  # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 왼쪽 끝(0) 아래로 내려가지 않도록 제한\n",
        "    # action이 1이면 오른쪽으로 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 오른쪽 끝(4) 위로 넘어가지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0                                # 목표 도달 보상 +1\n",
        "        done = True                                 # 에피소드 종료\n",
        "    # 그 외의 경우\n",
        "    else:\n",
        "        reward = -0.01                              # 한 스텝마다 작은 패널티 (빨리 도달하도록 유도)\n",
        "        done = False                                # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                 # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)   # 1행 n_states열의 0 벡터 생성\n",
        "    x[0, state] = 1.0                               # 해당 상태 인덱스 위치만 1로 설정\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. IQN(Implicit Quantile Network) 구조 정의\n",
        "#    - 입력: 상태(one-hot) + 샘플링된 quantile τ (스칼라)\n",
        "#    - 출력: 각 행동별 quantile 값 Q(s,a | τ)\n",
        "# ======================================\n",
        "input_dim  = n_states + 1    # 상태 one-hot(n_states) + τ(1)\n",
        "hidden_dim = 32              # 은닉층 크기\n",
        "output_dim = n_actions       # 출력 차원: 각 행동별 quantile 값\n",
        "\n",
        "# 온라인 네트워크(Online IQN) 파라미터\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)   # 1층 가중치 (입력 → 은닉)\n",
        "b1 = np.zeros((1, hidden_dim))                      # 1층 편향\n",
        "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)  # 2층 가중치 (은닉 → 행동별 quantile 값)\n",
        "b2 = np.zeros((1, output_dim))                      # 2층 편향\n",
        "\n",
        "# 타깃 네트워크(Target IQN) 파라미터 (초기에는 온라인과 동일하게 설정)\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "W2_tgt = W2.copy()\n",
        "b2_tgt = b2.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 0보다 작으면 0, 크면 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward_iqn(states, taus, W1, b1, W2, b2):\n",
        "    # IQN 순전파 함수\n",
        "    # states : (B, n_states)       - 배치 상태(one-hot)\n",
        "    # taus   : (B, N, 1)           - 각 배치에 대해 N개의 quantile τ 샘플\n",
        "    # 반환값 : inp_flat, z1, h, q  (q: (B, N, n_actions))\n",
        "    B = states.shape[0]                 # 배치 크기\n",
        "    N = taus.shape[1]                   # quantile 샘플 개수\n",
        "\n",
        "    # 상태를 (B, 1, n_states) → (B, N, n_states) 로 반복 확장\n",
        "    state_expanded = np.repeat(states[:, None, :], N, axis=1)  # (B, N, n_states)\n",
        "\n",
        "    # 상태와 τ를 concat → (B, N, n_states + 1)\n",
        "    inp = np.concatenate([state_expanded, taus], axis=2)\n",
        "\n",
        "    # (B, N, input_dim) → (B*N, input_dim) 로 평탄화\n",
        "    inp_flat = inp.reshape(B * N, -1)\n",
        "\n",
        "    # 1층: 선형 + ReLU\n",
        "    z1 = inp_flat @ W1 + b1            # (B*N, hidden_dim)\n",
        "    h  = relu(z1)                      # (B*N, hidden_dim)\n",
        "\n",
        "    # 2층: 선형 출력 (각 행동별 quantile 값)\n",
        "    out = h @ W2 + b2                  # (B*N, n_actions)\n",
        "\n",
        "    # 다시 (B, N, n_actions) 형태로 reshape\n",
        "    q = out.reshape(B, N, output_dim)\n",
        "\n",
        "    return inp_flat, z1, h, q\n",
        "\n",
        "# ======================================\n",
        "# 3. IQN 하이퍼파라미터 설정\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율\n",
        "epsilon = 1.0           # ε-greedy에서 탐험 비율 시작값\n",
        "epsilon_min = 0.05      # ε의 최소값\n",
        "epsilon_decay = 0.995   # 에피소드마다 ε 감소 비율\n",
        "\n",
        "learning_rate = 0.001   # 신경망 파라미터 학습률\n",
        "n_episodes = 500        # 총 학습 에피소드 수\n",
        "max_steps  = 20         # 한 에피소드에서 최대 스텝 수\n",
        "\n",
        "buffer_capacity   = 1000  # 리플레이 버퍼 최대 크기\n",
        "batch_size        = 32    # 미니배치 크기\n",
        "warmup_steps      = 100   # 최소 이 정도 샘플이 쌓인 후부터 학습 시작\n",
        "target_update_epi = 50    # 타깃 네트워크를 몇 에피소드마다 업데이트할지\n",
        "\n",
        "n_quantiles_train = 32    # 학습 시 사용할 quantile 샘플 개수\n",
        "n_quantiles_eval  = 128   # 평가(Q-테이블 출력) 시 사용할 quantile 샘플 개수\n",
        "\n",
        "# ======================================\n",
        "# 4. 리플레이 버퍼 구현 (간단한 리스트 버퍼)\n",
        "# ======================================\n",
        "replay_buffer = []  # (state, action, reward, next_state, done) 튜플 저장\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 버퍼가 가득 찼으면 가장 오래된 데이터 삭제\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)\n",
        "    # 새 transition 추가\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    # 버퍼에서 랜덤하게 batch_size 개의 transition 샘플 추출\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, replace=False)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    return batch\n",
        "\n",
        "# ======================================\n",
        "# 5. ε-greedy 정책 (IQN 분포의 기대값을 이용)\n",
        "# ======================================\n",
        "def choose_action(state, epsilon):\n",
        "    # ε 확률로 랜덤 행동(탐험)\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "\n",
        "    # 1-ε 확률로 분포의 기대값(평균)을 기준으로 greedy 행동 선택\n",
        "    x = state_to_onehot(state)                                        # (1, n_states)\n",
        "    B = 1\n",
        "    N = n_quantiles_train                                             # 기대값 계산용 τ 샘플 개수\n",
        "    taus = np.random.rand(B, N, 1).astype(np.float32)                 # (1, N, 1), U(0,1)에서 샘플링\n",
        "\n",
        "    _, _, _, q = forward_iqn(x, taus, W1, b1, W2, b2)                 # q: (1, N, n_actions)\n",
        "\n",
        "    q_mean = np.mean(q, axis=1)                                       # (1, n_actions)  → 각 행동별 기대값\n",
        "    action = int(np.argmax(q_mean, axis=1)[0])                        # 기대값이 가장 큰 행동 선택\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 6. IQN 학습 함수\n",
        "#    - quantile regression + Huber loss 사용\n",
        "# ======================================\n",
        "def train_iqn(batch_size):\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    # 리플레이 버퍼에서 미니배치 샘플 추출\n",
        "    batch = sample_from_buffer(batch_size)\n",
        "\n",
        "    # 각 성분별로 분리하여 numpy 배열로 변환\n",
        "    states      = np.array([s  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r  for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d  for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태들을 one-hot 벡터로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "\n",
        "    B = batch_size                       # 배치 크기\n",
        "    N = n_quantiles_train                # 학습 시 quantile 샘플 개수\n",
        "\n",
        "    # 현재 상태용 quantile τ 샘플링 (U(0,1))\n",
        "    taus      = np.random.rand(B, N, 1).astype(np.float32)         # (B, N, 1)\n",
        "    # 다음 상태용 quantile τ 샘플링 (타깃용)\n",
        "    taus_next = np.random.rand(B, N, 1).astype(np.float32)         # (B, N, 1)\n",
        "\n",
        "    # 온라인 네트워크로 현재 상태의 quantile 분포 계산\n",
        "    inp_flat, z1, h, q = forward_iqn(X, taus, W1, b1, W2, b2)      # q: (B, N, n_actions)\n",
        "\n",
        "    # 현재 상태에서 선택된 행동에 대한 quantile 값만 추출 (B, N)\n",
        "    batch_idx = np.arange(B)[:, None]                              # (B, 1)\n",
        "    tau_idx   = np.arange(N)[None, :]                              # (1, N)\n",
        "    q_pred = q[batch_idx, tau_idx, actions[:, None]]               # (B, N)\n",
        "\n",
        "    # 타깃 네트워크로 다음 상태의 quantile 분포 계산\n",
        "    _, _, _, q_next_all = forward_iqn(X_next, taus_next, W1_tgt, b1_tgt, W2_tgt, b2_tgt)  # (B, N, n_actions)\n",
        "\n",
        "    # 다음 상태에서 각 행동별 기대 Q값(평균) 계산\n",
        "    q_next_mean = np.mean(q_next_all, axis=1)                      # (B, n_actions)\n",
        "\n",
        "    # 다음 상태에서 greedy 행동 선택 (Double DQN 스타일의 행동 선택)\n",
        "    best_next_actions = np.argmax(q_next_mean, axis=1)             # (B,)\n",
        "\n",
        "    # 선택된 greedy 행동에 대한 다음 상태의 quantile 값만 추출 (B, N)\n",
        "    q_next = q_next_all[batch_idx, tau_idx, best_next_actions[:, None]]  # (B, N)\n",
        "\n",
        "    # done이면 미래 보상이 없으므로 (1 - done) 곱해줌\n",
        "    not_dones = (~dones).astype(np.float32)                        # (B,)\n",
        "\n",
        "    # TD Target: Z_target = r + γ * (1 - done) * Z_next\n",
        "    Z_target = rewards[:, None] + gamma * not_dones[:, None] * q_next   # (B, N)\n",
        "\n",
        "    # δ = Z_target - Z_pred  (quantile 간 오차)\n",
        "    delta = Z_target - q_pred                                      # (B, N)\n",
        "\n",
        "    # quantile fraction τ (각 샘플마다 다름)\n",
        "    taus_flat = taus.squeeze(-1)                                   # (B, N)\n",
        "\n",
        "    # δ < 0 인지에 따른 indicator (quantile regression의 비대칭 가중치 요소)\n",
        "    indicator = (delta < 0).astype(np.float32)                     # (B, N)\n",
        "\n",
        "    # Huber 손실의 파라미터 kappa\n",
        "    kappa = 1.0\n",
        "\n",
        "    # Huber 손실 값 계산\n",
        "    abs_delta = np.abs(delta)                                      # (B, N)\n",
        "    huber = np.where(abs_delta <= kappa,\n",
        "                     0.5 * delta**2,\n",
        "                     kappa * (abs_delta - 0.5 * kappa))           # (B, N)\n",
        "\n",
        "    # IQN의 quantile 가중치: |τ - 1_{δ<0}|\n",
        "    weight = np.abs(taus_flat - indicator)                         # (B, N)\n",
        "\n",
        "    # 최종 loss 요소 = weight * huber  (여기서는 mean(loss_elements)를 쓰지만,\n",
        "    # backward에서 평균을 고려하여 gradient에 1/(B*N) 반영)\n",
        "    # loss_elements = weight * huber\n",
        "    # L = mean(loss_elements)\n",
        "\n",
        "    # Huber 손실의 도함수: d huber / dδ\n",
        "    grad_huber = np.where(abs_delta <= kappa,\n",
        "                          delta,\n",
        "                          kappa * np.sign(delta))                  # (B, N)\n",
        "\n",
        "    # L = (1/(B*N)) * Σ weight * huber 이므로\n",
        "    # dL/dδ = (1/(B*N)) * weight * d huber/dδ\n",
        "    BN = B * N\n",
        "    dL_ddelta = weight * grad_huber / BN                           # (B, N)\n",
        "\n",
        "    # δ = Z_target - Z_pred 이므로 dL/dZ_pred = - dL/dδ\n",
        "    dL_dZ_pred = -dL_ddelta                                        # (B, N)\n",
        "\n",
        "    # q (B, N, n_actions) 에서 선택된 action 위치에만 gradient를 반영\n",
        "    dQ = np.zeros_like(q)                                          # (B, N, n_actions)\n",
        "    dQ[batch_idx, tau_idx, actions[:, None]] = dL_dZ_pred          # (B, N)\n",
        "\n",
        "    # 이제 dQ를 이용해 신경망 파라미터에 대한 gradient 계산\n",
        "    # (B, N, n_actions) → (B*N, n_actions) 로 reshape\n",
        "    dQ_flat = dQ.reshape(BN, output_dim)                           # (B*N, n_actions)\n",
        "\n",
        "    # 2층(출력층) gradient: out = h @ W2 + b2\n",
        "    dW2 = h.T @ dQ_flat                                            # (hidden_dim x n_actions)\n",
        "    db2 = np.sum(dQ_flat, axis=0, keepdims=True)                   # (1 x n_actions)\n",
        "\n",
        "    # 은닉층에 전달되는 gradient\n",
        "    dh = dQ_flat @ W2.T                                            # (B*N, hidden_dim)\n",
        "\n",
        "    # 1층 z1에 대한 gradient (ReLU 도함수 곱)\n",
        "    dz1 = dh * relu_deriv(z1)                                      # (B*N, hidden_dim)\n",
        "\n",
        "    # 1층(입력층) gradient: z1 = inp_flat @ W1 + b1\n",
        "    dW1 = inp_flat.T @ dz1                                         # (input_dim x hidden_dim)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)                       # (1 x hidden_dim)\n",
        "\n",
        "    # 경사하강법으로 파라미터 업데이트\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "# ======================================\n",
        "# 7. IQN 학습 루프\n",
        "# ======================================\n",
        "reward_history = []   # 에피소드별 총 보상을 기록할 리스트\n",
        "total_steps    = 0    # 전체 스텝 수 (옵션)\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 IQN(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()                     # 에피소드 시작 상태(0) 초기화\n",
        "    total_reward = 0.0                  # 에피소드 누적 보상 초기화\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        total_steps += 1\n",
        "\n",
        "        # (1) ε-greedy 정책으로 행동 선택\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # (2) 선택한 행동을 환경에 적용 → 다음 상태, 보상, 종료 여부 반환\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # (3) 리플레이 버퍼에 transition 저장\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # (4) 일정 step 이상 샘플이 쌓였으면 학습 시작\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_iqn(batch_size)\n",
        "\n",
        "        # (5) 누적 보상 업데이트\n",
        "        total_reward += reward\n",
        "\n",
        "        # (6) 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # (7) 목표 상태에 도달한 경우 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # (8) 에피소드 종료 후 epsilon 감소 (탐험 비율을 점점 줄임)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # (9) 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # (10) 일정 에피소드마다 타깃 네트워크를 업데이트하고 로그 출력\n",
        "    if episode % target_update_epi == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        W2_tgt = W2.copy()\n",
        "        b2_tgt = b2.copy()\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 8. 학습된 IQN으로 '근사 Q-테이블' 출력\n",
        "#    - 여러 τ를 샘플링하여 기대값 Q(s,a)를 추정\n",
        "# ======================================\n",
        "print(\"▶ 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                                            # (1, n_states)\n",
        "    B = 1\n",
        "    N = n_quantiles_eval                                              # 평가용 quantile 샘플 개수\n",
        "    taus = np.random.rand(B, N, 1).astype(np.float32)                 # (1, N, 1)\n",
        "\n",
        "    _, _, _, q = forward_iqn(x, taus, W1, b1, W2, b2)                 # (1, N, n_actions)\n",
        "    q_mean = np.mean(q, axis=1)[0]                                    # (n_actions,)  기대값\n",
        "\n",
        "    print(f\"상태 {s}: {q_mean}\")\n",
        "\n",
        "# ======================================\n",
        "# 9. 학습된 정책(Policy) 확인 (greedy 정책)\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}                                     # 행동 인덱스를 화살표로 표현\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                                           # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        B = 1\n",
        "        N = n_quantiles_eval\n",
        "        taus = np.random.rand(B, N, 1).astype(np.float32)\n",
        "        _, _, _, q = forward_iqn(x, taus, W1, b1, W2, b2)\n",
        "        q_mean = np.mean(q, axis=1)                                   # (1, n_actions)\n",
        "        best_action = int(np.argmax(q_mean, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 10. 학습된 정책으로 1회 에피소드 실행 (탐험 없이 greedy만 사용)\n",
        "# ======================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                               # 초기 상태 0\n",
        "trajectory = [state]                          # 방문한 상태들을 저장할 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "\n",
        "    x = state_to_onehot(state)\n",
        "    B = 1\n",
        "    N = n_quantiles_eval\n",
        "    taus = np.random.rand(B, N, 1).astype(np.float32)\n",
        "    _, _, _, q = forward_iqn(x, taus, W1, b1, W2, b2)\n",
        "    q_mean = np.mean(q, axis=1)               # (1, n_actions)\n",
        "    action = int(np.argmax(q_mean, axis=1)[0])# 탐험 없이 항상 greedy 행동 선택\n",
        "\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7c14ac8f-f793-4e38-893d-3c8450b9ec05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c14ac8f-f793-4e38-893d-3c8450b9ec05",
        "outputId": "f7a190b5-4aae-429d-cf9b-b56263fd0070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Rainbow (Double + Dueling + PER + NoisyNet + C51, NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.741\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.779\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.652\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.442\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.284\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.059\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.110\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.728\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.908\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.893\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ Rainbow 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [-0.01080802 -0.01070711]\n",
            "상태 1: [-0.01080802 -0.01070711]\n",
            "상태 2: [-0.01080802 -0.01070711]\n",
            "상태 3: [-0.01080802 -0.01070711]\n",
            "상태 4: [-0.01080802 -0.01070711]\n",
            "\n",
            "▶ 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ Rainbow 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-10) Rainbow : 여러 DQN 확장(PER, Double DQN, C51 등)을 결합한 통합 알고리즘 (DeepMind, 2017)\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# 난수 시드 고정 (재현 가능한 결과를 위해)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================\n",
        "# 1. 환경 설정 (1차원 선형 월드)\n",
        "#    상태: 0,1,2,3,4  (4가 목표 상태)\n",
        "#    행동: 0=왼쪽, 1=오른쪽\n",
        "# ======================================\n",
        "n_states = 5\n",
        "n_actions = 2\n",
        "\n",
        "def step(state, action):\n",
        "    # action이 0이면 왼쪽으로 한 칸 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)\n",
        "    # action이 1이면 오른쪽으로 한 칸 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    # 목표 상태(4)에 도달한 경우\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0      # 목표 도달 보상\n",
        "        done = True       # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01    # 매 스텝마다 작은 패널티\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 항상 상태 0에서 에피소드 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 상태를 one-hot 벡터(1 x n_states)로 변환\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)\n",
        "    x[0, state] = 1.0\n",
        "    return x\n",
        "\n",
        "# ======================================\n",
        "# 2. Rainbow에서 사용할 C51 설정\n",
        "#    - 분포형 값함수(Categorical DQN)\n",
        "#    - 가치 범위 [Vmin, Vmax]를 n_atoms개 구간으로 나눔\n",
        "# ======================================\n",
        "n_atoms = 51                 # atom 개수\n",
        "Vmin = -1.0                  # 최소 가치\n",
        "Vmax =  1.0                  # 최대 가치\n",
        "delta_z = (Vmax - Vmin) / (n_atoms - 1)  # atom 간격\n",
        "z_support = np.linspace(Vmin, Vmax, n_atoms)  # atom 값들 (shape: (n_atoms,))\n",
        "\n",
        "# ======================================\n",
        "# 3. Rainbow용 네트워크 구조\n",
        "#    - 입력: 상태 one-hot (5차원)\n",
        "#    - 은닉층: Noisy Linear (고정 sigma, 학습되는 W,b)\n",
        "#    - 출력: Dueling 구조 + C51 분포\n",
        "#      * Value stream  : V(s, z) (각 atom에 대한 값)\n",
        "#      * Advantage stream: A(s, a, z)\n",
        "#      * Q 분포: V + (A - 평균(A))\n",
        "# ======================================\n",
        "input_dim  = n_states       # 상태 one-hot 차원\n",
        "hidden_dim = 32             # 은닉층 크기\n",
        "\n",
        "# Noisy layer의 sigma는 여기서는 \"고정\"으로 두고, W,b만 학습\n",
        "noisy_sigma_1   = 0.5       # 1층 노이즈 스케일\n",
        "noisy_sigma_val = 0.5       # Value stream 노이즈 스케일\n",
        "noisy_sigma_adv = 0.5       # Advantage stream 노이즈 스케일\n",
        "\n",
        "# ----- 온라인 네트워크 파라미터 -----\n",
        "# 1층: 입력 → 은닉 (Noisy Linear)\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)\n",
        "b1 = np.zeros((1, hidden_dim))\n",
        "W1_sigma = noisy_sigma_1 * np.ones_like(W1)  # 고정 sigma\n",
        "\n",
        "# Value stream: 은닉 → atom 값 (Noisy Linear)\n",
        "W_val = 0.1 * np.random.randn(hidden_dim, n_atoms)\n",
        "b_val = np.zeros((1, n_atoms))\n",
        "W_val_sigma = noisy_sigma_val * np.ones_like(W_val)\n",
        "\n",
        "# Advantage stream: 은닉 → (행동 × atom) (Noisy Linear)\n",
        "W_adv = 0.1 * np.random.randn(hidden_dim, n_actions * n_atoms)\n",
        "b_adv = np.zeros((1, n_actions * n_atoms))\n",
        "W_adv_sigma = noisy_sigma_adv * np.ones_like(W_adv)\n",
        "\n",
        "# ----- 타깃 네트워크 파라미터 (초기에는 동일하게 설정) -----\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "W_val_tgt = W_val.copy()\n",
        "b_val_tgt = b_val.copy()\n",
        "W_adv_tgt = W_adv.copy()\n",
        "b_adv_tgt = b_adv.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def noisy_linear(x, W, b, W_sigma, use_noise=True):\n",
        "    # Noisy Linear 레이어:\n",
        "    # y = x @ (W + W_sigma * eps) + b\n",
        "    # - eps는 N(0,1)에서 샘플\n",
        "    # - 여기서는 sigma는 고정, W만 학습\n",
        "    if use_noise:\n",
        "        eps = np.random.randn(*W.shape)          # W와 같은 shape의 노이즈\n",
        "        W_eff = W + W_sigma * eps                # 유효 가중치\n",
        "    else:\n",
        "        W_eff = W                                # 타깃 네트워크 등에서 noise 없이 사용\n",
        "\n",
        "    y = x @ W_eff + b                            # 선형 연산\n",
        "    return y, W_eff                              # y와, 역전파에 사용할 W_eff 반환\n",
        "\n",
        "def forward_rainbow(x, use_noise=True, target=False):\n",
        "    # Rainbow 네트워크 순전파\n",
        "    # x          : (배치, input_dim)\n",
        "    # use_noise  : Noisy layer 사용 여부\n",
        "    # target     : 타깃 네트워크 사용 여부\n",
        "    # 반환값     : h, logits, probs\n",
        "    if not target:\n",
        "        # 온라인 네트워크 사용\n",
        "        z1, W1_eff = noisy_linear(x, W1, b1, W1_sigma, use_noise=use_noise)  # (B, hidden)\n",
        "        h = relu(z1)                                                         # (B, hidden)\n",
        "\n",
        "        v_logits, W_val_eff = noisy_linear(h, W_val, b_val, W_val_sigma, use_noise=use_noise)   # (B, n_atoms)\n",
        "        adv_flat_logits, W_adv_eff = noisy_linear(h, W_adv, b_adv, W_adv_sigma, use_noise=use_noise)  # (B, n_actions*n_atoms)\n",
        "    else:\n",
        "        # 타깃 네트워크 사용 (noise 없이)\n",
        "        z1 = x @ W1_tgt + b1_tgt                              # (B, hidden)\n",
        "        h = relu(z1)                                          # (B, hidden)\n",
        "\n",
        "        v_logits = h @ W_val_tgt + b_val_tgt                  # (B, n_atoms)\n",
        "        adv_flat_logits = h @ W_adv_tgt + b_adv_tgt           # (B, n_actions*n_atoms)\n",
        "        W1_eff = W1_tgt\n",
        "        W_val_eff = W_val_tgt\n",
        "        W_adv_eff = W_adv_tgt\n",
        "\n",
        "    batch_size = x.shape[0]                                   # 배치 크기\n",
        "    adv_logits = adv_flat_logits.reshape(batch_size, n_actions, n_atoms)  # (B, A, Z)\n",
        "\n",
        "    v_logits_expanded = v_logits[:, None, :]                  # (B, 1, Z)\n",
        "    adv_mean = np.mean(adv_logits, axis=1, keepdims=True)     # (B, 1, Z)\n",
        "\n",
        "    logits = v_logits_expanded + (adv_logits - adv_mean)      # Dueling 결합 (B, A, Z)\n",
        "\n",
        "    # Softmax를 atom 축(Z) 방향으로 적용하여 확률 분포로 변환\n",
        "    logits_shifted = logits - np.max(logits, axis=2, keepdims=True)  # overflow 방지\n",
        "    exp_logits = np.exp(logits_shifted)\n",
        "    probs = exp_logits / (np.sum(exp_logits, axis=2, keepdims=True) + 1e-8)  # (B, A, Z)\n",
        "\n",
        "    return h, logits, probs, W1_eff, W_val_eff, W_adv_eff, z1, v_logits, adv_logits\n",
        "\n",
        "# ======================================\n",
        "# 4. Prioritized Replay Buffer (PER)\n",
        "#    - Rainbow 구성 요소: PER (Proportional)\n",
        "# ======================================\n",
        "buffer_capacity = 1000     # 최대 버퍼 크기\n",
        "replay_buffer = []         # (s, a, r, ns, done) 튜플 저장\n",
        "priorities = []            # 각 transition의 priority 저장\n",
        "\n",
        "alpha = 0.6                # PER에서 priority 지수 (0=uniform, 1=greedy)\n",
        "beta = 0.4                 # IS(importance sampling) 지수\n",
        "priority_eps = 1e-6        # priority가 0이 되지 않도록 하는 작은 값\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 새로운 transition을 리플레이 버퍼에 추가\n",
        "    # priority는 현재까지의 최대 priority로 초기화\n",
        "    if len(priorities) > 0:\n",
        "        max_prio = max(priorities)\n",
        "    else:\n",
        "        max_prio = 1.0\n",
        "\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        # 버퍼가 가득 찼으면 FIFO로 가장 오래된 것 제거\n",
        "        replay_buffer.pop(0)\n",
        "        priorities.pop(0)\n",
        "\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    priorities.append(max_prio)\n",
        "\n",
        "def sample_from_buffer_per(batch_size):\n",
        "    # PER(Proportional)에 따라 미니배치 샘플링\n",
        "    prios = np.array(priorities, dtype=np.float32)\n",
        "    # priority^alpha\n",
        "    scaled_prios = prios ** alpha\n",
        "    # 확률 분포로 정규화\n",
        "    P = scaled_prios / (np.sum(scaled_prios) + 1e-8)\n",
        "\n",
        "    # 인덱스를 확률 P에 따라 샘플\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, p=P, replace=False)\n",
        "\n",
        "    # IS weight 계산: w_i = (N * P(i))^-beta / max_i(...)\n",
        "    N = len(replay_buffer)\n",
        "    weights = (N * P[indices]) ** (-beta)\n",
        "    weights = weights / (np.max(weights) + 1e-8)\n",
        "    weights = weights.astype(np.float32)\n",
        "\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    return batch, indices, weights\n",
        "\n",
        "# ======================================\n",
        "# 5. C51 분포 프로젝션 함수\n",
        "#    - r + γ z 를 [Vmin, Vmax] 구간의 atom들로 분배\n",
        "# ======================================\n",
        "def project_distribution(rewards, dones, next_probs):\n",
        "    # rewards : (B,)       - 보상\n",
        "    # dones   : (B,) bool  - 종료 여부\n",
        "    # next_probs : (B, n_atoms) - 다음 상태 분포 p(z|s',a*)\n",
        "    # 반환값 : (B, n_atoms) - 현재 상태 분포의 타깃 m(z)\n",
        "    batch_size = rewards.shape[0]\n",
        "\n",
        "    # 종료 상태면 미래 보상 없음, 아니면 r + γ z\n",
        "    Tz = rewards[:, None] + (1.0 - dones.astype(np.float32))[:, None] * (gamma * z_support[None, :])\n",
        "    Tz = np.clip(Tz, Vmin, Vmax)           # [Vmin, Vmax] 범위로 자름\n",
        "\n",
        "    b = (Tz - Vmin) / delta_z              # atom index의 실수 위치\n",
        "    l = np.floor(b).astype(int)\n",
        "    u = np.ceil(b).astype(int)\n",
        "\n",
        "    l = np.clip(l, 0, n_atoms - 1)\n",
        "    u = np.clip(u, 0, n_atoms - 1)\n",
        "\n",
        "    m = np.zeros((batch_size, n_atoms), dtype=np.float32)\n",
        "\n",
        "    for j in range(n_atoms):\n",
        "        lj = l[:, j]\n",
        "        uj = u[:, j]\n",
        "        bj = b[:, j]\n",
        "        pj = next_probs[:, j]\n",
        "\n",
        "        m[np.arange(batch_size), lj] += pj * (uj - bj)\n",
        "        m[np.arange(batch_size), uj] += pj * (bj - lj)\n",
        "\n",
        "    m_sum = np.sum(m, axis=1, keepdims=True)\n",
        "    m = m / (m_sum + 1e-8)\n",
        "    return m\n",
        "\n",
        "# ======================================\n",
        "# 6. 하이퍼파라미터 설정 (Rainbow 스타일)\n",
        "#    - Double DQN + Dueling + PER + NoisyNet + C51\n",
        "# ======================================\n",
        "gamma = 0.9             # 할인율\n",
        "learning_rate = 0.0005  # 학습률 (조금 작게)\n",
        "n_episodes = 500        # 에피소드 수\n",
        "max_steps  = 20         # 에피소드당 최대 스텝 수\n",
        "\n",
        "batch_size       = 32   # 미니배치 크기\n",
        "warmup_steps     = 100  # 최소 이 정도 샘플 쌓인 후 학습 시작\n",
        "target_update_ep = 20   # 타깃 네트워크 업데이트 주기(에피소드 단위)\n",
        "\n",
        "# Rainbow에서는 보통 NoisyNet으로 탐험을 하므로 epsilon 거의 사용하지 않음\n",
        "epsilon = 0.0           # ε-greedy는 0으로 두고, NoisyNet만으로 탐험\n",
        "epsilon_min = 0.0\n",
        "epsilon_decay = 1.0     # 사용하지 않지만 형식상 둠\n",
        "\n",
        "# ======================================\n",
        "# 7. 행동 선택 함수 (NoisyNet + 분포 기대값 기반 greedy)\n",
        "# ======================================\n",
        "def choose_action(state):\n",
        "    # 상태를 one-hot으로 변환\n",
        "    x = state_to_onehot(state)                         # (1, n_states)\n",
        "    # 온라인 네트워크 순전파 (Noisy 사용)\n",
        "    _, _, probs, _, _, _, _, _, _ = forward_rainbow(x, use_noise=True, target=False)\n",
        "    # 기대 Q값 계산: Q(a) = Σ_z z * p(z|s,a)\n",
        "    q_values = np.sum(z_support[None, None, :] * probs, axis=2)  # (1, n_actions)\n",
        "    action = int(np.argmax(q_values, axis=1)[0])\n",
        "    return action\n",
        "\n",
        "# ======================================\n",
        "# 8. Rainbow 학습 함수\n",
        "# ======================================\n",
        "def train_rainbow(batch_size):\n",
        "    global W1, b1, W_val, b_val, W_adv, b_adv\n",
        "    global W1_tgt, b1_tgt, W_val_tgt, b_val_tgt, W_adv_tgt, b_adv_tgt\n",
        "    global priorities\n",
        "\n",
        "    # PER에서 미니배치 샘플\n",
        "    batch, indices, is_weights = sample_from_buffer_per(batch_size)\n",
        "\n",
        "    # 배치를 각 성분별로 분리\n",
        "    states      = np.array([s  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r  for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d  for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태를 one-hot으로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "    B = X.shape[0]\n",
        "\n",
        "    batch_idx = np.arange(B)\n",
        "\n",
        "    # ----- 1) 온라인 네트워크로 현재 상태 분포 계산 -----\n",
        "    h, logits, probs, W1_eff, W_val_eff, W_adv_eff, z1, v_logits, adv_logits = forward_rainbow(\n",
        "        X, use_noise=True, target=False\n",
        "    )  # probs: (B, A, Z)\n",
        "\n",
        "    # 현재 상태에서 수행한 행동 a에 대한 분포 p(z|s,a)\n",
        "    current_probs = probs[batch_idx, actions, :]                   # (B, n_atoms)\n",
        "\n",
        "    # ----- 2) Double DQN: 다음 상태에서 온라인 네트워크로 행동 선택 -----\n",
        "    _, _, probs_next_online, _, _, _, _, _, _ = forward_rainbow(\n",
        "        X_next, use_noise=True, target=False\n",
        "    )  # (B, A, Z)\n",
        "\n",
        "    q_next_online = np.sum(z_support[None, None, :] * probs_next_online, axis=2)  # (B, A)\n",
        "    best_next_actions = np.argmax(q_next_online, axis=1)                          # (B,)\n",
        "\n",
        "    # ----- 3) 타깃 네트워크에서 선택된 행동의 분포 가져오기 -----\n",
        "    _, _, probs_next_target, _, _, _, _, _, _ = forward_rainbow(\n",
        "        X_next, use_noise=False, target=True\n",
        "    )  # (B, A, Z)\n",
        "\n",
        "    next_dist = probs_next_target[batch_idx, best_next_actions, :]               # (B, n_atoms)\n",
        "\n",
        "    # ----- 4) C51 분포 프로젝션으로 타깃 분포 계산 -----\n",
        "    target_dist = project_distribution(rewards, dones, next_dist)               # (B, n_atoms)\n",
        "\n",
        "    # ----- 5) 손실 및 gradient 계산 (Cross-Entropy + IS weight) -----\n",
        "    # cross-entropy: L = - Σ m(z) * log p(z)\n",
        "    # softmax + cross-entropy의 gradient: dL/dlogits = p - m\n",
        "    # PER의 IS weight w_i를 곱해줌\n",
        "    is_w = is_weights[:, None]                                                  # (B, 1)\n",
        "\n",
        "    d_logits = np.zeros_like(logits)                                            # (B, A, Z)\n",
        "    diff = (current_probs - target_dist) * is_w                                 # (B, Z)\n",
        "    d_logits[batch_idx, actions, :] = diff                                      # (B, Z)를 해당 action 위치에 배치\n",
        "\n",
        "    # ----- 6) Dueling 구조에 따른 gradient 분리 -----\n",
        "    # logits = V + (Adv - mean_adv)\n",
        "    # ∂L/∂V = Σ_a ∂L/∂logits_a\n",
        "    # ∂L/∂Adv_a = ∂L/∂logits_a - 1/A Σ_a' ∂L/∂logits_a'\n",
        "    dV = np.sum(d_logits, axis=1)                                               # (B, Z)\n",
        "    mean_d_logits = np.mean(d_logits, axis=1, keepdims=True)                    # (B, 1, Z)\n",
        "    dAdv = d_logits - mean_d_logits                                             # (B, A, Z)\n",
        "\n",
        "    # ----- 7) Value/Advantage stream에 대한 gradient -----\n",
        "    dV_flat = dV                                                                # (B, Z)\n",
        "    dAdv_flat = dAdv.reshape(B, n_actions * n_atoms)                            # (B, A*Z)\n",
        "\n",
        "    dW_val = h.T @ dV_flat                                                      # (hidden x Z)\n",
        "    db_val = np.sum(dV_flat, axis=0, keepdims=True)                             # (1 x Z)\n",
        "\n",
        "    dW_adv = h.T @ dAdv_flat                                                    # (hidden x A*Z)\n",
        "    db_adv = np.sum(dAdv_flat, axis=0, keepdims=True)                           # (1 x A*Z)\n",
        "\n",
        "    # 은닉층에 대한 gradient: dh = dV @ W_val_eff^T + dAdv_flat @ W_adv_eff^T\n",
        "    dh = dV_flat @ W_val_eff.T + dAdv_flat @ W_adv_eff.T                        # (B, hidden)\n",
        "\n",
        "    # 1층 z1에 대한 gradient (ReLU 도함수 곱)\n",
        "    dz1 = dh * relu_deriv(z1)                                                   # (B, hidden)\n",
        "\n",
        "    # 1층에 대한 gradient: dW1, db1\n",
        "    dW1 = X.T @ dz1                                                             # (input_dim x hidden)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)                                    # (1 x hidden)\n",
        "\n",
        "    # ----- 8) 파라미터 업데이트 (경사하강법) -----\n",
        "    W_val -= learning_rate * dW_val\n",
        "    b_val -= learning_rate * db_val\n",
        "\n",
        "    W_adv -= learning_rate * dW_adv\n",
        "    b_adv -= learning_rate * db_adv\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # ----- 9) PER priority 업데이트 -----\n",
        "    # priority는 |p - m|의 합으로 정의(간단한 형태)\n",
        "    td_errors = np.sum(np.abs(current_probs - target_dist), axis=1)             # (B,)\n",
        "    new_priorities = td_errors + priority_eps\n",
        "\n",
        "    for idx_buf, pr in zip(indices, new_priorities):\n",
        "        priorities[idx_buf] = pr\n",
        "\n",
        "# ======================================\n",
        "# 9. Rainbow 학습 루프\n",
        "# ======================================\n",
        "reward_history = []\n",
        "total_steps = 0\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Rainbow (Double + Dueling + PER + NoisyNet + C51, NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "\n",
        "    state = reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        total_steps += 1\n",
        "\n",
        "        # 1) NoisyNet 기반 행동 선택 (ε-greedy는 사용하지 않음)\n",
        "        action = choose_action(state)\n",
        "\n",
        "        # 2) 환경에 행동 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) 리플레이 버퍼에 저장\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # 4) 충분한 샘플이 쌓이면 Rainbow 학습 수행\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_rainbow(batch_size)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # 5) 타깃 네트워크 파라미터 주기적으로 동기화\n",
        "    if episode % target_update_ep == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        W_val_tgt = W_val.copy()\n",
        "        b_val_tgt = b_val.copy()\n",
        "        W_adv_tgt = W_adv.copy()\n",
        "        b_adv_tgt = b_adv.copy()\n",
        "\n",
        "    # 6) 50 에피소드마다 로그 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ======================================\n",
        "# 10. Rainbow 근사 Q-테이블 출력\n",
        "#      - 각 상태에서 분포의 기대값으로 Q(s,a) 추정\n",
        "# ======================================\n",
        "print(\"▶ Rainbow 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)\n",
        "    _, _, probs, _, _, _, _, _, _ = forward_rainbow(x, use_noise=False, target=False)\n",
        "    q_vals = np.sum(z_support[None, None, :] * probs, axis=2)  # (1, A)\n",
        "    q_row = q_vals[0]\n",
        "    print(f\"상태 {s}: {q_row}\")\n",
        "\n",
        "# ======================================\n",
        "# 11. 학습된 정책(Policy) 확인\n",
        "# ======================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, probs, _, _, _, _, _, _ = forward_rainbow(x, use_noise=False, target=False)\n",
        "        q_vals = np.sum(z_support[None, None, :] * probs, axis=2)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ======================================\n",
        "# 12. Rainbow 정책으로 1회 에피소드 실행 예시\n",
        "# ======================================\n",
        "print(\"\\n▶ Rainbow 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    x = state_to_onehot(state)\n",
        "    _, _, probs, _, _, _, _, _, _ = forward_rainbow(x, use_noise=False, target=False)\n",
        "    q_vals = np.sum(z_support[None, None, :] * probs, axis=2)\n",
        "    action = int(np.argmax(q_vals, axis=1)[0])\n",
        "\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2ceb7c57-19cc-499d-9b08-4c79b56a3ef1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ceb7c57-19cc-499d-9b08-4c79b56a3ef1",
        "outputId": "75864499-b4a7-4090-a410-f1c2229368c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Soft Q-Learning(SQL) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.600\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.569\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.456\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.486\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.601\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.556\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.531\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.400\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.444\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.611\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ 최종 Soft Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.29254955 0.27151373]\n",
            "상태 1: [0.29254955 0.22254117]\n",
            "상태 2: [0.27151343 0.07617304]\n",
            "상태 3: [0.2223115  0.99999976]\n",
            "상태 4: [0. 0.]\n",
            "\n",
            "▶ 학습된 정책(Policy: greedy w.r.t Soft Q)\n",
            "상태 0  1  2  3  4\n",
            "      ←  ←  ←  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "스텝 수: 20\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "########################################################################################################\n",
        "## (1-11) SQL(Soft Q-Learning): 엔트로피를 추가하여 Q값 학습을 안정화하는 방식\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# =====================================================\n",
        "# 0. 난수 시드 고정 (실행마다 같은 결과가 나오도록)\n",
        "# =====================================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# =====================================================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "#    - 상태: 0, 1, 2, 3, 4 (4가 목표 상태)\n",
        "#    - 행동: 0=왼쪽, 1=오른쪽\n",
        "# =====================================================\n",
        "n_states = 5   # 상태 개수\n",
        "n_actions = 2  # 행동 개수: 0(←), 1(→)\n",
        "\n",
        "def step(state, action):\n",
        "    # 현재 상태 state에서 action을 했을 때, 다음 상태와 보상을 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)              # 0보다 작아지지 않도록 처리\n",
        "    # 행동이 1이면 오른쪽으로 한 칸 이동\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)   # 4보다 커지지 않도록 처리\n",
        "\n",
        "    # 만약 목표 상태(4)에 도달했다면\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0        # 목표 도달 보상\n",
        "        done = True         # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01      # 매 스텝마다 작은 패널티(빨리 도달하도록 유도)\n",
        "        done = False        # 아직 종료 아님\n",
        "\n",
        "    return next_state, reward, done  # (다음 상태, 보상, 종료 여부)\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "# =====================================================\n",
        "# 2. Soft Q-Learning (Maximum Entropy RL) 설정\n",
        "#    - Q(s,a): 상태-행동 가치 함수 (표 형태)\n",
        "#    - 정책: softmax( Q(s,·) / tau )  (탐험은 tau로 제어)\n",
        "# =====================================================\n",
        "Q = np.zeros((n_states, n_actions), dtype=np.float32)  # Q 테이블 0으로 초기화\n",
        "\n",
        "alpha = 0.1      # 학습률 (learning rate)\n",
        "gamma = 0.9      # 할인율 (discount factor)\n",
        "tau   = 0.5      # 소프트맥스 온도(temperature), 크면 탐험↑, 작으면 greedy에 가까움\n",
        "\n",
        "n_episodes = 500 # 총 학습 에피소드 수\n",
        "max_steps  = 20  # 한 에피소드에서 최대 스텝 수(무한루프 방지용)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Softmax 기반 정책 함수 π(a|s)\n",
        "#    - Q값을 소프트맥스로 확률로 변환해서 행동을 샘플링\n",
        "# =====================================================\n",
        "def softmax_policy(state, tau):\n",
        "    # 주어진 상태 state에서 softmax( Q(state,·)/tau )로 행동 확률을 계산하고 하나 샘플링\n",
        "\n",
        "    # 현재 상태에서의 Q값 벡터 (길이: n_actions)\n",
        "    q_vals = Q[state]                                 # shape: (n_actions,)\n",
        "\n",
        "    # 온도 tau로 나누어서 softmax에 넣을 값 생성\n",
        "    prefs = q_vals / tau                              # 선호도(기준값)\n",
        "\n",
        "    # 숫자 안정성을 위해 최대값을 빼고 exp 계산\n",
        "    prefs_shifted = prefs - np.max(prefs)             # overflow 방지\n",
        "    exp_prefs = np.exp(prefs_shifted)                 # exp 연산\n",
        "    probs = exp_prefs / np.sum(exp_prefs)             # 정규화해서 확률 벡터로 변환\n",
        "\n",
        "    # probs에 따라 행동을 하나 샘플링\n",
        "    action = np.random.choice(np.arange(n_actions), p=probs)\n",
        "\n",
        "    return action, probs                              # (선택된 행동, 행동확률분포)\n",
        "\n",
        "# =====================================================\n",
        "# 4. Soft Q-Learning 업데이트 식\n",
        "#    - Soft Bellman backup:\n",
        "#      V(s') = tau * log Σ_a' exp( Q(s',a') / tau )\n",
        "#      Q(s,a) ← Q(s,a) + α [ r + γ * V(s') - Q(s,a) ]\n",
        "# =====================================================\n",
        "def soft_value(next_state, tau):\n",
        "    # 다음 상태 next_state에서의 soft value V(s')\n",
        "    # V(s') = tau * log Σ_a exp(Q(s',a)/tau)\n",
        "\n",
        "    q_next = Q[next_state]                               # (n_actions,)\n",
        "    prefs = q_next / tau\n",
        "    prefs_shifted = prefs - np.max(prefs)                # overflow 방지\n",
        "    exp_prefs = np.exp(prefs_shifted)\n",
        "    log_sum = np.log(np.sum(exp_prefs) + 1e-8)           # log Σ exp\n",
        "    v = tau * log_sum                                    # soft value\n",
        "    return v\n",
        "\n",
        "# =====================================================\n",
        "# 5. 학습 루프 (Soft Q-Learning)\n",
        "# =====================================================\n",
        "reward_history = []  # 에피소드별 총 보상 기록용 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Soft Q-Learning(SQL) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    # 에피소드 시작 시 상태 초기화\n",
        "    state = reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) softmax 정책으로 행동 선택 (탐험/이용 자동 조절)\n",
        "        action, action_probs = softmax_policy(state, tau)\n",
        "\n",
        "        # 2) 환경에 행동 적용 → 다음 상태, 보상, 종료 여부\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) soft Q-러닝 업데이트\n",
        "        #    - 다음 상태가 종료 상태이면 V(next_state)=0으로 처리\n",
        "        if done:\n",
        "            v_next = 0.0\n",
        "        else:\n",
        "            v_next = soft_value(next_state, tau)    # soft value V(s')\n",
        "\n",
        "        td_target = reward + gamma * v_next         # r + γ V(s')\n",
        "        td_error  = td_target - Q[state, action]    # TD 오차\n",
        "        Q[state, action] += alpha * td_error        # Q 업데이트\n",
        "\n",
        "        # 4) 누적 보상 계산 및 상태 이동\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        # 5) 목표 상태 도달 시 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 에피소드별 총 보상을 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 평균 리워드 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# =====================================================\n",
        "# 6. 학습된 Q-테이블 출력\n",
        "# =====================================================\n",
        "print(\"▶ 최종 Soft Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    print(f\"상태 {s}: {Q[s]}\")\n",
        "\n",
        "# =====================================================\n",
        "# 7. 학습된 정책(가장 확률이 높은 행동 기준의 greedy 정책) 출력\n",
        "#    - 실제 정책은 softmax지만, 설명을 위해 argmax 기준으로 화살표 표시\n",
        "# =====================================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}  # 행동 인덱스를 화살표로 매핑\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy: greedy w.r.t Soft Q)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"                         # 목표 상태는 G로 표시\n",
        "    else:\n",
        "        # 해당 상태에서 Q값이 가장 큰 행동을 greedy 선택\n",
        "        best_action = int(np.argmax(Q[s]))\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# =====================================================\n",
        "# 8. 학습된 정책(softmax 기반 greedy)으로 1회 에피소드 실행 예시\n",
        "#    - 여기서는 tau를 매우 작게 해서 거의 greedy에 가깝게 사용해도 되고\n",
        "#      그냥 argmax로만 행동을 골라도 됨.\n",
        "# =====================================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    # softmax 정책 대신, Q값 기준 argmax로 완전히 greedy하게 행동\n",
        "    action = int(np.argmax(Q[state]))\n",
        "\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bbae0523-c999-42c6-b9b5-2f2c5b7b20b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbae0523-c999-42c6-b9b5-2f2c5b7b20b8",
        "outputId": "fb224cd6-ffd8-4bbc-a4cd-fdfbadcc4c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 DQN + PER 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.965, epsilon = 0.078\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.967, epsilon = 0.061\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.967, epsilon = 0.050\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.970, epsilon = 0.050\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.969, epsilon = 0.050\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.969, epsilon = 0.050\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.968, epsilon = 0.050\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.968, epsilon = 0.050\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.969, epsilon = 0.050\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.968, epsilon = 0.050\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ DQN + PER 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.06371025 0.55240762]\n",
            "상태 1: [0.04028242 0.55052092]\n",
            "상태 2: [0.02519569 0.64619044]\n",
            "상태 3: [0.05680502 0.84578399]\n",
            "상태 4: [0.04570611 0.53712368]\n",
            "\n",
            "▶ 학습된 정책(Policy: greedy w.r.t Q)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "########################################################################################################\n",
        "## (1-12) PER(Prioritized Experience Replay) : 중요한 경험을 우선적으로 학습하는 경험 리플레이 전략\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산을 위한 numpy\n",
        "\n",
        "# =====================================================\n",
        "# 0. 난수 시드 고정 (실행마다 같은 결과가 나오도록)\n",
        "# =====================================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# =====================================================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "#    - 상태: 0, 1, 2, 3, 4  (4가 목표 상태)\n",
        "#    - 행동: 0=왼쪽, 1=오른쪽\n",
        "# =====================================================\n",
        "n_states = 5   # 상태 개수\n",
        "n_actions = 2  # 행동 개수: 0(←), 1(→)\n",
        "\n",
        "def step(state, action):\n",
        "    # 현재 상태 state에서 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동\n",
        "    if action == 0:\n",
        "        # 최소 상태 0을 넘지 않도록 방지\n",
        "        next_state = max(0, state - 1)\n",
        "    # 행동이 1이면 오른쪽으로 한 칸 이동\n",
        "    else:\n",
        "        # 최대 상태 4를 넘지 않도록 방지\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    # 목표 상태(4)에 도달했는지 확인\n",
        "    if next_state == n_states - 1:\n",
        "        # 목표 도달 시 보상 +1\n",
        "        reward = 1.0\n",
        "        # 에피소드 종료\n",
        "        done = True\n",
        "    else:\n",
        "        # 그 외에는 한 스텝당 작은 패널티 부여(-0.01)\n",
        "        # → 조금이라도 더 빨리 목표에 가도록 유도\n",
        "        reward = -0.01\n",
        "        done = False\n",
        "\n",
        "    # (다음 상태, 보상, 종료 여부) 반환\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 정수 상태를 one-hot 벡터로 변환\n",
        "    # 예: 상태 2 → [0, 0, 1, 0, 0]\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)\n",
        "    x[0, state] = 1.0\n",
        "    return x\n",
        "\n",
        "# =====================================================\n",
        "# 2. DQN 신경망 구조 정의 (NumPy로 구현)\n",
        "#    - 입력: 상태(one-hot, 5차원)\n",
        "#    - 은닉층: 32 노드, ReLU\n",
        "#    - 출력: 각 행동(2개)에 대한 Q값\n",
        "# =====================================================\n",
        "input_dim  = n_states   # 5\n",
        "hidden_dim = 32         # 은닉 노드 수\n",
        "output_dim = n_actions  # 2\n",
        "\n",
        "# 온라인 Q-네트워크 파라미터 (학습 대상)\n",
        "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)  # 1층 가중치\n",
        "b1 = np.zeros((1, hidden_dim))                     # 1층 편향\n",
        "W2 = 0.1 * np.random.randn(hidden_dim, output_dim) # 2층 가중치\n",
        "b2 = np.zeros((1, output_dim))                     # 2층 편향\n",
        "\n",
        "# 타깃 Q-네트워크 파라미터 (온라인 네트워크를 주기적으로 복사)\n",
        "W1_tgt = W1.copy()\n",
        "b1_tgt = b1.copy()\n",
        "W2_tgt = W2.copy()\n",
        "b2_tgt = b2.copy()\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 음수는 0, 양수는 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward_network(x, use_target=False):\n",
        "    # DQN 순전파 함수\n",
        "    # x        : 입력 상태 배치 (배치 크기 B, 차원 input_dim)\n",
        "    # use_target: True면 타깃 네트워크 사용, False면 온라인 네트워크 사용\n",
        "    # 반환값   : (은닉층 전개전 z1, 은닉층 활성값 h, 출력 Q값)\n",
        "    if not use_target:\n",
        "        # 온라인 네트워크 사용\n",
        "        z1 = x @ W1 + b1          # 1층 선형\n",
        "        h  = relu(z1)             # ReLU\n",
        "        out = h @ W2 + b2         # 2층 선형 (Q값)\n",
        "    else:\n",
        "        # 타깃 네트워크 사용\n",
        "        z1 = x @ W1_tgt + b1_tgt\n",
        "        h  = relu(z1)\n",
        "        out = h @ W2_tgt + b2_tgt\n",
        "\n",
        "    return z1, h, out\n",
        "\n",
        "# =====================================================\n",
        "# 3. Prioritized Experience Replay(PER) 버퍼 구현\n",
        "#    - 리플레이 버퍼 + 각각의 transition에 priority 부여\n",
        "#    - 샘플링 시 priority 비례 확률로 샘플\n",
        "# =====================================================\n",
        "buffer_capacity = 1000   # 리플레이 버퍼 최대 크기\n",
        "replay_buffer  = []      # (state, action, reward, next_state, done) 튜플 목록\n",
        "priorities     = []      # 각 transition의 priority (같은 인덱스)\n",
        "\n",
        "alpha = 0.6              # PER에서 priority의 비중 (0=균등, 1=완전 priority)\n",
        "beta  = 0.4              # IS(importance sampling) 보정 계수\n",
        "priority_eps = 1e-6      # priority가 0이 되는 것을 방지하는 작은 값\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 새 transition을 리플레이 버퍼에 추가\n",
        "    # priority는 현재까지의 최대 priority로 초기화 (새로운 경험을 우선 학습)\n",
        "    if len(priorities) > 0:\n",
        "        max_prio = max(priorities)\n",
        "    else:\n",
        "        max_prio = 1.0\n",
        "\n",
        "    # 버퍼가 가득 차면 가장 오래된 transition 제거(FIFO)\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)\n",
        "        priorities.pop(0)\n",
        "\n",
        "    # 새 transition 및 priority 추가\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    priorities.append(max_prio)\n",
        "\n",
        "def sample_from_buffer_per(batch_size):\n",
        "    # Prioritized Experience Replay 방식으로 미니배치 샘플링\n",
        "    # 1) priority 리스트를 기반으로 확률 분포 P(i) 계산\n",
        "    # 2) P(i)에 따라 인덱스 샘플링\n",
        "    # 3) IS weight 계산\n",
        "\n",
        "    # priority 배열로 변환\n",
        "    prios = np.array(priorities, dtype=np.float32)  # shape: (N,)\n",
        "\n",
        "    # priority^alpha (alpha가 클수록 큰 TD오차에 더 집중)\n",
        "    scaled_prios = prios ** alpha\n",
        "\n",
        "    # 확률 분포로 정규화\n",
        "    P = scaled_prios / (np.sum(scaled_prios) + 1e-8)\n",
        "\n",
        "    # P를 이용해 인덱스 샘플링\n",
        "    indices = np.random.choice(len(replay_buffer), size=batch_size, p=P, replace=False)\n",
        "\n",
        "    # IS(importance sampling) weight 계산\n",
        "    # w_i = (N * P(i))^-beta / max_j (N * P(j))^-beta\n",
        "    N = len(replay_buffer)\n",
        "    weights = (N * P[indices]) ** (-beta)\n",
        "    weights = weights / (np.max(weights) + 1e-8)\n",
        "    weights = weights.astype(np.float32)\n",
        "\n",
        "    # 샘플링된 transition들\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "\n",
        "    return batch, indices, weights\n",
        "\n",
        "# =====================================================\n",
        "# 4. 하이퍼파라미터 설정\n",
        "# =====================================================\n",
        "gamma = 0.9             # 할인율\n",
        "learning_rate = 0.001   # 학습률\n",
        "n_episodes = 500        # 에피소드 수\n",
        "max_steps  = 20         # 에피소드당 최대 스텝 수\n",
        "\n",
        "batch_size        = 32  # 미니배치 크기\n",
        "warmup_steps      = 100 # 최소 이 정도 샘플이 쌓여야 학습 시작\n",
        "target_update_ep  = 20  # 타깃 네트워크 동기화 주기(에피소드 단위)\n",
        "\n",
        "epsilon     = 0.1       # ε-greedy에서 탐험 확률 (PER 자체는 탐험이 아님)\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995   # 필요시 서서히 줄일 수 있음\n",
        "\n",
        "# =====================================================\n",
        "# 5. ε-greedy 행동 선택 함수 (DQN + PER)\n",
        "# =====================================================\n",
        "def choose_action(state, epsilon):\n",
        "    # 확률 epsilon으로 무작위 행동 -> 탐험\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "\n",
        "    # 그렇지 않으면 현재 Q값 기준으로 greedy 행동 선택\n",
        "    x = state_to_onehot(state)                      # (1, n_states)\n",
        "    _, _, q_values = forward_network(x, use_target=False)  # 온라인 네트워크 사용\n",
        "    action = int(np.argmax(q_values, axis=1)[0])   # Q값이 가장 큰 행동 선택\n",
        "    return action\n",
        "\n",
        "# =====================================================\n",
        "# 6. PER + DQN 학습 함수\n",
        "# =====================================================\n",
        "def train_dqn_per(batch_size):\n",
        "    # 전역 변수로 선언 (이 함수 안에서 수정할 예정)\n",
        "    global W1, b1, W2, b2, priorities\n",
        "\n",
        "    # PER 방식으로 미니배치 샘플링\n",
        "    batch, indices, is_weights = sample_from_buffer_per(batch_size)\n",
        "\n",
        "    # 배치를 각각 분리 (numpy 배열로 변환)\n",
        "    states      = np.array([s  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r  for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d  for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태를 one-hot 벡터로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "    B = X.shape[0]\n",
        "\n",
        "    # 배치 인덱스 (0 ~ B-1)\n",
        "    batch_idx = np.arange(B)\n",
        "\n",
        "    # (1) 온라인 네트워크로 현재 상태의 Q값 계산\n",
        "    z1, h, q_values = forward_network(X, use_target=False)  # q_values: (B, n_actions)\n",
        "\n",
        "    # 현재 상태에서 실제로 선택된 행동에 대한 Q값만 추출\n",
        "    q_pred = q_values[batch_idx, actions]                   # (B,)\n",
        "\n",
        "    # (2) 타깃 네트워크로 다음 상태의 Q값 계산\n",
        "    _, _, q_next = forward_network(X_next, use_target=True) # (B, n_actions)\n",
        "\n",
        "    # 다음 상태에서의 최대 Q값 선택 (Double DQN이 아니라 단순 DQN 방식)\n",
        "    q_next_max = np.max(q_next, axis=1)                     # (B,)\n",
        "\n",
        "    # 종료 상태이면 미래 보상이 없으므로 0 처리\n",
        "    not_dones = (~dones).astype(np.float32)                 # (B,)\n",
        "\n",
        "    # TD target 계산: r + γ * (1-done) * max_a' Q(s',a')\n",
        "    td_target = rewards + gamma * not_dones * q_next_max    # (B,)\n",
        "\n",
        "    # TD오차 (δ = target - prediction)\n",
        "    td_error = td_target - q_pred                           # (B,)\n",
        "\n",
        "    # MSE 손실의 gradient (IS weight 포함):\n",
        "    # L = mean( w_i * (td_error_i)^2 )\n",
        "    # dL/dq_pred = -2 * w_i * td_error_i / B\n",
        "    is_w = is_weights                                      # (B,)\n",
        "    dL_dq_pred = -2.0 * is_w * td_error / B               # (B,)\n",
        "\n",
        "    # Q값 전체(q_values)에 대한 gradient 초기화 (0으로)\n",
        "    dQ = np.zeros_like(q_values)                           # (B, n_actions)\n",
        "    dQ[batch_idx, actions] = dL_dq_pred                    # 선택된 행동 위치에만 gradient 반영\n",
        "\n",
        "    # 2층(출력층)에 대한 gradient 계산\n",
        "    # q_values = h @ W2 + b2\n",
        "    dW2 = h.T @ dQ                                         # (hidden_dim, n_actions)\n",
        "    db2 = np.sum(dQ, axis=0, keepdims=True)               # (1, n_actions)\n",
        "\n",
        "    # 은닉층으로 gradient 전파\n",
        "    dh = dQ @ W2.T                                         # (B, hidden_dim)\n",
        "\n",
        "    # 1층 z1에 대한 gradient (ReLU 도함수 곱)\n",
        "    dz1 = dh * relu_deriv(z1)                              # (B, hidden_dim)\n",
        "\n",
        "    # 1층에 대한 gradient 계산\n",
        "    dW1 = X.T @ dz1                                        # (input_dim, hidden_dim)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)               # (1, hidden_dim)\n",
        "\n",
        "    # 파라미터 업데이트 (경사하강법)\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # PER의 priority 업데이트: |TD오차| + 작은 epsilon\n",
        "    new_priorities = np.abs(td_error) + priority_eps\n",
        "    for idx_buf, pr in zip(indices, new_priorities):\n",
        "        priorities[idx_buf] = pr\n",
        "\n",
        "# =====================================================\n",
        "# 7. 학습 루프 (DQN + PER)\n",
        "# =====================================================\n",
        "reward_history = []  # 에피소드별 총 보상을 저장\n",
        "total_steps    = 0   # 전체 스텝 수\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 DQN + PER 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    # 에피소드 시작 시 상태 초기화\n",
        "    state = reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        total_steps += 1\n",
        "\n",
        "        # 1) ε-greedy 정책으로 행동 선택\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # 2) 환경에 행동 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) 리플레이 버퍼에 transition 추가\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # 4) 리플레이 버퍼에 충분한 데이터가 쌓이면 PER 기반 학습 수행\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_dqn_per(batch_size)\n",
        "\n",
        "        # 5) 누적 보상 업데이트\n",
        "        total_reward += reward\n",
        "\n",
        "        # 6) 상태 이동\n",
        "        state = next_state\n",
        "\n",
        "        # 7) 목표 상태 도달 시 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 에피소드별 보상 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # ε 서서히 감소 (원하면 사용 / 여기서는 거의 고정)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # 타깃 네트워크 주기적으로 업데이트\n",
        "    if episode % target_update_ep == 0:\n",
        "        W1_tgt = W1.copy()\n",
        "        b1_tgt = b1.copy()\n",
        "        W2_tgt = W2.copy()\n",
        "        b2_tgt = b2.copy()\n",
        "\n",
        "    # 로그 출력 (50 에피소드마다)\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# =====================================================\n",
        "# 8. 학습된 근사 Q-테이블 출력\n",
        "#    - 각 상태 s에 대해 one-hot을 넣고 Q(s,a)를 추정\n",
        "# =====================================================\n",
        "print(\"▶ DQN + PER 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)                     # (1, n_states)\n",
        "    _, _, q_vals = forward_network(x, use_target=False)\n",
        "    q_row = q_vals[0]\n",
        "    print(f\"상태 {s}: {q_row}\")\n",
        "\n",
        "# =====================================================\n",
        "# 9. 학습된 정책( greedy ) 출력\n",
        "# =====================================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy: greedy w.r.t Q)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        _, _, q_vals = forward_network(x, use_target=False)\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# =====================================================\n",
        "# 10. 학습된 정책으로 1회 에피소드 실행\n",
        "# =====================================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    # 탐험 없이 항상 greedy 행동 선택\n",
        "    x = state_to_onehot(state)\n",
        "    _, _, q_vals = forward_network(x, use_target=False)\n",
        "    action = int(np.argmax(q_vals, axis=1)[0])\n",
        "\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "67a945a0-5e94-4103-88e0-af45612af0fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a945a0-5e94-4103-88e0-af45612af0fa",
        "outputId": "f47684c9-7183-4af7-a02d-9a87f0a55890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 HER (Hindsight Experience Replay) + Tabular Q-Learning 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.720, epsilon = 0.778\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.940, epsilon = 0.606\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.980, epsilon = 0.471\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.367\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.286\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.222\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.173\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.135\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.105\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 1.000, epsilon = 0.082\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ HER + Q-Learning 근사 Q-테이블 (행: 상태 s, 열: 행동[←,→], 목표 g=4 기준)\n",
            "상태 0, 목표 4: [0.65609884 0.728999  ]\n",
            "상태 1, 목표 4: [0.65609884 0.8099992 ]\n",
            "상태 2, 목표 4: [0.728999  0.8999995]\n",
            "상태 3, 목표 4: [0.8099992  0.99999976]\n",
            "상태 4, 목표 4: [0. 0.]\n",
            "\n",
            "▶ 학습된 정책(Policy: greedy w.r.t Q(s,g=4,a))\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (목표 g=4)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "########################################################################################################\n",
        "## (1-13) HER(Hindsight Experience Replay) : 목표 달성을 학습할 수 있도록 과거 경험을 재사용하는 기법\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np  # 수치 계산용 numpy\n",
        "\n",
        "# ============================================\n",
        "# 0. 난수 시드 고정 (항상 같은 결과가 나오도록)\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "#    - 실제 물리 상태 s: 0,1,2,3,4  (자리 위치)\n",
        "#    - 목표 상태 g: 0~4 (HER에서는 목표도 상태공간 위에 놓음)\n",
        "#    - 여기서는 \"실제 환경 목표\"는 항상 4로 고정\n",
        "# ============================================\n",
        "n_states = 5          # 상태 개수: 0~4\n",
        "n_actions = 2         # 행동 개수: 0(←), 1(→)\n",
        "env_real_goal = 4     # 환경에서 실제 목표 상태 (고정)\n",
        "\n",
        "def env_step(state, action):\n",
        "    # ------------------------------------------------\n",
        "    # 환경 dynamics (목표와 무관한 순수 위치 이동)\n",
        "    #  - action=0: 왼쪽 한 칸,   상태는 최소 0\n",
        "    #  - action=1: 오른쪽 한 칸, 상태는 최대 4\n",
        "    # ------------------------------------------------\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 환경의 “실제 목표”에 대한 보상\n",
        "    #  - HER 학습 시에는 이 보상뿐 아니라\n",
        "    #    사후적으로 정의한 가짜 목표들에 대해서도\n",
        "    #    보상을 다시 계산해서 사용함\n",
        "    # ------------------------------------------------\n",
        "    if next_state == env_real_goal:\n",
        "        reward = 1.0      # 목표 도달\n",
        "        done = True\n",
        "    else:\n",
        "        reward = 0.0      # 실패(스파스 보상)\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "# ============================================\n",
        "# 2. HER + Tabular Q-Learning 설정\n",
        "#    - Q(s, g, a): (상태 s, 목표 g, 행동 a)에 대한 가치\n",
        "#    - s, g ∈ {0,1,2,3,4}, a ∈ {0,1}\n",
        "#    - 테이블 크기: [5(상태) x 5(목표) x 2(행동)]\n",
        "# ============================================\n",
        "Q = np.zeros((n_states, n_states, n_actions), dtype=np.float32)\n",
        "\n",
        "alpha = 0.1     # 학습률\n",
        "gamma = 0.9     # 할인율\n",
        "epsilon = 1.0   # 초기 ε-greedy 탐험 비율\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "n_episodes = 500  # 에피소드 수\n",
        "max_steps  = 20   # 에피소드당 최대 스텝 수\n",
        "\n",
        "# ============================================\n",
        "# 3. ε-greedy 정책 (goal-conditioned)\n",
        "#    - 현재 상태 s와 목표 g에 대해 Q(s,g,·)를 보고 행동 선택\n",
        "# ============================================\n",
        "def choose_action(state, goal, epsilon):\n",
        "    # ------------------------------------------------\n",
        "    # 확률 epsilon으로 무작위 행동 (탐험)\n",
        "    # ------------------------------------------------\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 그 외에는 Q(s,g,a)가 최대인 행동 선택 (이용)\n",
        "    # ------------------------------------------------\n",
        "    q_vals = Q[state, goal]          # shape: (n_actions,)\n",
        "    best_action = int(np.argmax(q_vals))\n",
        "    return best_action\n",
        "\n",
        "# ============================================\n",
        "# 4. HER 업데이트 함수\n",
        "#    - 한 에피소드에서 수집된 trajectory를 이용해\n",
        "#      (1) 실제 목표(4)에 대한 Q 업데이트\n",
        "#      (2) 사후 목표(trajectory 중 나중에 도달한 상태)를\n",
        "#          인공적인 목표로 삼아 Q를 추가 업데이트\n",
        "# ============================================\n",
        "def her_update(episode_transitions):\n",
        "    # episode_transitions: 리스트\n",
        "    #  각 원소: (s, a, s_next)\n",
        "    #  (실제 환경에서는 s_next == env_real_goal 이면 done=True로 종료)\n",
        "\n",
        "    # 에피소드 길이\n",
        "    T = len(episode_transitions)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 1) 실제 목표(env_real_goal=4)에 대한 Q 업데이트\n",
        "    # ------------------------------------------------\n",
        "    for t in range(T):\n",
        "        s, a, s_next = episode_transitions[t]\n",
        "\n",
        "        # 실제 목표 g_real에 대한 보상 (스파스 보상)\n",
        "        if s_next == env_real_goal:\n",
        "            r_real = 1.0\n",
        "            done_real = True\n",
        "        else:\n",
        "            r_real = 0.0\n",
        "            done_real = False\n",
        "\n",
        "        # TD target 계산 (실제 목표)\n",
        "        if done_real:\n",
        "            target_real = r_real\n",
        "        else:\n",
        "            target_real = r_real + gamma * np.max(Q[s_next, env_real_goal])\n",
        "\n",
        "        # TD 오차 및 Q 업데이트\n",
        "        td_error = target_real - Q[s, env_real_goal, a]\n",
        "        Q[s, env_real_goal, a] += alpha * td_error\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 2) HER (Hindsight Experience Replay)\n",
        "    #    - 각 시점 t에 대해, “나중에 실제로 도달한 상태들”을\n",
        "    #      목표 g_her로 삼아 다시 Q를 업데이트\n",
        "    #    - 여기서는 설명을 위해 가장 단순하게\n",
        "    #      “모든 미래 시점 t' >= t”를 사용\n",
        "    # ------------------------------------------------\n",
        "    for t in range(T):\n",
        "        s, a, s_next = episode_transitions[t]\n",
        "\n",
        "        # t 이후에 도달한 모든 s'들을 목표로 사용\n",
        "        for future_t in range(t, T):\n",
        "            # HER에서 사용하는 인공 목표 (achieved goal)\n",
        "            _, _, g_her = episode_transitions[future_t]  # 그 시점의 next_state를 목표로 사용\n",
        "\n",
        "            # HER용 보상: \"그 목표에 도달했는가?\"\n",
        "            if s_next == g_her:\n",
        "                r_her = 1.0\n",
        "                done_her = True\n",
        "            else:\n",
        "                r_her = 0.0\n",
        "                done_her = False\n",
        "\n",
        "            # TD target (HER 목표 g_her에 대한)\n",
        "            if done_her:\n",
        "                target_her = r_her\n",
        "            else:\n",
        "                target_her = r_her + gamma * np.max(Q[s_next, g_her])\n",
        "\n",
        "            td_error_her = target_her - Q[s, g_her, a]\n",
        "            Q[s, g_her, a] += alpha * td_error_her\n",
        "\n",
        "# ============================================\n",
        "# 5. HER + Tabular Q-Learning 학습 루프\n",
        "# ============================================\n",
        "reward_history = []\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 HER (Hindsight Experience Replay) + Tabular Q-Learning 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    # --------------------------------------------\n",
        "    # 에피소드 시작: 상태 초기화\n",
        "    # --------------------------------------------\n",
        "    state = reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    # 이 에피소드에서의 (s, a, s_next) trajectory를 저장\n",
        "    episode_transitions = []\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # ----------------------------------------\n",
        "        # 1) 현재 \"실제 목표\" env_real_goal(=4)에 대해 행동 선택\n",
        "        #    (goal-conditioned Q(s,g,a) 중 g=4)\n",
        "        # ----------------------------------------\n",
        "        action = choose_action(state, env_real_goal, epsilon)\n",
        "\n",
        "        # ----------------------------------------\n",
        "        # 2) 환경에 행동 적용\n",
        "        # ----------------------------------------\n",
        "        next_state, reward, done = env_step(state, action)\n",
        "\n",
        "        # trajectory에 저장 (HER에서 목표 재구성에 활용)\n",
        "        episode_transitions.append((state, action, next_state))\n",
        "\n",
        "        # 실제 목표에 대한 보상으로 누적 보상 계산\n",
        "        total_reward += reward\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        state = next_state\n",
        "\n",
        "        # 목표 도달 시 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # --------------------------------------------\n",
        "    # 3) 한 에피소드가 끝난 뒤 HER 업데이트 수행\n",
        "    #    - 실제 목표 + HER 목표들에 대해 Q를 모두 학습\n",
        "    # --------------------------------------------\n",
        "    her_update(episode_transitions)\n",
        "\n",
        "    # --------------------------------------------\n",
        "    # 4) ε 감소 및 로그 기록\n",
        "    # --------------------------------------------\n",
        "    reward_history.append(total_reward)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}, epsilon = {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 6. 목표 g=4에 대한 Q(s,g=4,a)만 따로 출력\n",
        "#    - 이제까지 다른 알고리즘과 비교하기 위해\n",
        "#      g=4일 때의 Q 테이블을 꺼내서 보여줌\n",
        "# ============================================\n",
        "print(\"▶ HER + Q-Learning 근사 Q-테이블 (행: 상태 s, 열: 행동[←,→], 목표 g=4 기준)\")\n",
        "for s in range(n_states):\n",
        "    print(f\"상태 {s}, 목표 4: {Q[s, env_real_goal]}\")\n",
        "\n",
        "# ============================================\n",
        "# 7. 목표 g=4에 대한 greedy 정책 출력\n",
        "# ============================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy: greedy w.r.t Q(s,g=4,a))\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == env_real_goal:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        best_action = int(np.argmax(Q[s, env_real_goal]))\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ============================================\n",
        "# 8. 학습된 정책으로 1회 에피소드 실행 예시 (목표 g=4)\n",
        "# ============================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (목표 g=4)\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    best_action = int(np.argmax(Q[state, env_real_goal]))\n",
        "    next_state, reward, done = env_step(state, best_action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "85dfcbd8-ec52-4d0b-a7bd-691e843828d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85dfcbd8-ec52-4d0b-a7bd-691e843828d7",
        "outputId": "49c73a63-ea9f-44be-fe52-c954e7c182a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 NoisyNet DQN(NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 리워드 = 0.962\n",
            "[Episode  100] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  150] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  200] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  250] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  300] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  350] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  400] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  450] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "[Episode  500] 최근 50 에피소드 평균 리워드 = 0.970\n",
            "\n",
            "=== 학습 종료 ===\n",
            "\n",
            "▶ NoisyNet DQN 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.04956277 0.55301418]\n",
            "상태 1: [0.02100446 0.56870416]\n",
            "상태 2: [0.0087512  0.67418015]\n",
            "상태 3: [0.03827106 0.87848686]\n",
            "상태 4: [0.03131548 0.55306848]\n",
            "\n",
            "▶ 학습된 정책(Policy: greedy w.r.t Q(mu))\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "########################################################################################################\n",
        "## (1-14) NoisyNet : 신경망 가중치에 노이즈를 추가해 탐색 효율성을 높이는 방식\n",
        "########################################################################################################\n",
        "import numpy as np  # 수치 계산용 numpy\n",
        "\n",
        "# ============================================\n",
        "# 0. 난수 시드 고정 (실행마다 같은 결과를 얻기 위해)\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "#    - 상태: 0,1,2,3,4  (5개)\n",
        "#    - 행동: 0=왼쪽(←), 1=오른쪽(→)\n",
        "#    - 목표 상태: 4에 도달하면 보상 +1, 에피소드 종료\n",
        "#    - 그 외에는 보상 -0.01 (빨리 도달하도록 유도)\n",
        "# ============================================\n",
        "n_states  = 5                 # 상태 개수\n",
        "n_actions = 2                 # 행동 개수 (←, →)\n",
        "goal_state = 4                # 목표 상태\n",
        "\n",
        "def step(state, action):\n",
        "    # 환경 dynamics:\n",
        "    #  - action=0: 왼쪽으로 한 칸 이동 (최소 0)\n",
        "    #  - action=1: 오른쪽으로 한 칸 이동 (최대 4)\n",
        "    if action == 0:\n",
        "        next_state = max(0, state - 1)\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    # 보상 설계:\n",
        "    #  - 목표 상태(4)에 도달하면 +1, 에피소드 종료\n",
        "    #  - 그 외에는 작은 패널티(-0.01)\n",
        "    if next_state == goal_state:\n",
        "        reward = 1.0\n",
        "        done   = True\n",
        "    else:\n",
        "        reward = -0.01\n",
        "        done   = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 항상 상태 0에서 에피소드 시작\n",
        "    return 0\n",
        "\n",
        "def state_to_onehot(state):\n",
        "    # 정수 상태를 one-hot 벡터로 변환\n",
        "    # 예: 상태 2 → [0, 0, 1, 0, 0]\n",
        "    x = np.zeros((1, n_states), dtype=np.float32)\n",
        "    x[0, state] = 1.0\n",
        "    return x\n",
        "\n",
        "# ============================================\n",
        "# 2. NoisyNet DQN 신경망 구조 정의 (NumPy로 구현)\n",
        "#    - 입력: 상태(one-hot, 5차원)\n",
        "#    - 은닉층: 32 노드, Noisy Linear + ReLU\n",
        "#    - 출력층: 2 노드, Noisy Linear (각 행동의 Q값)\n",
        "#    - Noisy Linear:\n",
        "#        W = W_mu + W_sigma ⊙ eps_W\n",
        "#        b = b_mu + b_sigma ⊙ eps_b\n",
        "# ============================================\n",
        "input_dim  = n_states   # 5\n",
        "hidden_dim = 32         # 은닉 노드 수\n",
        "output_dim = n_actions  # 2\n",
        "\n",
        "# 2-1. 온라인 네트워크 파라미터 (학습 대상)\n",
        "#   - 각 층마다 (mu, sigma) 쌍으로 구성\n",
        "# 1층 가중치/편향 (입력 → 은닉)\n",
        "W1_mu    = 0.1 * np.random.randn(input_dim, hidden_dim)\n",
        "W1_sigma = 0.017 * np.ones((input_dim, hidden_dim), dtype=np.float32)  # 초기 sigma는 작은 값으로\n",
        "b1_mu    = np.zeros((1, hidden_dim), dtype=np.float32)\n",
        "b1_sigma = 0.017 * np.ones((1, hidden_dim), dtype=np.float32)\n",
        "\n",
        "# 2층 가중치/편향 (은닉 → 출력)\n",
        "W2_mu    = 0.1 * np.random.randn(hidden_dim, output_dim)\n",
        "W2_sigma = 0.017 * np.ones((hidden_dim, output_dim), dtype=np.float32)\n",
        "b2_mu    = np.zeros((1, output_dim), dtype=np.float32)\n",
        "b2_sigma = 0.017 * np.ones((1, output_dim), dtype=np.float32)\n",
        "\n",
        "# 2-2. 타깃 네트워크 파라미터 (mu만 복사해서 사용, sigma는 사용하지 않음)\n",
        "W1_tgt_mu = W1_mu.copy()\n",
        "b1_tgt_mu = b1_mu.copy()\n",
        "W2_tgt_mu = W2_mu.copy()\n",
        "b2_tgt_mu = b2_mu.copy()\n",
        "\n",
        "# 2-3. NoisyNet에서 사용한 노이즈를 저장할 변수 (역전파 시 필요)\n",
        "last_eps_W1 = None\n",
        "last_eps_b1 = None\n",
        "last_eps_W2 = None\n",
        "last_eps_b2 = None\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU 활성화 함수: 음수는 0, 양수는 그대로\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    # ReLU 도함수: x>0이면 1, 아니면 0\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def forward_noisy(x):\n",
        "    # ---------------------------------------------\n",
        "    # NoisyNet 순전파 (온라인 네트워크)\n",
        "    #  - 각 층에서 새로운 노이즈를 샘플링하여\n",
        "    #    가중치/편향에 더해줌\n",
        "    # ---------------------------------------------\n",
        "    global last_eps_W1, last_eps_b1, last_eps_W2, last_eps_b2\n",
        "\n",
        "    # 1층 노이즈 샘플링 (정규분포 N(0,1))\n",
        "    eps_W1 = np.random.randn(*W1_mu.shape).astype(np.float32)\n",
        "    eps_b1 = np.random.randn(*b1_mu.shape).astype(np.float32)\n",
        "\n",
        "    # 1층 실제 가중치/편향 계산\n",
        "    W1 = W1_mu + W1_sigma * eps_W1\n",
        "    b1 = b1_mu + b1_sigma * eps_b1\n",
        "\n",
        "    # 1층 순전파\n",
        "    z1 = x @ W1 + b1\n",
        "    h  = relu(z1)\n",
        "\n",
        "    # 2층 노이즈 샘플링\n",
        "    eps_W2 = np.random.randn(*W2_mu.shape).astype(np.float32)\n",
        "    eps_b2 = np.random.randn(*b2_mu.shape).astype(np.float32)\n",
        "\n",
        "    # 2층 실제 가중치/편향 계산\n",
        "    W2 = W2_mu + W2_sigma * eps_W2\n",
        "    b2 = b2_mu + b2_sigma * eps_b2\n",
        "\n",
        "    # 2층 순전파 (Q값)\n",
        "    q = h @ W2 + b2\n",
        "\n",
        "    # 사용한 노이즈를 전역 변수에 저장 (역전파에서 사용)\n",
        "    last_eps_W1 = eps_W1\n",
        "    last_eps_b1 = eps_b1\n",
        "    last_eps_W2 = eps_W2\n",
        "    last_eps_b2 = eps_b2\n",
        "\n",
        "    return z1, h, q\n",
        "\n",
        "def forward_target(x):\n",
        "    # ---------------------------------------------\n",
        "    # 타깃 네트워크 순전파\n",
        "    #  - 여기서는 단순히 mu 파라미터만 사용 (deterministic)\n",
        "    # ---------------------------------------------\n",
        "    z1 = x @ W1_tgt_mu + b1_tgt_mu\n",
        "    h  = relu(z1)\n",
        "    q  = h @ W2_tgt_mu + b2_tgt_mu\n",
        "    return z1, h, q\n",
        "\n",
        "# ============================================\n",
        "# 3. 리플레이 버퍼 (일반 DQN과 동일, PER 아님)\n",
        "# ============================================\n",
        "buffer_capacity = 1000      # 버퍼 최대 크기\n",
        "replay_buffer   = []        # (state, action, reward, next_state, done)\n",
        "\n",
        "def add_to_buffer(state, action, reward, next_state, done):\n",
        "    # 버퍼가 찼으면 가장 오래된 transition 제거\n",
        "    if len(replay_buffer) >= buffer_capacity:\n",
        "        replay_buffer.pop(0)\n",
        "    # 새로운 transition 추가\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "def sample_from_buffer(batch_size):\n",
        "    # 랜덤하게 mini-batch 샘플링\n",
        "    idx = np.random.choice(len(replay_buffer), size=batch_size, replace=False)\n",
        "    batch = [replay_buffer[i] for i in idx]\n",
        "    return batch\n",
        "\n",
        "# ============================================\n",
        "# 4. 하이퍼파라미터 설정\n",
        "# ============================================\n",
        "gamma         = 0.9        # 할인율\n",
        "learning_rate = 0.001      # 학습률\n",
        "\n",
        "n_episodes = 500           # 에피소드 수\n",
        "max_steps  = 20            # 에피소드당 최대 스텝 수\n",
        "\n",
        "batch_size       = 32      # mini-batch 크기\n",
        "warmup_steps     = 100     # 학습 시작 전 최소 transition 수\n",
        "target_update_ep = 20      # 타깃 네트워크 업데이트 주기(에피소드 단위)\n",
        "\n",
        "# NoisyNet에서는 보통 ε-greedy를 쓰지 않지만,\n",
        "# 여기서는 완전한 NoisyNet 스타일로 ε = 0으로 두고 사용\n",
        "epsilon = 0.0              # ε-greedy 사용 안 함\n",
        "\n",
        "# ============================================\n",
        "# 5. 행동 선택 (NoisyNet 기반, ε-greedy 없음)\n",
        "# ============================================\n",
        "def choose_action(state):\n",
        "    # 상태를 one-hot 벡터로 변환\n",
        "    x = state_to_onehot(state)       # (1, n_states)\n",
        "    # NoisyNet 순전파로 Q값 계산 (노이즈 포함)\n",
        "    _, _, q_values = forward_noisy(x)\n",
        "    # Q값이 최대인 행동 선택\n",
        "    action = int(np.argmax(q_values, axis=1)[0])\n",
        "    return action\n",
        "\n",
        "# ============================================\n",
        "# 6. NoisyNet DQN 학습 함수\n",
        "# ============================================\n",
        "def train_dqn_noisynet(batch_size):\n",
        "    global W1_mu, W1_sigma, b1_mu, b1_sigma\n",
        "    global W2_mu, W2_sigma, b2_mu, b2_sigma\n",
        "\n",
        "    # 리플레이 버퍼에서 mini-batch 샘플링\n",
        "    batch = sample_from_buffer(batch_size)\n",
        "\n",
        "    # 배치를 각각 분리\n",
        "    states      = np.array([s  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    actions     = np.array([a  for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    rewards     = np.array([r  for (s, a, r, ns, d) in batch], dtype=np.float32)\n",
        "    next_states = np.array([ns for (s, a, r, ns, d) in batch], dtype=np.int32)\n",
        "    dones       = np.array([d  for (s, a, r, ns, d) in batch], dtype=bool)\n",
        "\n",
        "    # 상태/다음 상태를 one-hot으로 변환\n",
        "    X      = np.vstack([state_to_onehot(s)  for s in states])      # (B, n_states)\n",
        "    X_next = np.vstack([state_to_onehot(ns) for ns in next_states])# (B, n_states)\n",
        "    B = X.shape[0]\n",
        "\n",
        "    batch_idx = np.arange(B)\n",
        "\n",
        "    # ------------------------------\n",
        "    # (1) 온라인 NoisyNet 순전파 (Q(s,a))\n",
        "    # ------------------------------\n",
        "    z1, h, q_values = forward_noisy(X)      # q_values: (B, n_actions)\n",
        "    q_pred = q_values[batch_idx, actions]   # 선택한 행동에 대한 Q(s,a)만 뽑기\n",
        "\n",
        "    # ------------------------------\n",
        "    # (2) 타깃 네트워크 순전파 (Q_target(s',a'))\n",
        "    # ------------------------------\n",
        "    _, _, q_next = forward_target(X_next)   # (B, n_actions)\n",
        "    q_next_max = np.max(q_next, axis=1)     # max_a' Q_target(s',a')\n",
        "\n",
        "    not_dones = (~dones).astype(np.float32)\n",
        "\n",
        "    # TD target 계산: r + γ * (1-done) * max_a' Q_target(s',a')\n",
        "    td_target = rewards + gamma * not_dones * q_next_max\n",
        "\n",
        "    # TD 오차 (δ)\n",
        "    td_error = td_target - q_pred          # (B,)\n",
        "\n",
        "    # MSE 손실의 gradient: L = mean(δ^2)\n",
        "    # dL/dq_pred = -2 * δ / B\n",
        "    dL_dq_pred = -2.0 * td_error / B       # (B,)\n",
        "\n",
        "    # Q값 전체에 대한 gradient (선택한 행동 위치에만 존재)\n",
        "    dQ = np.zeros_like(q_values)           # (B, n_actions)\n",
        "    dQ[batch_idx, actions] = dL_dq_pred\n",
        "\n",
        "    # ------------------------------\n",
        "    # (3) 2층(출력층) 역전파 (Noisy Linear)\n",
        "    # ------------------------------\n",
        "    # q_values = h @ W2 + b2  (W2, b2는 noisy된 것)\n",
        "    # 하지만 우리는 mu, sigma에 대한 gradient가 필요함\n",
        "    # dL/dW2_eff = h^T @ dQ  (W2_eff = W2_mu + W2_sigma ⊙ eps_W2)\n",
        "    dW2_eff = h.T @ dQ                         # (hidden_dim, output_dim)\n",
        "    db2_eff = np.sum(dQ, axis=0, keepdims=True)\n",
        "\n",
        "    # eps 노이즈는 forward_noisy에서 저장된 것을 사용\n",
        "    eps_W2 = last_eps_W2\n",
        "    eps_b2 = last_eps_b2\n",
        "\n",
        "    # mu와 sigma에 대한 gradient\n",
        "    dW2_mu    = dW2_eff\n",
        "    dW2_sigma = dW2_eff * eps_W2\n",
        "    b2_mu_grad    = db2_eff\n",
        "    b2_sigma_grad = db2_eff * eps_b2\n",
        "\n",
        "    # 은닉층으로 gradient 전파\n",
        "    dh = dQ @ (W2_mu + W2_sigma * eps_W2).T    # (B, hidden_dim)\n",
        "\n",
        "    # ------------------------------\n",
        "    # (4) 1층 역전파 (Noisy Linear)\n",
        "    # ------------------------------\n",
        "    dz1 = dh * relu_deriv(z1)                  # (B, hidden_dim)\n",
        "    dW1_eff = X.T @ dz1                        # (input_dim, hidden_dim)\n",
        "    db1_eff = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    eps_W1 = last_eps_W1\n",
        "    eps_b1 = last_eps_b1\n",
        "\n",
        "    dW1_mu    = dW1_eff\n",
        "    dW1_sigma = dW1_eff * eps_W1\n",
        "    b1_mu_grad    = db1_eff\n",
        "    b1_sigma_grad = db1_eff * eps_b1\n",
        "\n",
        "    # ------------------------------\n",
        "    # (5) 파라미터 업데이트 (경사하강법)\n",
        "    # ------------------------------\n",
        "    W2_mu    -= learning_rate * dW2_mu\n",
        "    W2_sigma -= learning_rate * dW2_sigma\n",
        "    b2_mu    -= learning_rate * b2_mu_grad\n",
        "    b2_sigma -= learning_rate * b2_sigma_grad\n",
        "\n",
        "    W1_mu    -= learning_rate * dW1_mu\n",
        "    W1_sigma -= learning_rate * dW1_sigma\n",
        "    b1_mu    -= learning_rate * b1_mu_grad\n",
        "    b1_sigma -= learning_rate * b1_sigma_grad\n",
        "\n",
        "# ============================================\n",
        "# 7. 학습 루프 (NoisyNet DQN)\n",
        "# ============================================\n",
        "reward_history = []\n",
        "total_steps    = 0\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 NoisyNet DQN(NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        total_steps += 1\n",
        "\n",
        "        # 1) NoisyNet 기반 행동 선택 (ε-greedy 없이)\n",
        "        action = choose_action(state)\n",
        "\n",
        "        # 2) 환경 한 스텝 진행\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) 리플레이 버퍼에 transition 추가\n",
        "        add_to_buffer(state, action, reward, next_state, done)\n",
        "\n",
        "        # 4) 일정량 이상 쌓이면 학습 진행\n",
        "        if len(replay_buffer) >= max(batch_size, warmup_steps):\n",
        "            train_dqn_noisynet(batch_size)\n",
        "\n",
        "        # 5) 누적 보상 업데이트\n",
        "        total_reward += reward\n",
        "\n",
        "        # 6) 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # 7) 목표 도달 시 에피소드 종료\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 에피소드별 보상 기록\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    # 타깃 네트워크 주기적 동기화 (mu 파라미터 복사)\n",
        "    if episode % target_update_ep == 0:\n",
        "        W1_tgt_mu = W1_mu.copy()\n",
        "        b1_tgt_mu = b1_mu.copy()\n",
        "        W2_tgt_mu = W2_mu.copy()\n",
        "        b2_tgt_mu = b2_mu.copy()\n",
        "\n",
        "    # 50 에피소드마다 로그 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(reward_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 리워드 = {avg_reward:.3f}\")\n",
        "\n",
        "print(\"\\n=== 학습 종료 ===\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 8. 학습된 NoisyNet Q값 (mu 기준) 출력\n",
        "#    - 비교를 위해, 노이즈 없이 mu 파라미터만 사용\n",
        "# ============================================\n",
        "print(\"▶ NoisyNet DQN 근사 Q-테이블 (행: 상태, 열: 행동[←,→])\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    x = state_to_onehot(s)\n",
        "    # 타깃 네트워크 대신, 온라인 mu 파라미터를 사용해 deterministic Q를 계산\n",
        "    z1 = x @ W1_mu + b1_mu\n",
        "    h  = relu(z1)\n",
        "    q_vals = h @ W2_mu + b2_mu\n",
        "    q_row = q_vals[0]\n",
        "    print(f\"상태 {s}: {q_row}\")\n",
        "\n",
        "# ============================================\n",
        "# 9. 학습된 정책( greedy ) 출력\n",
        "# ============================================\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "print(\"\\n▶ 학습된 정책(Policy: greedy w.r.t Q(mu))\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == goal_state:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        x = state_to_onehot(s)\n",
        "        z1 = x @ W1_mu + b1_mu\n",
        "        h  = relu(z1)\n",
        "        q_vals = h @ W2_mu + b2_mu\n",
        "        best_action = int(np.argmax(q_vals, axis=1)[0])\n",
        "        policy_str += f\" {action_symbols[best_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ============================================\n",
        "# 10. 학습된 정책으로 1회 에피소드 실행 예시\n",
        "# ============================================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    x = state_to_onehot(state)\n",
        "    z1 = x @ W1_mu + b1_mu\n",
        "    h  = relu(z1)\n",
        "    q_vals = h @ W2_mu + b2_mu\n",
        "    action = int(np.argmax(q_vals, axis=1)[0])\n",
        "\n",
        "    next_state, reward, done = step(state, action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ddd62b9-b3e6-47bf-854c-ab9947ef65d8",
      "metadata": {
        "id": "7ddd62b9-b3e6-47bf-854c-ab9947ef65d8"
      },
      "source": [
        "# [2] Model-free RL : Policy Iteration\n",
        "\n",
        "\t기본 Policy Gradient 및 Actor-Critic 계열\n",
        "\t\t(2-1) REINFORCE: 정책 경사법의 기본 형태\n",
        "\t\t(2-2) Actor-Critic: 기본적인 Actor-Critic 구조, Policy Gradient와 Critic의 Q값 평가 결합\n",
        "\t\t(2-3) NAC(Natural Actor-Critic): Natural Gradient를 적용해 정책의 효율적 업데이트를 수행\n",
        "\tAdvantage Actor-Critic 및 분산 학습 계열\n",
        "\t\t(2-4) A2C/A3C(Advantage Actor-Critic): 분산형 Actor-Critic 모델\n",
        "\t\t(2-5) ACER(Actor-Critic with Experience Replay): 경험 리플레이를 추가한 Actor-Critic 방법\n",
        "\t\t(2-6) IMPALA(Importance Weighted Actor-Learner Architecture): 분산 학습에 최적화된 구조\n",
        "\t\t(2-7) Off-PAC(Off-Policy Actor-Critic): 오프폴리시 데이터를 활용하는 Actor-Critic 기법\n",
        "\t신뢰 구간 기반 정책 최적화 계열\n",
        "\t\t(2-8) PPO(Proximal Policy Optimization): 신뢰 구간을 사용해 안정적으로 정책을 업데이트\n",
        "\t\t(2-9) TRPO(Trust Region Policy Optimization): 정책 급변을 방지하는 최적화 기법\n",
        "\t연속 행동 공간 최적화 계열\n",
        "\t\t(2-10) DDPG(Deep Deterministic Policy Gradient): 연속적 행동 공간에서 학습하는 Actor-Critic 모델\n",
        "\t\t(2-11) TD3(Twin Delayed DDPG): DDPG의 한계점을 극복하기 위한 개선된 모델\n",
        "\t\t(2-12) SAC(Soft Actor-Critic): 탐색과 활용의 균형을 유지하도록 설계된 정책 학습 모델\n",
        "\t전문가 시범 데이터 활용 계열\n",
        "\t\t(2-13) BC(Behavioral Cloning): 데이터를 기반으로 정책을 모방하는 방식\n",
        "\t\t(2-14) DDPGfD(DDPG from Demonstrations): 전문가의 시범을 사용해 DDPG 성능을 개선"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7d4b980e-0b7e-4f3e-a569-76d6d52010e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d4b980e-0b7e-4f3e-a569-76d6d52010e6",
        "outputId": "cfffe382-7575-46e1-c4ca-90398dad9acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 REINFORCE 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.663\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.828\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.867\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.898\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.883\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.905\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.893\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.896\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.900\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.892\n",
            "\n",
            "=== REINFORCE 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.10977985 0.89022015]\n",
            "상태 1: [0.43454976 0.56545024]\n",
            "상태 2: [0.10441219 0.89558781]\n",
            "상태 3: [0.20137174 0.79862826]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-1) REINFORCE : 정책 경사법의 기본 형태\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. REINFORCE 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)   # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99         # 할인율 (미래 보상을 얼마나 반영할지 결정)\n",
        "alpha = 0.05         # 학습률 (정책 파라미터 업데이트 크기)\n",
        "n_episodes = 500     # 전체 학습 에피소드 수\n",
        "max_steps  = 20      # 한 에피소드에서 허용하는 최대 스텝 수 (무한 루프 방지)\n",
        "\n",
        "# 정책 파라미터 theta: (상태 x 행동) 크기의 실수 행렬\n",
        "# 각 상태에서 두 행동(왼쪽, 오른쪽)에 대한 \"선호도(preference)\"를 나타냄\n",
        "theta = np.zeros((n_states, n_actions))    # 처음에는 모든 선호도를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                     # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)               # 지수 함수 적용\n",
        "    return exp / np.sum(exp)               # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state):\n",
        "    # 주어진 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta[state]는 해당 상태에서 각 행동에 대한 선호도 벡터\n",
        "    return softmax(theta[state])           # softmax를 적용하여 확률로 변환\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책에 따라 행동을 하나 샘플링하는 함수\n",
        "    probs = policy_probs(state)            # 행동 확률 π(a|s; θ) 구하기\n",
        "    return np.random.choice(n_actions, p=probs)  # 해당 확률에 따라 0 또는 1 선택\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ) 를 계산하는 함수\n",
        "    # 반환값은 길이 2의 벡터 (각 행동에 대한 gradient)\n",
        "    probs = policy_probs(state)            # π(a|s; θ) 계산\n",
        "    grad = -probs.copy()                   # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                    # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                           # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. REINFORCE 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 REINFORCE 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    # 한 에피소드에 대해 (상태, 행동, 보상) 시퀀스를 저장하기 위한 리스트 초기화\n",
        "    states = []                           # 방문한 상태들을 순서대로 저장\n",
        "    actions = []                          # 각 시점에서 선택한 행동들을 저장\n",
        "    rewards = []                          # 각 시점에서 받은 보상을 저장\n",
        "\n",
        "    # 에피소드 시작: 초기 상태로 리셋\n",
        "    state = reset()\n",
        "\n",
        "    # 1) 정책에 따라 에피소드 한 번 실행하여 trajectory 수집\n",
        "    for t in range(max_steps):\n",
        "        action = sample_action(state)     # 현재 정책에 따라 행동 샘플링\n",
        "        next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "        states.append(state)             # 시점 t의 상태 기록\n",
        "        actions.append(action)           # 시점 t의 행동 기록\n",
        "        rewards.append(reward)           # 시점 t의 보상 기록\n",
        "\n",
        "        state = next_state               # 다음 상태로 전이\n",
        "\n",
        "        if done:                         # 목표 상태 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 2) 에피소드 내 각 시점 t에 대한 Return G_t 계산 (뒤에서부터 누적)\n",
        "    T = len(rewards)                     # 실제로 진행된 스텝 수\n",
        "    returns = np.zeros(T)                # 각 시점 t의 G_t를 저장할 배열\n",
        "    G = 0.0                              # 뒤에서부터 누적할 Return 값\n",
        "    for t in reversed(range(T)):         # 마지막 시점에서부터 거꾸로 진행\n",
        "        G = rewards[t] + gamma * G       # G_t = r_t + γ G_{t+1}\n",
        "        returns[t] = G                   # 계산된 G_t 저장\n",
        "\n",
        "    # 3) 정책 파라미터 θ 업데이트 (Monte Carlo Policy Gradient)\n",
        "    #    θ ← θ + α ∑_t ∇θ log π(a_t|s_t; θ) * G_t\n",
        "    for t in range(T):\n",
        "        s = states[t]                    # 시점 t의 상태\n",
        "        a = actions[t]                   # 시점 t의 행동\n",
        "        G_t = returns[t]                 # 시점 t의 Return G_t\n",
        "\n",
        "        grad = grad_log_policy(s, a)     # ∇θ(s,:) log π(a|s; θ) 계산 (shape: (2,))\n",
        "        theta[s] += alpha * grad * G_t   # 해당 상태 s의 파라미터에 gradient ascent 적용\n",
        "\n",
        "    # 4) 에피소드의 시작 시점 Return(G_0)을 기록\n",
        "    episode_return = returns[0]          # G_0: 에피소드 전체 Return\n",
        "    return_history.append(episode_return)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== REINFORCE 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s)              # 상태 s에서의 행동 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (확률이 더 큰 행동 선택)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s))  # 가장 확률이 높은 행동 선택\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state)          # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동을 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c84d925e-61b2-4768-b5b3-901ea1ce33bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c84d925e-61b2-4768-b5b3-901ea1ce33bc",
        "outputId": "9cee9abd-bac7-432c-d253-cfa4ab33b332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Actor-Critic 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.684\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.820\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.862\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.890\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.886\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.895\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.899\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.907\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.910\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.909\n",
            "\n",
            "=== Actor-Critic 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.26635851 0.73364149]\n",
            "상태 1: [0.15665809 0.84334191]\n",
            "상태 2: [0.13892206 0.86107794]\n",
            "상태 3: [0.17650638 0.82349362]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-2) Actor-Critic: 기본적인 Actor-Critic 구조, Policy Gradient와 Critic의 Q값 평가 결합\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. Actor-Critic 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)   # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99         # 할인율\n",
        "alpha_theta = 0.05   # Actor(정책 파라미터) 학습률\n",
        "alpha_v = 0.1        # Critic(가치함수) 학습률\n",
        "n_episodes = 500     # 전체 학습 에피소드 수\n",
        "max_steps  = 20      # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "# 정책 파라미터 theta: (상태 x 행동) 크기의 실수 행렬\n",
        "# 각 상태에서 두 행동(왼쪽, 오른쪽)에 대한 \"선호도(preference)\"를 나타냄\n",
        "theta = np.zeros((n_states, n_actions))    # 처음에는 모든 선호도를 0으로 시작\n",
        "\n",
        "# 가치함수 파라미터 v: 각 상태의 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                     # 초기에는 모든 상태가치를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                     # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)               # 지수 함수 적용\n",
        "    return exp / np.sum(exp)               # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state):\n",
        "    # 주어진 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta[state]는 해당 상태에서 각 행동에 대한 선호도 벡터\n",
        "    return softmax(theta[state])           # softmax를 적용하여 확률로 변환\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책에 따라 행동을 하나 샘플링하는 함수\n",
        "    probs = policy_probs(state)            # 행동 확률 π(a|s; θ) 구하기\n",
        "    return np.random.choice(n_actions, p=probs)  # 해당 확률에 따라 0 또는 1 선택\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ) 를 계산하는 함수\n",
        "    # 반환값은 길이 2의 벡터 (각 행동에 대한 gradient)\n",
        "    probs = policy_probs(state)            # π(a|s; θ) 계산\n",
        "    grad = -probs.copy()                   # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                    # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                           # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. Actor-Critic 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Actor-Critic 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    # 한 에피소드에 대해 총 Return을 계산하기 위한 변수 초기화\n",
        "    state = reset()                       # 에피소드 시작 상태\n",
        "    G0 = 0.0                              # 시작 시점의 Return 누적용 (로그용)\n",
        "    discount = 1.0                        # γ^t 계수 누적용\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        # 1) 상태에서 정책에 따라 행동 선택\n",
        "        action = sample_action(state)     # 현재 Actor(정책)에 따른 행동 샘플링\n",
        "\n",
        "        # 2) 환경에 행동 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) TD 오차(delta) 계산\n",
        "        #    δ = r + γ V(s') - V(s)\n",
        "        v_s = v[state]                    # 현재 상태의 가치 V(s)\n",
        "        v_s_next = v[next_state] if not done else 0.0  # 종료 시에는 V(s') = 0으로 처리\n",
        "        delta = reward + gamma * v_s_next - v_s\n",
        "\n",
        "        # 4) Critic 업데이트: V(s) ← V(s) + α_v * δ\n",
        "        v[state] += alpha_v * delta\n",
        "\n",
        "        # 5) Actor 업데이트: θ ← θ + α_θ * δ * ∇θ log π(a|s; θ)\n",
        "        grad = grad_log_policy(state, action)\n",
        "        theta[state] += alpha_theta * delta * grad\n",
        "\n",
        "        # 6) 에피소드 Return(G_0) 추적 (로그용)\n",
        "        #    G_0 = Σ_t γ^t r_t\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        # 7) 다음 시점으로 전이\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # 목표 상태에 도달하면 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 한 에피소드가 끝나면 Return(G_0)을 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== Actor-Critic 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s)              # 상태 s에서의 행동 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (확률이 더 큰 행동 선택)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s))  # 가장 확률이 높은 행동 선택\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state)          # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동을 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "32638cbb-ff15-4db5-a423-e528220d6af8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32638cbb-ff15-4db5-a423-e528220d6af8",
        "outputId": "3f1a38b5-94de-4daf-f25f-8167b54ea816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 NAC (Natural Actor-Critic) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.725\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.941\n",
            "\n",
            "=== NAC 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [2.27874868e-05 9.99977213e-01]\n",
            "상태 1: [2.21007898e-05 9.99977899e-01]\n",
            "상태 2: [2.16594454e-05 9.99978341e-01]\n",
            "상태 3: [2.11265621e-05 9.99978873e-01]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-3) NAC(Natural Actor-Critic) : Natural Gradient를 적용해 정책의 효율적 업데이트를 수행\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. NAC 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)   # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99         # 할인율 (미래 보상 반영 정도)\n",
        "alpha = 0.05         # 학습률 (Natural Gradient 업데이트 크기)\n",
        "n_episodes = 500     # 전체 학습 에피소드 수\n",
        "max_steps  = 20      # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "# 정책 파라미터 theta: (상태 x 행동) 크기의 실수 행렬\n",
        "# 각 상태에서 두 행동(왼쪽, 오른쪽)에 대한 \"선호도(preference)\"를 나타냄\n",
        "theta = np.zeros((n_states, n_actions))    # 처음에는 모든 선호도를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                     # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)               # 지수 함수 적용\n",
        "    return exp / np.sum(exp)               # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state):\n",
        "    # 주어진 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta[state]는 해당 상태에서 각 행동에 대한 선호도 벡터\n",
        "    return softmax(theta[state])           # softmax를 적용하여 확률로 변환\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책에 따라 행동을 하나 샘플링하는 함수\n",
        "    probs = policy_probs(state)            # 행동 확률 π(a|s; θ) 구하기\n",
        "    return np.random.choice(n_actions, p=probs)  # 해당 확률에 따라 0 또는 1 선택\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ) 를 계산하는 함수\n",
        "    # 반환값은 길이 2의 벡터 (각 행동에 대한 gradient)\n",
        "    probs = policy_probs(state)            # π(a|s; θ) 계산\n",
        "    grad = -probs.copy()                   # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                    # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                           # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. NAC 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 NAC (Natural Actor-Critic) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    # 한 에피소드에 대해 (상태, 행동, 보상) 시퀀스를 저장하기 위한 리스트 초기화\n",
        "    states = []                           # 방문한 상태들을 순서대로 저장\n",
        "    actions = []                          # 각 시점에서 선택한 행동들을 저장\n",
        "    rewards = []                          # 각 시점에서 받은 보상을 저장\n",
        "\n",
        "    state = reset()                       # 에피소드 시작: 초기 상태로 리셋\n",
        "\n",
        "    # 1) 정책에 따라 에피소드 한 번 실행하여 trajectory 수집\n",
        "    for t in range(max_steps):\n",
        "        action = sample_action(state)     # 현재 정책에 따라 행동 샘플링\n",
        "        next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "        states.append(state)             # 시점 t의 상태 기록\n",
        "        actions.append(action)           # 시점 t의 행동 기록\n",
        "        rewards.append(reward)           # 시점 t의 보상 기록\n",
        "\n",
        "        state = next_state               # 다음 상태로 전이\n",
        "\n",
        "        if done:                         # 목표 상태 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 2) 에피소드 내 각 시점 t에 대한 Return G_t 계산 (뒤에서부터 누적)\n",
        "    T = len(rewards)                     # 실제로 진행된 스텝 수\n",
        "    returns = np.zeros(T)                # 각 시점 t의 G_t를 저장할 배열\n",
        "    G = 0.0                              # 뒤에서부터 누적할 Return 값\n",
        "    for t in reversed(range(T)):         # 마지막 시점에서부터 거꾸로 진행\n",
        "        G = rewards[t] + gamma * G       # G_t = r_t + γ G_{t+1}\n",
        "        returns[t] = G                   # 계산된 G_t 저장\n",
        "\n",
        "    # 3) 상태별 policy gradient / Fisher matrix 누적\n",
        "    #    각 상태 s에 대해:\n",
        "    #    g_s = Σ_t ∇θ log π(a_t|s_t=s) * G_t\n",
        "    #    F_s = Σ_t ∇θ log π(a_t|s_t=s) ∇θ log π(a_t|s_t=s)^T\n",
        "    g_per_state = np.zeros((n_states, n_actions))            # 각 상태별 gradient 벡터\n",
        "    F_per_state = np.zeros((n_states, n_actions, n_actions)) # 각 상태별 Fisher(2x2)\n",
        "\n",
        "    for t in range(T):\n",
        "        s = states[t]                    # 시점 t의 상태\n",
        "        a = actions[t]                   # 시점 t의 행동\n",
        "        G_t = returns[t]                 # 시점 t의 Return G_t\n",
        "\n",
        "        grad = grad_log_policy(s, a)     # ∇θ log π(a|s; θ) 계산 (shape: (2,))\n",
        "        g_per_state[s] += grad * G_t     # g_s 누적\n",
        "        F_per_state[s] += np.outer(grad, grad)  # F_s 누적 (2x2 outer product)\n",
        "\n",
        "    # 4) Natural Gradient 업데이트\n",
        "    #    θ_s ← θ_s + α * F_s^{-1} * g_s\n",
        "    #    2x2 행렬이므로 상태별로 간단히 계산 가능\n",
        "    for s in range(n_states):\n",
        "        g_s = g_per_state[s]             # shape: (2,)\n",
        "        F_s = F_per_state[s]             # shape: (2,2)\n",
        "\n",
        "        # 에피소드 동안 해당 상태를 방문하지 않았다면 건너뜀\n",
        "        if np.all(F_s == 0):\n",
        "            continue\n",
        "\n",
        "        # 수치 안정성을 위한 정규화(작은 항 추가): F_s_reg = F_s + λI\n",
        "        lam = 1e-3\n",
        "        F_s_reg = F_s + lam * np.eye(n_actions)\n",
        "\n",
        "        # 2x2 행렬이므로 역행렬 계산 시도\n",
        "        try:\n",
        "            F_inv = np.linalg.inv(F_s_reg)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # 역행렬 계산 실패 시, 대각 성분만 이용한 근사 역행렬 사용\n",
        "            F_inv = np.diag(1.0 / np.diag(F_s_reg))\n",
        "\n",
        "        natural_grad = F_inv @ g_s       # 자연 그래디언트: F_s^{-1} g_s  (shape: (2,))\n",
        "        theta[s] += alpha * natural_grad # 상태 s에 대한 정책 파라미터 업데이트\n",
        "\n",
        "    # 5) 에피소드 시작 시점 Return(G_0)을 기록 (로그용)\n",
        "    episode_return = returns[0]          # G_0: 에피소드 전체 Return\n",
        "    return_history.append(episode_return)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== NAC 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s)              # 상태 s에서의 행동 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (확률이 더 큰 행동 선택)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s))  # 가장 확률이 높은 행동 선택\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state)          # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동을 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f08d5425-fc5b-4948-a1c6-4fc3c415b70f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f08d5425-fc5b-4948-a1c6-4fc3c415b70f",
        "outputId": "0c3ee553-c296-4836-862b-0e161df1eef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 A2C/A3C(Advantage Actor-Critic) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.661\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.770\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.849\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.853\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.880\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.894\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.893\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.900\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.884\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.897\n",
            "\n",
            "=== A2C/A3C 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.29026512 0.70973488]\n",
            "상태 1: [0.17356323 0.82643677]\n",
            "상태 2: [0.16023098 0.83976902]\n",
            "상태 3: [0.19077707 0.80922293]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-4) A2C/A3C(Advantage Actor-Critic): 분산형 Actor-Critic 모델\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. A2C/A3C 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)   # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99         # 할인율\n",
        "alpha_theta = 0.05   # Actor(정책 파라미터) 학습률\n",
        "alpha_v = 0.1        # Critic(가치함수) 학습률\n",
        "n_episodes = 500     # 전체 학습 에피소드 수\n",
        "max_steps  = 20      # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "t_max = 5            # A2C/A3C 스타일 n-step rollout 길이 (멀티스텝 Advantage 계산용)\n",
        "\n",
        "# 정책 파라미터 theta: (상태 x 행동) 크기의 실수 행렬\n",
        "# 각 상태에서 두 행동(왼쪽, 오른쪽)에 대한 \"선호도(preference)\"를 나타냄\n",
        "theta = np.zeros((n_states, n_actions))    # 처음에는 모든 선호도를 0으로 시작\n",
        "\n",
        "# 가치함수 파라미터 v: 각 상태의 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                     # 초기에는 모든 상태가치를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                     # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)               # 지수 함수 적용\n",
        "    return exp / np.sum(exp)               # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state):\n",
        "    # 주어진 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta[state]는 해당 상태에서 각 행동에 대한 선호도 벡터\n",
        "    return softmax(theta[state])           # softmax를 적용하여 확률로 변환\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책에 따라 행동을 하나 샘플링하는 함수\n",
        "    probs = policy_probs(state)            # 행동 확률 π(a|s; θ) 구하기\n",
        "    return np.random.choice(n_actions, p=probs)  # 해당 확률에 따라 0 또는 1 선택\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ) 를 계산하는 함수\n",
        "    # 반환값은 길이 2의 벡터 (각 행동에 대한 gradient)\n",
        "    probs = policy_probs(state)            # π(a|s; θ) 계산\n",
        "    grad = -probs.copy()                   # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                    # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                           # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. A2C/A3C 스타일 Advantage Actor-Critic 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 A2C/A3C(Advantage Actor-Critic) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                       # 에피소드 시작 상태\n",
        "    G0 = 0.0                              # 에피소드 시작 Return(G_0) 추적용\n",
        "    discount = 1.0                        # γ^t 계수 누적용\n",
        "\n",
        "    step_count = 0                        # 에피소드 내 전체 스텝 카운터\n",
        "\n",
        "    while step_count < max_steps:\n",
        "        # 1) 최대 t_max 길이까지 rollout을 수행하며 (s_t, a_t, r_t, done) 시퀀스를 모음\n",
        "        states = []                       # 미니배치 내부 상태 리스트\n",
        "        actions = []                      # 미니배치 내부 행동 리스트\n",
        "        rewards = []                      # 미니배치 내부 보상 리스트\n",
        "        dones = []                        # 미니배치 내부 done 플래그 리스트\n",
        "\n",
        "        for t in range(t_max):\n",
        "            # 현재 상태에서 정책에 따라 행동 샘플링\n",
        "            action = sample_action(state)\n",
        "\n",
        "            # 환경에 행동 적용\n",
        "            next_state, reward, done = step(state, action)\n",
        "\n",
        "            # rollout 시퀀스에 기록\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "\n",
        "            # 에피소드 Return(G_0) 추적 (로그용)\n",
        "            G0 += discount * reward\n",
        "            discount *= gamma\n",
        "\n",
        "            step_count += 1               # 전체 스텝 수 증가\n",
        "            state = next_state            # 다음 상태로 전이\n",
        "\n",
        "            if done or step_count >= max_steps:\n",
        "                # 목표 도달 또는 최대 스텝 도달 시 rollout 종료\n",
        "                break\n",
        "\n",
        "        # 2) rollout의 마지막 상태에서 bootstrap 값 R 초기화\n",
        "        #    A2C/A3C 스타일 n-step return:\n",
        "        #    R = V(s_{t_end})  (단, terminal이면 0)\n",
        "        if len(states) == 0:\n",
        "            # 방어적 코드: rollout이 비어 있으면 다음 에피소드로\n",
        "            break\n",
        "\n",
        "        if dones[-1]:                     # 마지막 스텝에서 에피소드가 끝났다면\n",
        "            R = 0.0                       # terminal state 이므로 bootstrap 없음\n",
        "        else:\n",
        "            R = v[state]                  # 아직 끝나지 않았다면 V(s_last)를 bootstrap 값으로 사용\n",
        "\n",
        "        # 3) rollout을 거꾸로 돌면서 n-step Return과 Advantage를 계산하고\n",
        "        #    Actor와 Critic을 동시에 업데이트\n",
        "        for i in reversed(range(len(states))):\n",
        "            s = states[i]                 # 시점 i의 상태\n",
        "            a = actions[i]                # 시점 i의 행동\n",
        "            r = rewards[i]                # 시점 i의 보상\n",
        "\n",
        "            # n-step return R_t 계산: R_t = r_t + γ R_{t+1}\n",
        "            R = r + gamma * R\n",
        "\n",
        "            # Advantage A_t = R_t - V(s_t)\n",
        "            V_s = v[s]\n",
        "            advantage = R - V_s\n",
        "\n",
        "            # Critic 업데이트: V(s) ← V(s) + α_v * Advantage\n",
        "            v[s] += alpha_v * advantage\n",
        "\n",
        "            # Actor 업데이트: θ ← θ + α_θ * Advantage * ∇θ log π(a|s; θ)\n",
        "            grad = grad_log_policy(s, a)\n",
        "            theta[s] += alpha_theta * advantage * grad\n",
        "\n",
        "        # rollout 종료 후, 에피소드가 종료되었다면 while 루프 탈출\n",
        "        if dones[-1]:\n",
        "            break\n",
        "\n",
        "    # 한 에피소드가 끝나면 Return(G_0)을 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== A2C/A3C 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s)              # 상태 s에서의 행동 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (확률이 더 큰 행동 선택)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s))  # 가장 확률이 높은 행동 선택\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state)          # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동을 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "875cb894-0327-4843-a42a-3a186b40b28b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875cb894-0327-4843-a42a-3a186b40b28b",
        "outputId": "9d825f99-2c9f-4014-f2a6-4e1f2739e3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 ACER(Actor-Critic with Experience Replay) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.665\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.886\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.898\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.917\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.891\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.917\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.916\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.911\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.923\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.923\n",
            "\n",
            "=== ACER 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.18338427 0.81661573]\n",
            "상태 1: [0.10233127 0.89766873]\n",
            "상태 2: [0.10469859 0.89530141]\n",
            "상태 3: [0.11631745 0.88368255]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-5) ACER(Actor-Critic with Experience Replay): 경험 리플레이를 추가한 Actor-Critic 방법\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. ACER 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)   # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99         # 할인율\n",
        "alpha_theta = 0.05   # Actor(정책 파라미터) 학습률\n",
        "alpha_v = 0.1        # Critic(가치함수) 학습률\n",
        "\n",
        "n_episodes = 500     # 전체 학습 에피소드 수\n",
        "max_steps  = 20      # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "c_bar = 10.0         # 중요도 비율(truncated importance sampling) 상한값\n",
        "\n",
        "# 정책 파라미터 theta: (상태 x 행동) 크기의 실수 행렬\n",
        "# 각 상태에서 두 행동(왼쪽, 오른쪽)에 대한 \"선호도(preference)\"를 나타냄\n",
        "theta = np.zeros((n_states, n_actions))    # 처음에는 모든 선호도를 0으로 시작\n",
        "\n",
        "# 가치함수 파라미터 v: 각 상태의 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                     # 초기에는 모든 상태가치를 0으로 시작\n",
        "\n",
        "# 리플레이 버퍼 (간단하게 파이썬 리스트로 구현)\n",
        "# 각 원소는 (s, a, r, s_next, done, mu_a) 튜플\n",
        "replay_buffer = []\n",
        "buffer_capacity = 10000                    # 버퍼 최대 크기\n",
        "min_buffer_size = 50                       # 리플레이 시작을 위한 최소 저장 개수\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                     # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)               # 지수 함수 적용\n",
        "    return exp / np.sum(exp)               # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state):\n",
        "    # 주어진 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta[state]는 해당 상태에서 각 행동에 대한 선호도 벡터\n",
        "    return softmax(theta[state])           # softmax를 적용하여 확률로 변환\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책에 따라 행동을 하나 샘플링하는 함수\n",
        "    probs = policy_probs(state)            # 행동 확률 π(a|s; θ) 구하기\n",
        "    return np.random.choice(n_actions, p=probs), probs  # 행동과 그 때의 확률분포 반환\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ) 를 계산하는 함수\n",
        "    # 반환값은 길이 2의 벡터 (각 행동에 대한 gradient)\n",
        "    probs = policy_probs(state)            # π(a|s; θ) 계산\n",
        "    grad = -probs.copy()                   # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                    # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                           # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. 리플레이 버퍼에서 샘플링하여 ACER 업데이트 수행\n",
        "# ==============================\n",
        "def acer_replay_update():\n",
        "    # 리플레이 버퍼가 충분히 쌓이지 않았으면 업데이트를 하지 않음\n",
        "    if len(replay_buffer) < min_buffer_size:\n",
        "        return\n",
        "\n",
        "    # 리플레이 버퍼에서 하나의 transition을 무작위 샘플링\n",
        "    idx = np.random.randint(len(replay_buffer))\n",
        "    s, a, r, s_next, done, mu_a = replay_buffer[idx]\n",
        "\n",
        "    # 현재 정책 하에서의 행동 확률 π(a|s; θ)\n",
        "    probs = policy_probs(s)\n",
        "    pi_a = probs[a]\n",
        "\n",
        "    # 중요도 비율 ρ = π(a|s) / μ(a|s)\n",
        "    rho = pi_a / (mu_a + 1e-8)\n",
        "\n",
        "    # truncated importance sampling: c = min(c_bar, ρ)\n",
        "    c = min(c_bar, rho)\n",
        "\n",
        "    # TD 타깃 및 Advantage 계산\n",
        "    v_s = v[s]\n",
        "    v_s_next = v[s_next] if not done else 0.0\n",
        "    target = r + gamma * v_s_next\n",
        "    advantage = target - v_s\n",
        "\n",
        "    # Critic 업데이트: V(s) ← V(s) + α_v * Advantage\n",
        "    v[s] += alpha_v * advantage\n",
        "\n",
        "    # Actor 업데이트 (off-policy 보정): θ ← θ + α_θ * c * Advantage * ∇θ log π(a|s; θ)\n",
        "    grad = grad_log_policy(s, a)\n",
        "    theta[s] += alpha_theta * c * advantage * grad\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. ACER 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 ACER(Actor-Critic with Experience Replay) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                       # 에피소드 시작 상태\n",
        "    G0 = 0.0                              # 에피소드 시작 Return(G_0) 추적용\n",
        "    discount = 1.0                        # γ^t 계수 누적용\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) 현재 정책(behavior policy)로부터 행동을 샘플링\n",
        "        action, probs = sample_action(state)\n",
        "        mu_a = probs[action]              # 행동 a를 선택한 behavior policy 확률 μ(a|s)\n",
        "\n",
        "        # 2) 환경에 행동 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) on-policy TD 업데이트 (기본 Actor-Critic 업데이트)\n",
        "        v_s = v[state]\n",
        "        v_s_next = v[next_state] if not done else 0.0\n",
        "        target = reward + gamma * v_s_next\n",
        "        td_error = target - v_s\n",
        "\n",
        "        # Critic 업데이트\n",
        "        v[state] += alpha_v * td_error\n",
        "\n",
        "        # Actor 업데이트 (on-policy): θ ← θ + α_θ * δ * ∇θ log π(a|s; θ)\n",
        "        grad = grad_log_policy(state, action)\n",
        "        theta[state] += alpha_theta * td_error * grad\n",
        "\n",
        "        # 4) 리플레이 버퍼에 transition 추가\n",
        "        if len(replay_buffer) >= buffer_capacity:\n",
        "            # 버퍼가 가득 찬 경우, 가장 오래된 데이터를 제거\n",
        "            replay_buffer.pop(0)\n",
        "        replay_buffer.append((state, action, reward, next_state, done, mu_a))\n",
        "\n",
        "        # 5) 리플레이 버퍼에서 off-policy ACER 업데이트 한 번 수행\n",
        "        acer_replay_update()\n",
        "\n",
        "        # 6) 에피소드 Return(G_0) 추적 (로그용)\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        state = next_state                # 다음 상태로 전이\n",
        "\n",
        "        if done:\n",
        "            # 목표 상태 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 한 에피소드가 끝나면 Return(G_0)을 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== ACER 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s)              # 상태 s에서의 행동 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (확률이 더 큰 행동 선택)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s))  # 가장 확률이 높은 행동 선택\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state)          # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동을 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8202e2d0-923f-45ed-bf19-00d8601dea2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8202e2d0-923f-45ed-bf19-00d8601dea2f",
        "outputId": "e489e5c8-4c03-4aa6-9ee1-84ec5ed328f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 IMPALA(Importance Weighted Actor-Learner Architecture) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.535\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.688\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.753\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.855\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.884\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.883\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.888\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.868\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.890\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.890\n",
            "\n",
            "=== IMPALA 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ_learner) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.33174848 0.66825152]\n",
            "상태 1: [0.18933591 0.81066409]\n",
            "상태 2: [0.18348034 0.81651966]\n",
            "상태 3: [0.18936821 0.81063179]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용, learner 기준)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-6) IMPALA(Importance Weighted Actor-Learner Architecture): 분산 학습에 최적화된 구조\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. IMPALA 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)    # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99          # 할인율\n",
        "alpha_theta = 0.05    # Actor(정책 파라미터) 학습률\n",
        "alpha_v = 0.1         # Critic(가치함수) 학습률\n",
        "\n",
        "n_episodes = 500      # 전체 학습 에피소드 수\n",
        "max_steps  = 20       # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "rho_bar = 1.0         # 중요도 비율 ρ̄ 상한 (정책 gradient에 사용)\n",
        "c_bar   = 1.0         # c_t 상한 (V-trace 보정에 사용)\n",
        "\n",
        "sync_rate = 0.1       # learner → behavior policy로 옮기는 속도 (지연 정도를 만들기 위함)\n",
        "\n",
        "# 학습용 정책 파라미터(learner): theta_learner[s, a]\n",
        "theta_learner = np.zeros((n_states, n_actions))   # 초기값 0\n",
        "\n",
        "# 데이터 수집용 정책 파라미터(behavior): theta_behavior[s, a]\n",
        "theta_behavior = np.zeros((n_states, n_actions))  # 초기에는 learner와 동일\n",
        "\n",
        "# 가치함수 파라미터 v: 각 상태의 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                            # 초기에는 모든 상태가치 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                      # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)                # 지수 함수 적용\n",
        "    return exp / np.sum(exp)                # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(theta, state):\n",
        "    # 주어진 파라미터 theta와 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta[state]는 해당 상태에서 각 행동에 대한 선호도 벡터\n",
        "    return softmax(theta[state])            # softmax를 적용하여 확률로 변환\n",
        "\n",
        "def sample_action(theta, state):\n",
        "    # 주어진 정책 파라미터 theta를 사용하여 행동을 샘플링하는 함수\n",
        "    probs = policy_probs(theta, state)      # 행동 확률 π(a|s; θ) 계산\n",
        "    action = np.random.choice(n_actions, p=probs)  # 해당 확률에 따라 0 또는 1 선택\n",
        "    return action, probs                    # 선택한 행동과 확률 분포 반환\n",
        "\n",
        "def grad_log_policy(theta, state, action):\n",
        "    # ∇θ log π(a|s; θ) 를 계산하는 함수\n",
        "    # 반환값은 길이 2의 벡터 (각 행동에 대한 gradient)\n",
        "    probs = policy_probs(theta, state)      # π(a|s; θ) 계산\n",
        "    grad = -probs.copy()                    # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                     # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                             # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. IMPALA 스타일 V-trace 업데이트\n",
        "# ==============================\n",
        "def impala_update_trajectory(states, actions, rewards, dones, behavior_probs_seq):\n",
        "    # 하나의 trajectory(에피소드 또는 일부 구간)에 대해\n",
        "    # IMPALA의 V-trace 아이디어를 이용하여 learner 파라미터를 업데이트하는 함수\n",
        "    global theta_learner, v                # 외부에서 정의한 파라미터 사용\n",
        "\n",
        "    # trajectory 길이 계산\n",
        "    T = len(states)                        # 시퀀스의 길이 (0 ~ T-1)\n",
        "\n",
        "    # 각 시점에서 learner 정책과 behavior 정책의 확률, 중요도 비율 계산\n",
        "    rhos = np.zeros(T)                     # 중요도 비율 ρ_t = π(a_t|s_t) / μ(a_t|s_t)\n",
        "    cs   = np.zeros(T)                     # V-trace 보정 계수 c_t = min(c_bar, ρ_t)\n",
        "\n",
        "    # 각 시점에서의 V(s_t)와 V(s_{t+1})를 미리 계산\n",
        "    V_s      = np.zeros(T)                 # V(s_t)\n",
        "    V_s_next = np.zeros(T)                 # V(s_{t+1}), terminal인 경우 0\n",
        "\n",
        "    for t in range(T):\n",
        "        s = states[t]                      # 시점 t의 상태\n",
        "        a = actions[t]                     # 시점 t의 행동\n",
        "\n",
        "        # learner 정책에서의 행동 확률 π(a_t|s_t)\n",
        "        pi_probs = policy_probs(theta_learner, s)\n",
        "        pi_a = pi_probs[a]\n",
        "\n",
        "        # behavior 정책에서의 행동 확률 μ(a_t|s_t)\n",
        "        mu_probs = behavior_probs_seq[t]\n",
        "        mu_a = mu_probs[a]\n",
        "\n",
        "        # 중요도 비율 ρ_t 계산 (작은 epsilon으로 0 나누기 방지)\n",
        "        rho = pi_a / (mu_a + 1e-8)\n",
        "        rhos[t] = min(rho_bar, rho)        # ρ̄_t = min(ρ_bar, ρ_t)\n",
        "        cs[t]   = min(c_bar, rho)          # c_t   = min(c_bar, ρ_t)\n",
        "\n",
        "        # 가치함수 값 계산\n",
        "        V_s[t] = v[s]                      # V(s_t)\n",
        "        if dones[t]:\n",
        "            V_s_next[t] = 0.0             # terminal이면 V(s_{t+1}) = 0\n",
        "        else:\n",
        "            V_s_next[t] = v[states[t+1]] if t+1 < T else v[states[t]]  # 마지막이면 대충 자기 자신\n",
        "\n",
        "    # V-trace 타깃 v_target_t를 뒤에서부터 역순으로 계산\n",
        "    v_target = np.zeros(T)                 # 각 시점의 v_target_t\n",
        "    # 마지막 시점부터 역순으로 계산\n",
        "    for t in reversed(range(T)):\n",
        "        # δ^V_t = ρ̄_t * (r_t + γ V(s_{t+1}) - V(s_t))\n",
        "        delta_v = rhos[t] * (rewards[t] + gamma * V_s_next[t] - V_s[t])\n",
        "        if t == T - 1:\n",
        "            # 마지막 시점에서는 v_target_T = V(s_T) + δ^V_T\n",
        "            v_target[t] = V_s[t] + delta_v\n",
        "        else:\n",
        "            # v_target_t = V(s_t) + δ^V_t + γ c_t (v_target_{t+1} - V(s_{t+1}))\n",
        "            v_target[t] = V_s[t] + delta_v + gamma * cs[t] * (v_target[t+1] - V_s_next[t])\n",
        "\n",
        "    # 이제 v_target을 사용하여 Critic와 Actor(learner)를 업데이트\n",
        "    for t in range(T):\n",
        "        s = states[t]                      # 상태 s_t\n",
        "        a = actions[t]                     # 행동 a_t\n",
        "\n",
        "        # Advantage_t = v_target_t - V(s_t)\n",
        "        advantage = v_target[t] - v[s]\n",
        "\n",
        "        # Critic 업데이트: V(s) ← V(s) + α_v * Advantage_t\n",
        "        v[s] += alpha_v * advantage\n",
        "\n",
        "        # Actor(learner) 업데이트:\n",
        "        # θ ← θ + α_θ * ρ̄_t * Advantage_t * ∇θ log π(a_t|s_t; θ)\n",
        "        grad = grad_log_policy(theta_learner, s, a)\n",
        "        theta_learner[s] += alpha_theta * rhos[t] * advantage * grad\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. IMPALA 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 IMPALA(Importance Weighted Actor-Learner Architecture) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                       # 에피소드 시작 상태\n",
        "    G0 = 0.0                              # 에피소드 Return(G_0) 추적용\n",
        "    discount = 1.0                        # γ^t 계수 누적용\n",
        "\n",
        "    # 하나의 trajectory(에피소드 전체)를 behavior policy로 수집\n",
        "    states = []                           # 상태 시퀀스\n",
        "    actions = []                          # 행동 시퀀스\n",
        "    rewards = []                          # 보상 시퀀스\n",
        "    dones = []                            # done 플래그 시퀀스\n",
        "    behavior_probs_seq = []               # 각 시점의 behavior 정책 확률 분포 μ(·|s_t)\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # behavior policy(theta_behavior)를 사용하여 행동 샘플링\n",
        "        action, probs_b = sample_action(theta_behavior, state)\n",
        "\n",
        "        # 환경에 행동 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 시퀀스에 저장\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "        behavior_probs_seq.append(probs_b)\n",
        "\n",
        "        # Return(G_0) 계산용\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        state = next_state                # 다음 상태로 전이\n",
        "\n",
        "        if done:\n",
        "            # 목표 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 수집된 trajectory를 이용해 IMPALA 스타일 V-trace 업데이트 수행\n",
        "    impala_update_trajectory(states, actions, rewards, dones, behavior_probs_seq)\n",
        "\n",
        "    # learner 정책을 behavior 정책 쪽으로 부분 반영 (지연된 actor 효과)\n",
        "    theta_behavior = (1.0 - sync_rate) * theta_behavior + sync_rate * theta_learner\n",
        "\n",
        "    # 에피소드 Return 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== IMPALA 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ_learner) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(theta_learner, s)  # learner 정책에서의 행동 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (확률이 더 큰 행동 선택)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(theta_learner, s))  # learner 기준 greedy 행동\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용, learner 기준)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(theta_learner, state)  # learner 정책에서의 행동 확률\n",
        "    action = np.argmax(probs)                  # 가장 확률이 높은 행동 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b5783c5c-3ab7-4d6b-a478-317587d1161c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5783c5c-3ab7-4d6b-a478-317587d1161c",
        "outputId": "716c7535-97fd-43be-8b82-0aed751a61e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Off-PAC(Off-Policy Actor-Critic) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.915\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.915\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.930\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.923\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.924\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.922\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.922\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.922\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.926\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.923\n",
            "\n",
            "=== Off-PAC 학습 종료 ===\n",
            "\n",
            "▶ 학습된 타깃 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.31338939 0.68661061]\n",
            "상태 1: [0.16443638 0.83556362]\n",
            "상태 2: [0.12063528 0.87936472]\n",
            "상태 3: [0.13076979 0.86923021]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 타깃 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 타깃 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-7) Off-PAC(Off-Policy Actor-Critic): 오프폴리시 데이터를 활용하는 Actor-Critic 기법\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. Off-PAC 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)    # 난수 시드 고정 (실행결과 재현 가능)\n",
        "\n",
        "gamma = 0.99          # 할인율\n",
        "alpha_theta = 0.05    # Actor(정책 파라미터) 학습률\n",
        "alpha_v = 0.1         # Critic(가치함수) 학습률\n",
        "\n",
        "n_episodes = 500      # 전체 학습 에피소드 수\n",
        "max_steps  = 20       # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "epsilon_behavior = 0.2  # behavior policy의 ε-greedy 탐험 비율\n",
        "rho_bar = 5.0           # 중요도 비율 ρ 상한 (굉장히 큰 값 방지용)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 파라미터 초기화\n",
        "# ==============================\n",
        "# 타깃 정책 파라미터 theta[s, a]\n",
        "theta = np.zeros((n_states, n_actions))   # 초기에는 모든 선호도를 0으로 시작\n",
        "\n",
        "# Critic: 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                    # 초기에는 모든 상태가치를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                      # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)                # 지수 함수 적용\n",
        "    return exp / np.sum(exp)                # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def target_policy_probs(state):\n",
        "    # 타깃 정책 π(a|s; θ) 의 행동 확률을 반환하는 함수\n",
        "    return softmax(theta[state])            # theta[state]에 softmax 적용\n",
        "\n",
        "def behavior_policy_probs(state):\n",
        "    # behavior 정책 μ(a|s) 의 행동 확률을 반환하는 함수\n",
        "    # 여기서는 타깃 정책의 greedy 행동을 기준으로 ε-greedy 형태로 구성\n",
        "    pi_probs = target_policy_probs(state)   # 타깃 정책 확률\n",
        "    greedy_action = np.argmax(pi_probs)     # 가장 확률이 높은 행동 (greedy)\n",
        "    probs = np.ones(n_actions) * (epsilon_behavior / n_actions)  # 기본적으로 균등 분포에 ε 나누기\n",
        "    probs[greedy_action] += (1.0 - epsilon_behavior)             # greedy 행동에 1-ε 추가\n",
        "    return probs\n",
        "\n",
        "def sample_behavior_action(state):\n",
        "    # behavior 정책 μ(a|s)를 이용하여 행동을 샘플링하는 함수\n",
        "    probs = behavior_policy_probs(state)    # μ(a|s) 확률 분포\n",
        "    action = np.random.choice(n_actions, p=probs)  # 그 확률에 따라 행동 선택\n",
        "    return action, probs                    # 선택한 행동과 해당 시점의 μ 분포 반환\n",
        "\n",
        "def grad_log_target_policy(state, action):\n",
        "    # 타깃 정책에 대한 ∇θ log π(a|s; θ)를 계산하는 함수\n",
        "    probs = target_policy_probs(state)      # π(a|s; θ)\n",
        "    grad = -probs.copy()                    # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                     # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                             # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. Off-PAC 학습 루프\n",
        "# ==============================\n",
        "return_history = []                       # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 Off-PAC(Off-Policy Actor-Critic) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                       # 에피소드 시작 상태\n",
        "    G0 = 0.0                              # 에피소드 Return(G_0) 추적용\n",
        "    discount = 1.0                        # γ^t 계수 누적용\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) behavior 정책 μ(a|s)에서 행동 샘플링 (off-policy)\n",
        "        action, mu_probs = sample_behavior_action(state)\n",
        "        mu_a = mu_probs[action]           # 실제 선택된 행동 a의 behavior 확률 μ(a|s)\n",
        "\n",
        "        # 2) 환경에 행동 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # 3) Critic 업데이트를 위한 TD 오차 계산\n",
        "        v_s = v[state]\n",
        "        v_s_next = v[next_state] if not done else 0.0\n",
        "        td_target = reward + gamma * v_s_next\n",
        "        td_error = td_target - v_s         # δ_t = r + γV(s') - V(s)\n",
        "\n",
        "        # 4) 타깃 정책 π(a|s; θ)의 행동 확률 계산\n",
        "        pi_probs = target_policy_probs(state)\n",
        "        pi_a = pi_probs[action]           # π(a|s) 중 실제 취한 행동의 확률\n",
        "\n",
        "        # 5) 중요도 비율 ρ_t = π(a|s) / μ(a|s)\n",
        "        rho = pi_a / (mu_a + 1e-8)        # 0 나누기 방지를 위한 작은 상수 추가\n",
        "        rho = min(rho_bar, rho)           # 너무 큰 값이 되지 않도록 상한 적용\n",
        "\n",
        "        # 6) Critic 업데이트 (off-policy 가중치 적용)\n",
        "        #    V(s) ← V(s) + α_v * ρ * δ_t\n",
        "        v[state] += alpha_v * rho * td_error\n",
        "\n",
        "        # 7) Actor 업데이트 (Off-PAC의 핵심)\n",
        "        #    θ ← θ + α_θ * ρ * δ_t * ∇θ log π(a|s; θ)\n",
        "        grad = grad_log_target_policy(state, action)\n",
        "        theta[state] += alpha_theta * rho * td_error * grad\n",
        "\n",
        "        # 8) Return(G_0) 추적 (로그용)\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        state = next_state                # 다음 상태로 전이\n",
        "\n",
        "        if done:\n",
        "            # 목표 상태 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 한 에피소드가 끝나면 Return(G_0)을 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== Off-PAC 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 타깃 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 타깃 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = target_policy_probs(s)        # 상태 s에서의 타깃 정책 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현 (타깃 정책 기준)\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 타깃 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(target_policy_probs(s))  # 타깃 정책 기준 greedy 행동\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. 학습된 타깃 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 타깃 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = target_policy_probs(state)   # 현재 상태에서의 타깃 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동을 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "56f6a717-d0d0-4879-ab55-5f957c45f71f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56f6a717-d0d0-4879-ab55-5f957c45f71f",
        "outputId": "4e10c6d1-b8e0-40c8-bc0c-9aa8e9a5ecb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 PPO(Proximal Policy Optimization) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.662\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.832\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.856\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.848\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.905\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.903\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.901\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.907\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.912\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.914\n",
            "\n",
            "=== PPO 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.25908815 0.74091185]\n",
            "상태 1: [0.12482231 0.87517769]\n",
            "상태 2: [0.09811593 0.90188407]\n",
            "상태 3: [0.18027466 0.81972534]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-8) PPO(Proximal Policy Optimization): 신뢰 구간을 사용해 안정적으로 정책을 업데이트\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산용 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)     # 상태 0보다 왼쪽으로는 가지 않도록 최소값 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)  # 상태 4보다 오른쪽으로는 가지 않도록 최대값 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                       # 성공 보상 1.0 부여\n",
        "        done = True                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. PPO 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)    # 난수 시드 고정 (실행 결과 재현 가능)\n",
        "\n",
        "gamma = 0.99          # 할인율\n",
        "alpha_theta = 0.05    # Actor(정책 파라미터) 학습률\n",
        "alpha_v = 0.1         # Critic(가치함수) 학습률\n",
        "\n",
        "n_episodes = 500      # 전체 학습 에피소드 수\n",
        "max_steps  = 20       # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "eps_clip = 0.2        # PPO 클리핑 파라미터 ε (r_t를 1±ε 사이로 제한)\n",
        "ppo_epochs = 1        # 한 에피소드 수집 후 몇 번 반복 업데이트할지 (최소 구현으로 1회)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 파라미터 초기화\n",
        "# ==============================\n",
        "# 정책 파라미터 theta[s, a]\n",
        "theta = np.zeros((n_states, n_actions))   # 초기에는 모든 선호도를 0으로 시작\n",
        "\n",
        "# Critic: 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                    # 초기에는 모든 상태가치를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                      # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)                # 지수 함수 적용\n",
        "    return exp / np.sum(exp)                # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state, theta_param=None):\n",
        "    # 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta_param이 None이면 전역 theta를 사용\n",
        "    if theta_param is None:\n",
        "        theta_param = theta\n",
        "    return softmax(theta_param[state])      # theta[state]에 softmax 적용\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책 π(a|s; θ)를 이용하여 행동을 샘플링하는 함수\n",
        "    probs = policy_probs(state, theta)      # 상태 s에서의 행동 확률\n",
        "    action = np.random.choice(n_actions, p=probs)  # 확률에 따라 행동 선택\n",
        "    return action, probs                    # 선택한 행동과 해당 시점의 정책 확률 분포 반환\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ)를 계산하는 함수\n",
        "    probs = policy_probs(state, theta)      # π(a|s; θ)\n",
        "    grad = -probs.copy()                    # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                     # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                             # shape: (2,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. PPO 학습 루프\n",
        "# ==============================\n",
        "return_history = []                        # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 PPO(Proximal Policy Optimization) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                        # 에피소드 시작 상태\n",
        "    G0 = 0.0                               # 에피소드 Return(G_0) 추적용\n",
        "    discount = 1.0                         # γ^t 계수 누적용\n",
        "\n",
        "    # 1) 한 에피소드 trajectory 수집\n",
        "    states  = []                           # 상태 시퀀스\n",
        "    actions = []                           # 행동 시퀀스\n",
        "    rewards = []                           # 보상 시퀀스\n",
        "    dones   = []                           # done 플래그 시퀀스\n",
        "    old_action_probs = []                  # 각 시점에서의 π_old(a_t|s_t) (스칼라)\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 현재 정책으로 행동 샘플링\n",
        "        action, probs = sample_action(state)\n",
        "        # 행동에 해당하는 확률 π_old(a_t|s_t)를 기록\n",
        "        old_prob_a = probs[action]\n",
        "\n",
        "        # 환경에 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # trajectory에 저장\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "        old_action_probs.append(old_prob_a)\n",
        "\n",
        "        # Return(G_0) 계산용\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        state = next_state                 # 다음 상태로 전이\n",
        "\n",
        "        if done:\n",
        "            # 목표 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 2) 수집된 trajectory로부터 Monte Carlo return 및 Advantage 계산\n",
        "    T = len(states)                        # 에피소드 길이\n",
        "    returns = np.zeros(T)                  # 각 시점의 G_t\n",
        "    G = 0.0                                # 뒤에서부터 누적할 Return\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        # terminal이면 그대로 보상에서 시작, 아니면 할인 Return 누적\n",
        "        G = rewards[t] + gamma * G\n",
        "        returns[t] = G\n",
        "\n",
        "    # Advantage A_t = G_t - V(s_t)\n",
        "    advantages = np.zeros(T)\n",
        "    for t in range(T):\n",
        "        s = states[t]\n",
        "        advantages[t] = returns[t] - v[s]\n",
        "\n",
        "    # 3) Critic(V) 업데이트: V(s) ← V(s) + α_v * (G_t - V(s))\n",
        "    for t in range(T):\n",
        "        s = states[t]\n",
        "        v[s] += alpha_v * (returns[t] - v[s])\n",
        "\n",
        "    # 4) PPO Actor 업데이트 (최소 구현: 한 번의 에포크만 수행)\n",
        "    for _ in range(ppo_epochs):\n",
        "        for t in range(T):\n",
        "            s = states[t]                  # s_t\n",
        "            a = actions[t]                 # a_t\n",
        "            old_prob_a = old_action_probs[t]  # π_old(a_t|s_t)\n",
        "            A_t = advantages[t]            # Advantage A_t\n",
        "\n",
        "            # 현재 정책에서의 π_new(a_t|s_t)\n",
        "            new_probs = policy_probs(s, theta)\n",
        "            new_prob_a = new_probs[a]\n",
        "\n",
        "            # 확률 비율 r_t = π_new(a_t|s_t) / π_old(a_t|s_t)\n",
        "            ratio = new_prob_a / (old_prob_a + 1e-8)\n",
        "\n",
        "            # 클리핑된 비율 r_t^clip = clip(r_t, 1-ε, 1+ε)\n",
        "            clipped_ratio = np.clip(ratio, 1.0 - eps_clip, 1.0 + eps_clip)\n",
        "\n",
        "            # PPO의 핵심: L_t = min(r_t * A_t, r_t^clip * A_t)\n",
        "            # 여기서는 gradient를 단순화해서, 효과적인 ratio를 선택해 사용\n",
        "            if A_t >= 0:\n",
        "                # 이득이 양수면 너무 크게 늘어나지 않도록 상한을 적용\n",
        "                effective_ratio = min(ratio, clipped_ratio)\n",
        "            else:\n",
        "                # 이득이 음수면 너무 크게 줄어들지 않도록 하한을 적용\n",
        "                effective_ratio = max(ratio, clipped_ratio)\n",
        "\n",
        "            # 정책 gradient: ∇θ L ≈ effective_ratio * A_t * ∇θ log π(a_t|s_t)\n",
        "            grad = grad_log_policy(s, a)\n",
        "            theta[s] += alpha_theta * effective_ratio * A_t * grad\n",
        "\n",
        "    # 한 에피소드가 끝나면 Return(G_0)을 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== PPO 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s, theta)        # 상태 s에서의 정책 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현\n",
        "action_symbols = {0: \"←\", 1: \"→\"}        # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:                # 목표 상태인 경우\n",
        "        policy_str += \" G \"              # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s, theta))  # greedy 행동\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                          # 초기 상태로 리셋\n",
        "trajectory = [state]                     # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state, theta)   # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)            # 가장 확률이 높은 행동 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)        # 방문한 상태 기록\n",
        "    state = next_state                   # 다음 상태로 이동\n",
        "\n",
        "    if done:                             # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d035268e-7c39-4604-b67f-24b3cd319c4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d035268e-7c39-4604-b67f-24b3cd319c4a",
        "outputId": "2d049a77-b71c-4ed7-c070-7fd48aff861e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 TRPO(Trust Region Policy Optimization) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.823\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.937\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.939\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.939\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.940\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.939\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.941\n",
            "\n",
            "=== TRPO 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\n",
            "상태 0: [0.00282126 0.99717874]\n",
            "상태 1: [0.00164994 0.99835006]\n",
            "상태 2: [0.0072049 0.9927951]\n",
            "상태 3: [0.00456071 0.99543929]\n",
            "상태 4: [0.5 0.5]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-9) TRPO(Trust Region Policy Optimization): 정책 급변을 방지하는 최적화 기법\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산용 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 행동 개수: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 행동 action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)                     # 상태 0보다 왼쪽으로는 가지 않도록 제한\n",
        "    else:            # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)          # 상태 4보다 오른쪽으로는 가지 않도록 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:                         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                                       # 성공 보상 1.0 부여\n",
        "        done = True                                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. TRPO 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)        # 난수 시드 고정 (실행 결과 재현 가능)\n",
        "\n",
        "gamma = 0.99              # 할인율\n",
        "alpha_v = 0.1             # Critic(가치함수) 학습률\n",
        "\n",
        "n_episodes = 500          # 전체 학습 에피소드 수\n",
        "max_steps  = 20           # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "max_kl = 0.01             # Trust region의 최대 KL 제한 (대략적인 상한)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 파라미터 초기화\n",
        "# ==============================\n",
        "# 정책 파라미터 theta[s, a]\n",
        "theta = np.zeros((n_states, n_actions))   # 초기에는 모든 선호도를 0으로 시작\n",
        "\n",
        "# Critic: 상태가치 V(s)를 나타내는 벡터\n",
        "v = np.zeros(n_states)                    # 초기에는 모든 상태가치를 0으로 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. 정책 관련 함수 정의\n",
        "# ==============================\n",
        "def softmax(logits):\n",
        "    # 주어진 선호도 벡터 logits를 softmax를 이용해 확률 분포로 변환하는 함수\n",
        "    c = np.max(logits)                    # 수치 안정성을 위해 최대값을 빼줌\n",
        "    exp = np.exp(logits - c)              # 지수 함수 적용\n",
        "    return exp / np.sum(exp)              # 전체 합으로 나누어 확률 분포로 변환\n",
        "\n",
        "def policy_probs(state, theta_param=None):\n",
        "    # 상태 state에서의 행동 확률 π(a|s; θ)를 반환하는 함수\n",
        "    # theta_param이 None이면 전역 theta를 사용\n",
        "    if theta_param is None:\n",
        "        theta_param = theta               # 기본적으로 전역 theta 사용\n",
        "    return softmax(theta_param[state])    # theta[state]에 softmax 적용\n",
        "\n",
        "def sample_action(state):\n",
        "    # 현재 정책 π(a|s; θ)를 이용하여 행동을 샘플링하는 함수\n",
        "    probs = policy_probs(state, theta)    # 상태 s에서의 행동 확률\n",
        "    action = np.random.choice(n_actions, p=probs)  # 확률에 따라 행동 선택\n",
        "    return action, probs                  # 선택한 행동과 해당 시점의 정책 확률 분포 반환\n",
        "\n",
        "def grad_log_policy(state, action):\n",
        "    # ∇θ log π(a|s; θ)를 계산하는 함수\n",
        "    probs = policy_probs(state, theta)    # π(a|s; θ)\n",
        "    grad = -probs.copy()                  # 기본값은 -π(a|s; θ)\n",
        "    grad[action] += 1.0                   # 선택된 행동 a에 대해서는 +1 더해줌\n",
        "    # 결과적으로:\n",
        "    # grad[k] = 1 - π(k|s)  if k == action\n",
        "    # grad[k] = -π(k|s)     if k != action\n",
        "    return grad                           # shape: (2,)\n",
        "\n",
        "\n",
        "def fisher_matrix_from_probs(probs):\n",
        "    # tabular softmax에서 Fisher 정보 행렬 F = E[∇logπ ∇logπ^T]\n",
        "    # 이론적으로 F = diag(π) - π π^T 로 쓸 수 있음 (2x2 행렬)\n",
        "    diag = np.diag(probs)                 # diag(π)\n",
        "    outer = np.outer(probs, probs)        # π π^T\n",
        "    F = diag - outer                      # Fisher(2x2)\n",
        "    return F\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. TRPO 학습 루프\n",
        "# ==============================\n",
        "return_history = []                      # 각 에피소드의 총 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 TRPO(Trust Region Policy Optimization) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                      # 에피소드 시작 상태\n",
        "    G0 = 0.0                             # 에피소드 Return(G_0) 추적용\n",
        "    discount = 1.0                       # γ^t 계수 누적용\n",
        "\n",
        "    # 1) 한 에피소드 trajectory 수집\n",
        "    states  = []                         # 상태 시퀀스\n",
        "    actions = []                         # 행동 시퀀스\n",
        "    rewards = []                         # 보상 시퀀스\n",
        "    dones   = []                         # done 플래그 시퀀스\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 현재 정책으로 행동 샘플링\n",
        "        action, probs = sample_action(state)\n",
        "\n",
        "        # 환경에 적용\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # trajectory에 저장\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "\n",
        "        # Return(G_0) 계산용\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        state = next_state               # 다음 상태로 전이\n",
        "\n",
        "        if done:\n",
        "            # 목표 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 2) 수집된 trajectory로부터 Monte Carlo return 및 Advantage 계산\n",
        "    T = len(states)                      # 에피소드 길이\n",
        "    returns = np.zeros(T)                # 각 시점의 G_t\n",
        "    G = 0.0                              # 뒤에서부터 누적할 Return\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        # terminal이면 그대로 보상에서 시작, 아니면 할인 Return 누적\n",
        "        G = rewards[t] + gamma * G\n",
        "        returns[t] = G\n",
        "\n",
        "    # Advantage A_t = G_t - V(s_t)\n",
        "    advantages = np.zeros(T)\n",
        "    for t in range(T):\n",
        "        s = states[t]\n",
        "        advantages[t] = returns[t] - v[s]\n",
        "\n",
        "    # 3) Critic(V) 업데이트: V(s) ← V(s) + α_v * (G_t - V(s))\n",
        "    for t in range(T):\n",
        "        s = states[t]\n",
        "        v[s] += alpha_v * (returns[t] - v[s])\n",
        "\n",
        "    # 4) TRPO Actor 업데이트\n",
        "    #    상태별로 policy gradient g_s와 Fisher F_s를 계산하고,\n",
        "    #    natural gradient step n_s = F_s^{-1} g_s 를 구한 뒤\n",
        "    #    trust region 조건 (approx KL <= max_kl)에 맞게 스케일링해서 θ 업데이트\n",
        "\n",
        "    # 4-1) 상태별 policy gradient g_s 초기화 (각 상태마다 2차원 벡터)\n",
        "    g = np.zeros_like(theta)             # g[s, a]\n",
        "\n",
        "    for t in range(T):\n",
        "        s = states[t]                    # s_t\n",
        "        a = actions[t]                   # a_t\n",
        "        A_t = advantages[t]              # Advantage A_t\n",
        "\n",
        "        # ∇θ log π(a_t|s_t; θ)를 계산\n",
        "        grad = grad_log_policy(s, a)     # shape: (2,)\n",
        "        # g_s에 A_t * grad를 누적 (에피소드의 합)\n",
        "        g[s] += A_t * grad\n",
        "\n",
        "    # 4-2) 상태별 Fisher F_s와 natural step n_s 계산\n",
        "    nat_step = np.zeros_like(theta)      # natural gradient step n[s, a]\n",
        "    reg = 1e-3                           # 수치 안정성을 위한 작은 정규화 항\n",
        "\n",
        "    # 우선 상태별 F_s, n_s를 구하면서 전체 quad form도 계산\n",
        "    quad = 0.0                           # δθ^T F δθ (KL 근사에 사용)\n",
        "\n",
        "    for s in range(n_states):\n",
        "        # 상태 s에서의 현재 정책 확률\n",
        "        probs_s = policy_probs(s, theta) # π(a|s; θ)\n",
        "        # Fisher 행렬 F_s = diag(π) - π π^T\n",
        "        F_s = fisher_matrix_from_probs(probs_s)\n",
        "\n",
        "        # F_s에 작은 정규화 항을 더해 역행렬 계산 안정화\n",
        "        F_s_reg = F_s + reg * np.eye(n_actions)\n",
        "\n",
        "        # 자연 그래디언트: n_s = F_s^{-1} g_s\n",
        "        try:\n",
        "            inv_F_s = np.linalg.inv(F_s_reg)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # 혹시라도 역행렬 계산이 실패하면 항등행렬 사용 (fallback)\n",
        "            inv_F_s = np.eye(n_actions)\n",
        "\n",
        "        n_s = inv_F_s @ g[s]             # 2차원 벡터\n",
        "        nat_step[s] = n_s\n",
        "\n",
        "        # KL 근사에 사용될 δθ^T F δθ 항을 누적 (각 상태별로 더함)\n",
        "        quad += n_s.T @ (F_s @ n_s)\n",
        "\n",
        "    # 4-3) trust region 조건에 맞게 전체 step 스케일링\n",
        "    #      작은 스텝이면 quad가 거의 0일 수 있으므로 방어 코드 포함\n",
        "    if quad > 0.0:\n",
        "        # KL ≈ 0.5 * quad <= max_kl  이 되도록 scale 결정\n",
        "        scale = np.sqrt(2.0 * max_kl / quad)\n",
        "        scale = min(1.0, scale)         # 1을 넘지 않도록 제한 (너무 크게 업데이트 방지)\n",
        "    else:\n",
        "        scale = 1.0                     # quad가 0이면 스케일 그대로 1\n",
        "\n",
        "    # 4-4) 최종 θ 업데이트\n",
        "    theta += scale * nat_step           # TRPO 스타일 natural gradient + trust region 적용\n",
        "\n",
        "    # 5) 한 에피소드가 끝나면 Return(G_0)을 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력 (학습 진행 확인 용도)\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== TRPO 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 학습된 정책(상태별 행동 확률) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→])\")\n",
        "for s in range(n_states):\n",
        "    probs = policy_probs(s, theta)       # 상태 s에서의 정책 확률\n",
        "    print(f\"상태 {s}: {probs}\")\n",
        "\n",
        "# Greedy 정책으로 사람이 보기 쉽게 화살표로 표현\n",
        "action_symbols = {0: \"←\", 1: \"→\"}       # 0은 왼쪽 화살표, 1은 오른쪽 화살표\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:               # 목표 상태인 경우\n",
        "        policy_str += \" G \"             # Goal 표기\n",
        "    else:\n",
        "        best_a = np.argmax(policy_probs(s, theta))  # greedy 행동\n",
        "        policy_str += f\" {action_symbols[best_a]} \"\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책으로 1회 에피소드 실행 예시 (Greedy 정책 사용)\")\n",
        "\n",
        "state = reset()                         # 초기 상태로 리셋\n",
        "trajectory = [state]                    # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    probs = policy_probs(state, theta)  # 현재 상태에서의 정책 확률\n",
        "    action = np.argmax(probs)           # 가장 확률이 높은 행동 선택 (탐험 없이 greedy)\n",
        "    next_state, reward, done = step(state, action)  # 환경에 행동 적용\n",
        "\n",
        "    trajectory.append(next_state)       # 방문한 상태 기록\n",
        "    state = next_state                  # 다음 상태로 이동\n",
        "\n",
        "    if done:                            # 목표에 도달하면 종료\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "48174025-5d2e-40b7-ae6f-764ce826c3a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48174025-5d2e-40b7-ae6f-764ce826c3a3",
        "outputId": "ded04ac5-0725-440e-8b78-cf44de5e2587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 DDPG(Deep Deterministic Policy Gradient, NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.855, noise_std = 0.233\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.929, noise_std = 0.182\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.141\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.110\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.086\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.067\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.052\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.050\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.050\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.050\n",
            "\n",
            "=== DDPG 학습 종료 ===\n",
            "\n",
            "▶ 학습된 Actor의 결정론적 정책 μ(s) (상태별 연속 행동 값)\n",
            "상태 0: 행동 a = 0.6133\n",
            "상태 1: 행동 a = 0.9966\n",
            "상태 2: 행동 a = 0.9505\n",
            "상태 3: 행동 a = 0.4647\n",
            "상태 4: 행동 a = 0.7957\n",
            "\n",
            "▶ Greedy 기준 이산 정책(연속 행동 a의 부호를 기준으로 ←/→ 표시)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 결정론적 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-10) DDPG(Deep Deterministic Policy Gradient) : 연속적 행동 공간에서 학습하는 Actor-Critic 모델\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 불러오기\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 1       # DDPG에서는 연속 행동 1차원 (스칼라)\n",
        "\n",
        "def step(state, env_action):\n",
        "    # 환경에서 사용하는 행동 env_action은 이산 행동 (0=왼쪽, 1=오른쪽)\n",
        "    # 주어진 상태 state에서 env_action을 했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # 행동이 0이면 왼쪽으로 한 칸 이동, 1이면 오른쪽으로 한 칸 이동\n",
        "    if env_action == 0:  # 왼쪽 이동\n",
        "        next_state = max(0, state - 1)                     # 상태 0보다 왼쪽으로는 가지 않도록 제한\n",
        "    else:               # 오른쪽 이동\n",
        "        next_state = min(n_states - 1, state + 1)          # 상태 4보다 오른쪽으로는 가지 않도록 제한\n",
        "\n",
        "    # 보상과 종료 조건 설정\n",
        "    if next_state == n_states - 1:                         # 목표 상태(4)에 도달한 경우\n",
        "        reward = 1.0                                       # 성공 보상 1.0 부여\n",
        "        done = True                                        # 에피소드 종료\n",
        "    else:\n",
        "        reward = -0.01                                     # 그 외 상태에서는 시간 패널티 -0.01 부여 (빨리 도달 유도)\n",
        "        done = False                                       # 에피소드 계속 진행\n",
        "\n",
        "    return next_state, reward, done                        # 결과 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. 난수 시드 및 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)        # 난수 시드 고정 (실행 결과 재현 가능)\n",
        "\n",
        "gamma = 0.99              # 할인율\n",
        "tau = 0.01                # 타깃 네트워크 soft update 계수\n",
        "\n",
        "actor_lr = 0.01           # Actor 학습률\n",
        "critic_lr = 0.02          # Critic 학습률\n",
        "\n",
        "n_episodes = 500          # 전체 학습 에피소드 수\n",
        "max_steps  = 20           # 한 에피소드에서 허용하는 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 10000   # 리플레이 버퍼 최대 크기\n",
        "batch_size = 32           # 미니배치 크기\n",
        "start_learning = 100      # 일정 개수 이상 샘플이 쌓인 뒤부터 학습 시작\n",
        "\n",
        "noise_std_init = 0.3      # 초기 탐험 노이즈 표준편차\n",
        "noise_std_min = 0.05      # 최소 노이즈 크기\n",
        "noise_decay = 0.995       # 에피소드마다 노이즈 감소 비율\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 간단한 MLP 구현 (Critic 용)\n",
        "# ==============================\n",
        "class MLP:\n",
        "    # 입력 차원(in_dim) → hidden_dim → 1 출력 구조의 MLP를 정의\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        # Xavier 초기화를 사용하여 가중치 초기화\n",
        "        self.W1 = np.random.randn(in_dim, hidden_dim) / np.sqrt(in_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2 = np.zeros(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파 계산: x → h → q\n",
        "        # x: (in_dim,) 벡터\n",
        "        z1 = x @ self.W1 + self.b1           # 1층 선형결합\n",
        "        h = np.tanh(z1)                      # 1층 활성함수: tanh\n",
        "        z2 = h @ self.W2 + self.b2           # 2층 선형결합\n",
        "        q = z2[0]                            # 출력 스칼라\n",
        "        cache = (x, z1, h, z2, q)            # 역전파에 필요한 값들을 캐시에 저장\n",
        "        return q, cache\n",
        "\n",
        "    def backward(self, dq, cache, lr):\n",
        "        # Critic의 파라미터에 대한 역전파 및 파라미터 업데이트\n",
        "        # dq: dL/dq (스칼라, 여기서 L은 손실)\n",
        "        x, z1, h, z2, q = cache\n",
        "\n",
        "        dz2 = dq                             # 출력층 선형노드의 gradient\n",
        "        dW2 = np.outer(h, dz2)               # W2 gradient\n",
        "        db2 = dz2                            # b2 gradient\n",
        "\n",
        "        dh = self.W2.flatten() * dz2         # hidden 노드로 전파된 gradient\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)  # tanh 미분\n",
        "\n",
        "        dW1 = np.outer(x, dz1)               # W1 gradient\n",
        "        db1 = dz1                            # b1 gradient\n",
        "\n",
        "        # 파라미터 업데이트 (gradient descent)\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "\n",
        "    def backward_input(self, dq, cache):\n",
        "        # 입력 x에 대한 gradient (∂L/∂x)를 계산하는 함수\n",
        "        # Actor 업데이트에서 ∂Q/∂a를 얻기 위해 사용\n",
        "        x, z1, h, z2, q = cache\n",
        "\n",
        "        dz2 = dq                             # dL/dz2\n",
        "        dh = self.W2.flatten() * dz2         # dL/dh\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)  # dL/dz1\n",
        "        dx = dz1 @ self.W1.T                 # dL/dx\n",
        "        return dx                            # x와 같은 차원의 벡터\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. Actor 네트워크 구현\n",
        "# ==============================\n",
        "class Actor:\n",
        "    # 입력은 상태(one-hot), 출력은 연속 행동 a ∈ [-1, 1]\n",
        "    def __init__(self, state_dim, hidden_dim):\n",
        "        in_dim = state_dim\n",
        "        self.W1 = np.random.randn(in_dim, hidden_dim) / np.sqrt(in_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2 = np.zeros(1)\n",
        "\n",
        "    def forward(self, s_vec):\n",
        "        # 상태 벡터 s_vec (one-hot) → 행동 a 를 출력\n",
        "        z1 = s_vec @ self.W1 + self.b1              # 1층 선형\n",
        "        h = np.tanh(z1)                             # tanh 활성화\n",
        "        z2 = h @ self.W2 + self.b2                  # 2층 선형\n",
        "        a = np.tanh(z2[0])                          # 출력도 tanh로 [-1,1] 범위로 제한\n",
        "        cache = (s_vec, z1, h, z2, a)               # 역전파를 위한 캐시\n",
        "        return a, cache\n",
        "\n",
        "    def backward(self, da, cache, lr):\n",
        "        # Actor에 대한 역전파: da = dJ/da (J는 maximize할 목적함수)\n",
        "        # 실제 구현에서는 L = -J 로 두고 gradient descent 수행\n",
        "        # 즉, dL/da = -dJ/da 이므로 입력으로 넘어오는 da는 dL/da로 해석\n",
        "        s_vec, z1, h, z2, a = cache\n",
        "\n",
        "        # a = tanh(z2) 이므로 da/dz2 = 1 - a^2\n",
        "        dz2 = da * (1.0 - a ** 2)                  # dL/dz2\n",
        "        dW2 = np.outer(h, dz2)                     # W2 gradient\n",
        "        db2 = dz2                                  # b2 gradient\n",
        "\n",
        "        dh = self.W2.flatten() * dz2               # dL/dh\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)        # tanh 미분\n",
        "        dW1 = np.outer(s_vec, dz1)                 # W1 gradient\n",
        "        db1 = dz1                                  # b1 gradient\n",
        "\n",
        "        # 파라미터 업데이트 (gradient descent)\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 타깃 네트워크 soft update 함수\n",
        "# ==============================\n",
        "def soft_update(target, source, tau):\n",
        "    # target 파라미터를 source 파라미터 쪽으로 조금씩 이동시키는 함수\n",
        "    # θ_target ← τ θ + (1-τ) θ_target\n",
        "    for attr in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "        target_param = getattr(target, attr)\n",
        "        source_param = getattr(source, attr)\n",
        "        new_param = tau * source_param + (1.0 - tau) * target_param\n",
        "        setattr(target, attr, new_param.copy())\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 리플레이 버퍼 구현\n",
        "# ==============================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity          # 최대 저장 개수\n",
        "        self.buffer = []                  # (s, a, r, s', done) 튜플 리스트\n",
        "        self.position = 0                 # 다음에 덮어쓸 위치 인덱스\n",
        "\n",
        "    def push(self, s, a, r, ns, done):\n",
        "        # 새로운 transition을 버퍼에 추가\n",
        "        data = (s, a, r, ns, done)\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(data)\n",
        "        else:\n",
        "            self.buffer[self.position] = data\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # 버퍼에서 임의의 미니배치 샘플링\n",
        "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        batch = [self.buffer[i] for i in idxs]\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        # 현재 버퍼에 쌓인 transition 수 반환\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. DDPG 초기 설정\n",
        "# ==============================\n",
        "state_dim = n_states      # 상태를 one-hot으로 표현\n",
        "actor = Actor(state_dim, hidden_dim=16)           # 기본 Actor\n",
        "actor_target = Actor(state_dim, hidden_dim=16)    # 타깃 Actor\n",
        "\n",
        "critic = MLP(in_dim=state_dim + n_actions, hidden_dim=32)          # 기본 Critic\n",
        "critic_target = MLP(in_dim=state_dim + n_actions, hidden_dim=32)   # 타깃 Critic\n",
        "\n",
        "# 타깃 네트워크를 초기에는 동일하게 맞춤\n",
        "soft_update(actor_target, actor, tau=1.0)\n",
        "soft_update(critic_target, critic, tau=1.0)\n",
        "\n",
        "buffer = ReplayBuffer(buffer_capacity)  # 리플레이 버퍼 생성\n",
        "noise_std = noise_std_init             # 탐험 노이즈 초기값\n",
        "\n",
        "return_history = []                    # 에피소드별 Return(G_0)을 기록할 리스트\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 DDPG(Deep Deterministic Policy Gradient, NumPy) 학습 시작 ===\")\n",
        "\n",
        "# ==============================\n",
        "# 8. 학습 루프\n",
        "# ==============================\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()                    # 에피소드 시작 상태\n",
        "    G0 = 0.0                           # 에피소드 Return(G_0)\n",
        "    discount = 1.0                     # γ^t 계수 누적용\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) 상태를 one-hot 벡터로 변환\n",
        "        s_vec = np.zeros(state_dim)\n",
        "        s_vec[state] = 1.0\n",
        "\n",
        "        # 2) Actor에서 현재 상태에 대한 행동 a = μ(s) 계산\n",
        "        a_det, _ = actor.forward(s_vec)\n",
        "\n",
        "        # 3) 탐험을 위한 가우시안 노이즈 추가\n",
        "        a = a_det + noise_std * np.random.randn()\n",
        "        # 행동 범위를 [-1, 1]로 클리핑\n",
        "        a = np.clip(a, -1.0, 1.0)\n",
        "\n",
        "        # 4) 연속 행동 a를 이산 행동 env_action으로 매핑\n",
        "        #    a < 0 이면 왼쪽(0), a >= 0 이면 오른쪽(1)\n",
        "        env_action = 0 if a < 0.0 else 1\n",
        "\n",
        "        # 5) 환경에 적용하여 다음 상태, 보상, 종료 여부를 얻음\n",
        "        next_state, reward, done = step(state, env_action)\n",
        "\n",
        "        # 6) 리플레이 버퍼에 transition 저장\n",
        "        buffer.push(state, a, reward, next_state, done)\n",
        "\n",
        "        # 7) 학습 시작 조건이 되면 DDPG 업데이트 수행\n",
        "        if len(buffer) >= max(start_learning, batch_size):\n",
        "            # 미니배치 샘플링\n",
        "            batch = buffer.sample(batch_size)\n",
        "\n",
        "            for (s_b, a_b, r_b, ns_b, d_b) in batch:\n",
        "                # 상태 s_b를 one-hot으로 변환\n",
        "                s_vec_b = np.zeros(state_dim)\n",
        "                s_vec_b[s_b] = 1.0\n",
        "\n",
        "                # 다음 상태 ns_b 또한 one-hot으로 변환\n",
        "                ns_vec_b = np.zeros(state_dim)\n",
        "                ns_vec_b[ns_b] = 1.0\n",
        "\n",
        "                # 타깃 Actor로부터 다음 상태에서의 행동 a'\n",
        "                a_next, _ = actor_target.forward(ns_vec_b)\n",
        "\n",
        "                # Critic 타깃 네트워크로부터 Q_target(ns, a')\n",
        "                x_next = np.concatenate([ns_vec_b, np.array([a_next])])\n",
        "                q_next, _ = critic_target.forward(x_next)\n",
        "\n",
        "                # TD Target 계산: y = r + γ * (1-done) * q_next\n",
        "                y = r_b + (0.0 if d_b else gamma * q_next)\n",
        "\n",
        "                # 현재 Critic으로부터 Q(s, a)를 계산\n",
        "                x_curr = np.concatenate([s_vec_b, np.array([a_b])])\n",
        "                q_curr, cache_c = critic.forward(x_curr)\n",
        "\n",
        "                # Critic 손실 L = 0.5 * (q_curr - y)^2\n",
        "                # dL/dq_curr = (q_curr - y)\n",
        "                dq = (q_curr - y)\n",
        "                critic.backward(dq, cache_c, critic_lr)\n",
        "\n",
        "                # Actor 업데이트:\n",
        "                # J ≈ E_s[ Q(s, μ(s)) ] 를 최대화\n",
        "                # L_actor = -Q(s, μ(s))\n",
        "                # dL_actor/dQ = -1\n",
        "                a_policy, cache_a = actor.forward(s_vec_b)\n",
        "                x_actor = np.concatenate([s_vec_b, np.array([a_policy])])\n",
        "                q_for_actor, cache_c2 = critic.forward(x_actor)\n",
        "\n",
        "                # ∂L_actor/∂x = ∂L_actor/∂Q * ∂Q/∂x = -1 * ∂Q/∂x\n",
        "                dx = critic.backward_input(-1.0, cache_c2)\n",
        "\n",
        "                # x = [s_vec, a] 이므로 마지막 원소가 행동 a에 대한 gradient\n",
        "                da = dx[-1]\n",
        "\n",
        "                # Actor 파라미터 업데이트 (gradient descent on L_actor)\n",
        "                actor.backward(da, cache_a, actor_lr)\n",
        "\n",
        "                # 타깃 네트워크 soft update\n",
        "                soft_update(actor_target, actor, tau)\n",
        "                soft_update(critic_target, critic, tau)\n",
        "\n",
        "        # 8) Return(G_0) 계산용 누적\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        # 9) 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # 목표에 도달하면 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 에피소드가 끝난 후, 에피소드 Return 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 탐험 노이즈 감소\n",
        "    noise_std = max(noise_std_min, noise_std * noise_decay)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균을 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}, noise_std = {noise_std:.3f}\")\n",
        "\n",
        "print(\"\\n=== DDPG 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 9. 학습된 결정론적 정책 μ(s) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 Actor의 결정론적 정책 μ(s) (상태별 연속 행동 값)\")\n",
        "for s in range(n_states):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[s] = 1.0\n",
        "    a_det, _ = actor.forward(s_vec)\n",
        "    print(f\"상태 {s}: 행동 a = {a_det:.4f}\")\n",
        "\n",
        "# Greedy 정책(연속 행동의 부호를 이산 행동으로 변환) 출력\n",
        "print(\"\\n▶ Greedy 기준 이산 정책(연속 행동 a의 부호를 기준으로 ←/→ 표시)\")\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[s] = 1.0\n",
        "    a_det, _ = actor.forward(s_vec)\n",
        "    env_action = 0 if a_det < 0.0 else 1\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        policy_str += f\" {action_symbols[env_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 10. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 결정론적 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()                      # 초기 상태로 리셋\n",
        "trajectory = [state]                 # 방문한 상태들을 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[state] = 1.0\n",
        "    # 탐험 없이 결정론적 정책 a = μ(s) 사용\n",
        "    a_det, _ = actor.forward(s_vec)\n",
        "    env_action = 0 if a_det < 0.0 else 1\n",
        "\n",
        "    next_state, reward, done = step(state, env_action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "50fdd0a3-9546-40a0-89d7-16e83d17fdf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50fdd0a3-9546-40a0-89d7-16e83d17fdf1",
        "outputId": "bb1a0f47-215a-4c5b-9d0e-2f3e3bfacd53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 TD3(Twin Delayed DDPG, NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.861, noise_std = 0.233\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.182\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.940, noise_std = 0.141\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.110\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.086\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.067\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.052\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.050\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.050\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.941, noise_std = 0.050\n",
            "\n",
            "=== TD3 학습 종료 ===\n",
            "\n",
            "▶ 학습된 Actor의 결정론적 정책 μ(s) (상태별 연속 행동 값)\n",
            "상태 0: 행동 a = 0.7023\n",
            "상태 1: 행동 a = 0.9915\n",
            "상태 2: 행동 a = 0.9786\n",
            "상태 3: 행동 a = 0.4965\n",
            "상태 4: 행동 a = 0.8042\n",
            "\n",
            "▶ Greedy 기준 이산 정책(연속 행동 a의 부호를 기준으로 ←/→ 표시)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 결정론적 정책으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-11) TD3(Twin Delayed DDPG) : DDPG의 한계점을 극복하기 위한 개선된 모델\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy 임포트\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태 개수: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 1       # TD3에서 사용할 연속 행동 차원 (스칼라)\n",
        "\n",
        "def step(state, env_action):\n",
        "    # 주어진 상태 state에서 이산 행동 env_action(0=왼쪽, 1=오른쪽)을 수행했을 때\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환하는 함수\n",
        "\n",
        "    # env_action이 0이면 왼쪽 이동, 1이면 오른쪽 이동\n",
        "    if env_action == 0:  # 왼쪽\n",
        "        next_state = max(0, state - 1)                 # 상태는 0보다 작아지지 않도록 제한\n",
        "    else:               # 오른쪽\n",
        "        next_state = min(n_states - 1, state + 1)      # 상태는 4보다 커지지 않도록 제한\n",
        "\n",
        "    # 목표 상태(4)에 도달하면 보상 1.0, 에피소드 종료\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0\n",
        "        done = True\n",
        "    else:\n",
        "        reward = -0.01                                 # 그 외에는 시간 패널티 -0.01\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done                    # 다음 상태, 보상, 종료 여부 반환\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태를 반환하는 함수\n",
        "    # 여기서는 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. 난수 시드 및 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)        # 난수 시드 고정 (실행 결과 재현성 확보)\n",
        "\n",
        "gamma = 0.99              # 할인율 (미래 보상 가중치)\n",
        "tau = 0.01                # 타깃 네트워크 soft update 계수\n",
        "\n",
        "actor_lr = 0.01           # Actor 학습률\n",
        "critic_lr = 0.02          # Critic 학습률\n",
        "\n",
        "n_episodes = 500          # 전체 학습 에피소드 수\n",
        "max_steps  = 20           # 한 에피소드 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 10000   # 리플레이 버퍼 최대 크기\n",
        "batch_size = 32           # 미니배치 크기\n",
        "start_learning = 100      # 이 이상 샘플이 쌓인 후부터 학습 시작\n",
        "\n",
        "noise_std_init = 0.3      # 행동 탐험용 가우시안 노이즈 초기 표준편차\n",
        "noise_std_min = 0.05      # 행동 탐험 노이즈 최소값\n",
        "noise_decay = 0.995       # 에피소드마다 노이즈 감소 비율\n",
        "\n",
        "target_noise_std = 0.2    # 타깃 정책에 더하는 노이즈 표준편차 (Target Policy Smoothing)\n",
        "target_noise_clip = 0.5   # 타깃 정책 노이즈 클리핑 한계\n",
        "\n",
        "policy_delay = 2          # Critic 여러 번 업데이트 후 1번 Actor 업데이트 (Delayed Policy)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 간단한 MLP 구현 (Critic 용)\n",
        "# ==============================\n",
        "class MLP:\n",
        "    # 입력(in_dim) → hidden_dim → 1 출력 구조의 단순 MLP\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        # Xavier 초기화를 사용하여 가중치 초기화\n",
        "        self.W1 = np.random.randn(in_dim, hidden_dim) / np.sqrt(in_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2 = np.zeros(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파 계산: x → h → q\n",
        "        # x: (in_dim,) 형태의 벡터\n",
        "        z1 = x @ self.W1 + self.b1           # 1층 선형결합\n",
        "        h = np.tanh(z1)                      # 1층 활성화 함수: tanh\n",
        "        z2 = h @ self.W2 + self.b2           # 2층 선형결합\n",
        "        q = z2[0]                            # 스칼라 출력\n",
        "        cache = (x, z1, h, z2, q)            # 역전파용 캐시\n",
        "        return q, cache\n",
        "\n",
        "    def backward(self, dq, cache, lr):\n",
        "        # Critic 파라미터에 대한 역전파 및 업데이트\n",
        "        # dq: dL/dq (스칼라, L은 손실)\n",
        "        x, z1, h, z2, q = cache\n",
        "\n",
        "        dz2 = dq                             # dL/dz2\n",
        "        dW2 = np.outer(h, dz2)               # W2에 대한 gradient\n",
        "        db2 = dz2                            # b2에 대한 gradient\n",
        "\n",
        "        dh = self.W2.flatten() * dz2         # hidden 층으로 전파된 gradient\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)  # tanh 미분\n",
        "        dW1 = np.outer(x, dz1)               # W1 gradient\n",
        "        db1 = dz1                            # b1 gradient\n",
        "\n",
        "        # 파라미터 업데이트 (gradient descent)\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "\n",
        "    def backward_input(self, dq, cache):\n",
        "        # 입력 x에 대한 gradient ∂L/∂x 계산\n",
        "        # Actor 업데이트 시 ∂Q/∂a 를 얻기 위해 사용\n",
        "        x, z1, h, z2, q = cache\n",
        "\n",
        "        dz2 = dq                             # dL/dz2\n",
        "        dh = self.W2.flatten() * dz2         # dL/dh\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)  # dL/dz1\n",
        "        dx = dz1 @ self.W1.T                 # dL/dx\n",
        "        return dx\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. Actor 네트워크 구현\n",
        "# ==============================\n",
        "class Actor:\n",
        "    # 상태 one-hot 벡터 → 연속 행동 a ∈ [-1, 1] 출력\n",
        "    def __init__(self, state_dim, hidden_dim):\n",
        "        in_dim = state_dim\n",
        "        self.W1 = np.random.randn(in_dim, hidden_dim) / np.sqrt(in_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2 = np.zeros(1)\n",
        "\n",
        "    def forward(self, s_vec):\n",
        "        # 상태 벡터 s_vec (one-hot) → 행동 a를 출력\n",
        "        z1 = s_vec @ self.W1 + self.b1               # 1층 선형결합\n",
        "        h = np.tanh(z1)                              # tanh 활성화\n",
        "        z2 = h @ self.W2 + self.b2                   # 2층 선형결합\n",
        "        a = np.tanh(z2[0])                           # 출력 tanh로 [-1,1]로 제한\n",
        "        cache = (s_vec, z1, h, z2, a)                # 역전파용 캐시\n",
        "        return a, cache\n",
        "\n",
        "    def backward(self, da, cache, lr):\n",
        "        # Actor에 대한 역전파: da = dL/da (L은 최소화할 손실)\n",
        "        # 여기서는 L_actor = -Q(s, μ(s)) 이므로 dL/da = -∂Q/∂a 와 동일 개념\n",
        "        s_vec, z1, h, z2, a = cache\n",
        "\n",
        "        # a = tanh(z2) 이므로 da/dz2 = 1 - a^2\n",
        "        dz2 = da * (1.0 - a ** 2)                   # dL/dz2\n",
        "        dW2 = np.outer(h, dz2)                      # W2 gradient\n",
        "        db2 = dz2                                   # b2 gradient\n",
        "\n",
        "        dh = self.W2.flatten() * dz2                # dL/dh\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)         # tanh 미분\n",
        "        dW1 = np.outer(s_vec, dz1)                  # W1 gradient\n",
        "        db1 = dz1                                   # b1 gradient\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 타깃 네트워크 soft update 함수\n",
        "# ==============================\n",
        "def soft_update(target, source, tau):\n",
        "    # θ_target ← τ θ_source + (1 - τ) θ_target 형태의 soft update\n",
        "    for attr in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "        target_param = getattr(target, attr)\n",
        "        source_param = getattr(source, attr)\n",
        "        new_param = tau * source_param + (1.0 - tau) * target_param\n",
        "        setattr(target, attr, new_param.copy())\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 리플레이 버퍼 구현\n",
        "# ==============================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity          # 최대 저장 용량\n",
        "        self.buffer = []                  # (s, a, r, s', done) 저장 리스트\n",
        "        self.position = 0                 # 덮어쓸 위치 인덱스\n",
        "\n",
        "    def push(self, s, a, r, ns, done):\n",
        "        # 새로운 transition을 버퍼에 추가\n",
        "        data = (s, a, r, ns, done)\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(data)\n",
        "        else:\n",
        "            self.buffer[self.position] = data\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # 버퍼에서 랜덤하게 미니배치를 추출\n",
        "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        batch = [self.buffer[i] for i in idxs]\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        # 현재 버퍼에 들어있는 transition 수\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. TD3 네트워크 초기 설정\n",
        "# ==============================\n",
        "state_dim = n_states\n",
        "\n",
        "# Actor 및 타깃 Actor 생성\n",
        "actor = Actor(state_dim, hidden_dim=16)\n",
        "actor_target = Actor(state_dim, hidden_dim=16)\n",
        "\n",
        "# Critic1, Critic2 및 각 타깃 네트워크 생성 (Twin Critic 구조)\n",
        "critic1 = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "critic2 = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "critic1_target = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "critic2_target = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "\n",
        "# 타깃 네트워크를 초기에는 원본 네트워크와 동일하게 설정\n",
        "soft_update(actor_target, actor, tau=1.0)\n",
        "soft_update(critic1_target, critic1, tau=1.0)\n",
        "soft_update(critic2_target, critic2, tau=1.0)\n",
        "\n",
        "buffer = ReplayBuffer(buffer_capacity)    # 리플레이 버퍼 생성\n",
        "noise_std = noise_std_init               # 행동 탐험 노이즈 초기값\n",
        "\n",
        "return_history = []                      # 에피소드별 Return(G_0)을 기록할 리스트\n",
        "update_step = 0                          # Critic 업데이트 횟수(Actor 딜레이 업데이트에 사용)\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 TD3(Twin Delayed DDPG, NumPy) 학습 시작 ===\")\n",
        "\n",
        "# ==============================\n",
        "# 8. 학습 루프\n",
        "# ==============================\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()           # 에피소드 시작 상태\n",
        "    G0 = 0.0                  # 에피소드 Return(G_0)\n",
        "    discount = 1.0            # γ^t 계산용\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) 상태를 one-hot 벡터로 변환\n",
        "        s_vec = np.zeros(state_dim)\n",
        "        s_vec[state] = 1.0\n",
        "\n",
        "        # 2) Actor로부터 결정론적 행동 a_det = μ(s) 계산\n",
        "        a_det, _ = actor.forward(s_vec)\n",
        "\n",
        "        # 3) 탐험을 위해 가우시안 노이즈를 추가\n",
        "        a = a_det + noise_std * np.random.randn()\n",
        "        a = np.clip(a, -1.0, 1.0)         # 행동 범위를 [-1, 1]로 제한\n",
        "\n",
        "        # 4) 연속 행동 a를 이산 행동 env_action으로 매핑\n",
        "        env_action = 0 if a < 0.0 else 1  # a < 0 → 왼쪽, a >= 0 → 오른쪽\n",
        "\n",
        "        # 5) 환경에 적용하여 다음 상태, 보상, 종료 여부 얻기\n",
        "        next_state, reward, done = step(state, env_action)\n",
        "\n",
        "        # 6) 리플레이 버퍼에 (s, a, r, s', done) 저장\n",
        "        buffer.push(state, a, reward, next_state, done)\n",
        "\n",
        "        # 7) 충분히 샘플이 쌓이면 TD3 학습 수행\n",
        "        if len(buffer) >= max(start_learning, batch_size):\n",
        "            # 미니배치 샘플링\n",
        "            batch = buffer.sample(batch_size)\n",
        "            update_step += 1  # Critic 업데이트 횟수 증가\n",
        "\n",
        "            for (s_b, a_b, r_b, ns_b, d_b) in batch:\n",
        "                # 상태 s_b, 다음 상태 ns_b를 one-hot 벡터로 변환\n",
        "                s_vec_b = np.zeros(state_dim)\n",
        "                s_vec_b[s_b] = 1.0\n",
        "                ns_vec_b = np.zeros(state_dim)\n",
        "                ns_vec_b[ns_b] = 1.0\n",
        "\n",
        "                # 타깃 Actor로부터 다음 상태에서의 행동 a'_det 계산\n",
        "                a_next_det, _ = actor_target.forward(ns_vec_b)\n",
        "\n",
        "                # Target Policy Smoothing: a'_det에 노이즈 추가 후 클리핑\n",
        "                noise = target_noise_std * np.random.randn()\n",
        "                noise = np.clip(noise, -target_noise_clip, target_noise_clip)\n",
        "                a_next = a_next_det + noise\n",
        "                a_next = np.clip(a_next, -1.0, 1.0)\n",
        "\n",
        "                # Critic 타깃 네트워크들로부터 Q1', Q2' 계산\n",
        "                x_next = np.concatenate([ns_vec_b, np.array([a_next])])\n",
        "                q1_next, _ = critic1_target.forward(x_next)\n",
        "                q2_next, _ = critic2_target.forward(x_next)\n",
        "\n",
        "                # Twin Critic의 최소값 사용 (TD3의 핵심)\n",
        "                q_next_min = min(q1_next, q2_next)\n",
        "\n",
        "                # TD Target 계산: y = r + γ * (1-done) * q_next_min\n",
        "                y = r_b + (0.0 if d_b else gamma * q_next_min)\n",
        "\n",
        "                # 현재 Critic1, Critic2로부터 Q1(s,a), Q2(s,a) 계산\n",
        "                x_curr = np.concatenate([s_vec_b, np.array([a_b])])\n",
        "                q1_curr, cache_c1 = critic1.forward(x_curr)\n",
        "                q2_curr, cache_c2 = critic2.forward(x_curr)\n",
        "\n",
        "                # Critic 손실에 대한 gradient: dL/dQi = (Qi - y)\n",
        "                dq1 = (q1_curr - y)\n",
        "                dq2 = (q2_curr - y)\n",
        "\n",
        "                # Critic1, Critic2 각각 역전파 및 업데이트\n",
        "                critic1.backward(dq1, cache_c1, critic_lr)\n",
        "                critic2.backward(dq2, cache_c2, critic_lr)\n",
        "\n",
        "                # Critic 타깃 네트워크 soft update\n",
        "                soft_update(critic1_target, critic1, tau)\n",
        "                soft_update(critic2_target, critic2, tau)\n",
        "\n",
        "            # Delayed Policy Update: 일정 횟수마다 Actor 및 Actor 타깃 업데이트\n",
        "            if update_step % policy_delay == 0:\n",
        "                for (s_b, a_b, r_b, ns_b, d_b) in batch:\n",
        "                    # Actor 업데이트는 상태 분포에 대해 J ≈ E[Q1(s, μ(s))]를 최대화\n",
        "                    s_vec_b = np.zeros(state_dim)\n",
        "                    s_vec_b[s_b] = 1.0\n",
        "\n",
        "                    # 현재 Actor 정책으로부터 행동 a_policy = μ(s) 계산\n",
        "                    a_policy, cache_a = actor.forward(s_vec_b)\n",
        "\n",
        "                    # Critic1으로 Q1(s, μ(s)) 계산\n",
        "                    x_actor = np.concatenate([s_vec_b, np.array([a_policy])])\n",
        "                    q_for_actor, cache_c_for_actor = critic1.forward(x_actor)\n",
        "\n",
        "                    # Actor 손실 L_actor = -Q1(s, μ(s))\n",
        "                    # dL/dQ = -1 이므로 backward_input에 -1 전달\n",
        "                    dx = critic1.backward_input(-1.0, cache_c_for_actor)\n",
        "\n",
        "                    # x = [s_vec, a] 이므로 마지막 원소가 a에 대한 gradient\n",
        "                    da = dx[-1]\n",
        "\n",
        "                    # Actor 파라미터 업데이트 (gradient descent on L_actor)\n",
        "                    actor.backward(da, cache_a, actor_lr)\n",
        "\n",
        "                # Actor 타깃 네트워크도 soft update\n",
        "                soft_update(actor_target, actor, tau)\n",
        "\n",
        "        # 8) 에피소드 Return(G_0) 누적\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        # 9) 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # 목표 상태에 도달하면 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 에피소드 종료 후 Return 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 탐험 노이즈 감소\n",
        "    noise_std = max(noise_std_min, noise_std * noise_decay)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균과 현재 노이즈 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}, noise_std = {noise_std:.3f}\")\n",
        "\n",
        "print(\"\\n=== TD3 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 9. 학습된 결정론적 정책 μ(s) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 Actor의 결정론적 정책 μ(s) (상태별 연속 행동 값)\")\n",
        "for s in range(n_states):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[s] = 1.0\n",
        "    a_det, _ = actor.forward(s_vec)\n",
        "    print(f\"상태 {s}: 행동 a = {a_det:.4f}\")\n",
        "\n",
        "# Greedy 기준 이산 정책(행동 부호를 기준으로 ←/→) 출력\n",
        "print(\"\\n▶ Greedy 기준 이산 정책(연속 행동 a의 부호를 기준으로 ←/→ 표시)\")\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[s] = 1.0\n",
        "    a_det, _ = actor.forward(s_vec)\n",
        "    env_action = 0 if a_det < 0.0 else 1\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        policy_str += f\" {action_symbols[env_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 10. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 결정론적 정책으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()          # 초기 상태로 리셋\n",
        "trajectory = [state]     # 방문한 상태를 기록하기 위한 리스트\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[state] = 1.0\n",
        "\n",
        "    # 탐험 없이 결정론적 정책 a_det = μ(s) 사용\n",
        "    a_det, _ = actor.forward(s_vec)\n",
        "    env_action = 0 if a_det < 0.0 else 1\n",
        "\n",
        "    next_state, reward, done = step(state, env_action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "746a745e-d6fd-41de-8a4f-66037942deba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "746a745e-d6fd-41de-8a4f-66037942deba",
        "outputId": "5055ea53-31e9-44c7-8339-ad8563eb409b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 SAC(Soft Actor-Critic, NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.891\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.941\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.941\n",
            "\n",
            "=== SAC 학습 종료 ===\n",
            "\n",
            "▶ 학습된 Actor의 평균 정책 μ(s) (상태별 연속 행동 평균 값)\n",
            "상태 0: μ = 918.1782\n",
            "상태 1: μ = 918.2812\n",
            "상태 2: μ = 918.2171\n",
            "상태 3: μ = 918.2215\n",
            "상태 4: μ = 916.9561\n",
            "\n",
            "▶ Greedy 기준 이산 정책(μ(s)의 부호 기준 ←/→)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 평균 정책(μ)을 이용한 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-12) SAC(Soft Actor-Critic): 탐색과 활용의 균형을 유지하도록 설계된 정책 학습 모델\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산용 NumPy\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 1       # 연속 행동 차원 (스칼라)\n",
        "\n",
        "def step(state, env_action):\n",
        "    # 주어진 상태 state에서 이산 행동 env_action(0=왼쪽, 1=오른쪽)을 수행\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환\n",
        "\n",
        "    # 행동에 따라 상태 이동\n",
        "    if env_action == 0:           # 왼쪽\n",
        "        next_state = max(0, state - 1)\n",
        "    else:                         # 오른쪽\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    # 목표 상태(4)에 도달하면 종료 + 보상 1.0\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0\n",
        "        done = True\n",
        "    else:\n",
        "        reward = -0.01            # 그 외에는 시간 패널티 -0.01\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 초기 상태 반환\n",
        "    return 0                      # 항상 상태 0에서 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. 난수 시드 및 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)                # 시드 고정 (실행 결과 재현성 확보)\n",
        "\n",
        "gamma = 0.99                      # 할인율\n",
        "alpha = 0.2                       # 온도 파라미터(Entropy 가중치)\n",
        "\n",
        "tau = 0.01                        # 타깃 네트워크 soft update 계수\n",
        "\n",
        "actor_lr = 0.01                   # Actor 학습률\n",
        "critic_lr = 0.02                  # Critic 학습률\n",
        "\n",
        "n_episodes = 500                  # 학습 에피소드 수\n",
        "max_steps  = 20                   # 에피소드당 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 10000           # 리플레이 버퍼 용량\n",
        "batch_size = 32                   # 미니배치 크기\n",
        "start_learning = 100              # 버퍼에 이 이상 쌓이면 학습 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. MLP (Critic용) 정의\n",
        "# ==============================\n",
        "class MLP:\n",
        "    # 입력(in_dim) → hidden_dim → 1 출력 스칼라 Q값\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        # Xavier 초기화\n",
        "        self.W1 = np.random.randn(in_dim, hidden_dim) / np.sqrt(in_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "        self.W2 = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2 = np.zeros(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파: x → h → q\n",
        "        # x: (in_dim,) 벡터\n",
        "        z1 = x @ self.W1 + self.b1         # 1층 선형 결합\n",
        "        h = np.tanh(z1)                    # tanh 활성화\n",
        "        z2 = h @ self.W2 + self.b2         # 2층 선형 결합\n",
        "        q = z2[0]                          # 스칼라 출력\n",
        "        cache = (x, z1, h, z2, q)          # 역전파용 캐시\n",
        "        return q, cache\n",
        "\n",
        "    def backward(self, dq, cache, lr):\n",
        "        # Critic 파라미터에 대한 역전파\n",
        "        # dq: dL/dq (스칼라)\n",
        "        x, z1, h, z2, q = cache\n",
        "\n",
        "        dz2 = dq                           # dL/dz2\n",
        "        dW2 = np.outer(h, dz2)             # W2 gradient\n",
        "        db2 = dz2                          # b2 gradient\n",
        "\n",
        "        dh = self.W2.flatten() * dz2       # hidden gradient\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)  # tanh 미분\n",
        "        dW1 = np.outer(x, dz1)             # W1 gradient\n",
        "        db1 = dz1                          # b1 gradient\n",
        "\n",
        "        # 파라미터 업데이트 (gradient descent)\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "\n",
        "    def backward_input(self, dq, cache):\n",
        "        # 입력 x에 대한 gradient ∂L/∂x 계산\n",
        "        # Actor 업데이트 시 ∂Q/∂a 얻기 위해 사용\n",
        "        x, z1, h, z2, q = cache\n",
        "\n",
        "        dz2 = dq                           # dL/dz2\n",
        "        dh = self.W2.flatten() * dz2       # dL/dh\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)\n",
        "        dx = dz1 @ self.W1.T               # dL/dx\n",
        "        return dx\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. Actor(확률 정책, Gaussian) 정의\n",
        "# ==============================\n",
        "class ActorSAC:\n",
        "    # 상태 one-hot → (μ(s), logσ(s)) → z = μ + σ ε → a = clip(z, -1, 1)\n",
        "    def __init__(self, state_dim, hidden_dim):\n",
        "        in_dim = state_dim\n",
        "        self.W1 = np.random.randn(in_dim, hidden_dim) / np.sqrt(in_dim)\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "\n",
        "        # μ 헤드 파라미터\n",
        "        self.W2_mu = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2_mu = np.zeros(1)\n",
        "\n",
        "        # logσ 헤드 파라미터\n",
        "        self.W2_logstd = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
        "        self.b2_logstd = np.zeros(1)\n",
        "\n",
        "    def forward(self, s_vec):\n",
        "        # 상태 벡터 s_vec(one-hot) → (μ, logσ, σ, ε, z, a, logπ) 반환\n",
        "        z1 = s_vec @ self.W1 + self.b1                # 1층 선형 결합\n",
        "        h = np.tanh(z1)                               # tanh 활성화\n",
        "\n",
        "        z2_mu = h @ self.W2_mu + self.b2_mu           # μ 출력\n",
        "        mu = z2_mu[0]\n",
        "\n",
        "        z2_logstd = h @ self.W2_logstd + self.b2_logstd  # logσ 출력\n",
        "        log_std = z2_logstd[0]\n",
        "\n",
        "        # log_std를 적당한 범위로 클리핑 (너무 큰 분산 방지)\n",
        "        log_std = np.clip(log_std, -2.0, 1.0)\n",
        "        std = np.exp(log_std)                         # σ = exp(logσ)\n",
        "\n",
        "        # ε ~ N(0,1) 샘플링 (reparameterization trick)\n",
        "        eps = np.random.randn()\n",
        "        z = mu + std * eps                            # 샘플 z\n",
        "        a = np.clip(z, -1.0, 1.0)                     # [-1,1] 범위로 클리핑\n",
        "\n",
        "        # Gaussian 로그 확률 밀도 log π(z|μ,σ)\n",
        "        # 상수항은 무시해도 gradient에는 영향 없음\n",
        "        var = std ** 2\n",
        "        log_pi = -0.5 * ((z - mu) ** 2 / var + 2.0 * log_std)\n",
        "\n",
        "        cache = (s_vec, z1, h, z2_mu, z2_logstd, mu, log_std, std, eps, z, a, log_pi)\n",
        "        return mu, log_std, std, eps, z, a, log_pi, cache\n",
        "\n",
        "    def backward(self, dmu, dlogstd, cache, lr):\n",
        "        # Actor 파라미터에 대한 역전파\n",
        "        # 입력: dmu = dL/dμ, dlogstd = dL/d(logσ)\n",
        "        s_vec, z1, h, z2_mu, z2_logstd, mu, log_std, std, eps, z, a, log_pi = cache\n",
        "\n",
        "        # μ 헤드 역전파 (μ = z2_mu)\n",
        "        dz2_mu = dmu                                  # dL/dz2_mu\n",
        "        dW2_mu = np.outer(h, dz2_mu)                  # W2_mu gradient\n",
        "        db2_mu = dz2_mu                               # b2_mu gradient\n",
        "        dh_mu = self.W2_mu.flatten() * dz2_mu         # hidden으로의 gradient\n",
        "\n",
        "        # logσ 헤드 역전파 (logσ = z2_logstd)\n",
        "        dz2_logstd = dlogstd                          # dL/dz2_logstd\n",
        "        dW2_logstd = np.outer(h, dz2_logstd)          # W2_logstd gradient\n",
        "        db2_logstd = dz2_logstd                       # b2_logstd gradient\n",
        "        dh_logstd = self.W2_logstd.flatten() * dz2_logstd\n",
        "\n",
        "        # 두 헤드에서 온 gradient를 합산\n",
        "        dh = dh_mu + dh_logstd                        # dL/dh\n",
        "\n",
        "        # 1층으로 역전파 (h = tanh(z1))\n",
        "        dz1 = dh * (1.0 - np.tanh(z1) ** 2)           # dL/dz1\n",
        "        dW1 = np.outer(s_vec, dz1)                    # W1 gradient\n",
        "        db1 = dz1                                     # b1 gradient\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        self.W2_mu      -= lr * dW2_mu\n",
        "        self.b2_mu      -= lr * db2_mu\n",
        "        self.W2_logstd  -= lr * dW2_logstd\n",
        "        self.b2_logstd  -= lr * db2_logstd\n",
        "        self.W1         -= lr * dW1\n",
        "        self.b1         -= lr * db1\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 타깃 네트워크 soft update\n",
        "# ==============================\n",
        "def soft_update(target, source, tau):\n",
        "    # θ_target ← τ θ_source + (1 - τ) θ_target\n",
        "    for attr in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "        target_param = getattr(target, attr)\n",
        "        source_param = getattr(source, attr)\n",
        "        new_param = tau * source_param + (1.0 - tau) * target_param\n",
        "        setattr(target, attr, new_param.copy())\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 리플레이 버퍼\n",
        "# ==============================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, s, a, r, ns, done):\n",
        "        # (s, a, r, ns, done) transition 저장\n",
        "        data = (s, a, r, ns, done)\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(data)\n",
        "        else:\n",
        "            self.buffer[self.position] = data\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # 랜덤 미니배치 추출\n",
        "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        batch = [self.buffer[i] for i in idxs]\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. SAC 네트워크 초기화\n",
        "# ==============================\n",
        "state_dim = n_states\n",
        "\n",
        "# Actor (확률 정책)\n",
        "actor = ActorSAC(state_dim, hidden_dim=16)\n",
        "\n",
        "# Twin Q-Critic 및 타깃 Q-Critic\n",
        "critic1 = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "critic2 = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "critic1_target = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "critic2_target = MLP(in_dim=state_dim + n_actions, hidden_dim=32)\n",
        "\n",
        "# 타깃 네트워크를 초기에는 원본과 동일하게 설정\n",
        "soft_update(critic1_target, critic1, tau=1.0)\n",
        "soft_update(critic2_target, critic2, tau=1.0)\n",
        "\n",
        "buffer = ReplayBuffer(buffer_capacity)    # 리플레이 버퍼 생성\n",
        "\n",
        "return_history = []                       # 에피소드별 Return 기록\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 SAC(Soft Actor-Critic, NumPy) 학습 시작 ===\")\n",
        "\n",
        "# ==============================\n",
        "# 8. 학습 루프\n",
        "# ==============================\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()           # 초기 상태\n",
        "    G0 = 0.0                  # 에피소드 Return(G_0)\n",
        "    discount = 1.0            # γ^t 계산용\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) 상태를 one-hot 벡터로 변환\n",
        "        s_vec = np.zeros(state_dim)\n",
        "        s_vec[state] = 1.0\n",
        "\n",
        "        # 2) 현재 정책에서 행동 샘플링 (탐험을 위한 stochastic 정책)\n",
        "        mu, log_std, std, eps, z, a, log_pi, cache_actor_step = actor.forward(s_vec)\n",
        "\n",
        "        # 3) 연속 행동 a의 부호를 기준으로 이산 행동 env_action 선택\n",
        "        env_action = 0 if a < 0.0 else 1\n",
        "\n",
        "        # 4) 환경 한 스텝 진행\n",
        "        next_state, reward, done = step(state, env_action)\n",
        "\n",
        "        # 5) 리플레이 버퍼에 transition 저장\n",
        "        #    여기서는 연속 행동 a(클리핑된 값)를 저장\n",
        "        buffer.push(state, a, reward, next_state, done)\n",
        "\n",
        "        # 6) 버퍼에 충분히 쌓이면 SAC 업데이트 수행\n",
        "        if len(buffer) >= max(start_learning, batch_size):\n",
        "            # 미니배치 샘플링\n",
        "            batch = buffer.sample(batch_size)\n",
        "\n",
        "            # ----------------------\n",
        "            # 6-1. Critic(Q1, Q2) 업데이트\n",
        "            # ----------------------\n",
        "            for (s_b, a_b, r_b, ns_b, d_b) in batch:\n",
        "                # 상태 및 다음 상태를 one-hot 벡터로 변환\n",
        "                s_vec_b = np.zeros(state_dim)\n",
        "                s_vec_b[s_b] = 1.0\n",
        "                ns_vec_b = np.zeros(state_dim)\n",
        "                ns_vec_b[ns_b] = 1.0\n",
        "\n",
        "                # 다음 상태에서의 정책 샘플 a' 및 log π(a'|s')\n",
        "                mu_next, log_std_next, std_next, eps_next, z_next, a_next, log_pi_next, cache_actor_next = actor.forward(ns_vec_b)\n",
        "\n",
        "                # 타깃 Q1', Q2' 계산\n",
        "                x_next = np.concatenate([ns_vec_b, np.array([a_next])])\n",
        "                q1_next, _ = critic1_target.forward(x_next)\n",
        "                q2_next, _ = critic2_target.forward(x_next)\n",
        "                q_next_min = min(q1_next, q2_next)\n",
        "\n",
        "                # SAC 타깃: y = r + γ(1-done) * (q_next_min - α logπ_next)\n",
        "                target = r_b + (0.0 if d_b else gamma * (q_next_min - alpha * log_pi_next))\n",
        "\n",
        "                # 현재 Q1, Q2 계산\n",
        "                x_curr = np.concatenate([s_vec_b, np.array([a_b])])\n",
        "                q1_curr, cache_c1 = critic1.forward(x_curr)\n",
        "                q2_curr, cache_c2 = critic2.forward(x_curr)\n",
        "\n",
        "                # MSE 손실의 gradient: dL/dQi = (Qi - target)\n",
        "                dq1 = (q1_curr - target)\n",
        "                dq2 = (q2_curr - target)\n",
        "\n",
        "                # Critic1, Critic2 업데이트\n",
        "                critic1.backward(dq1, cache_c1, critic_lr)\n",
        "                critic2.backward(dq2, cache_c2, critic_lr)\n",
        "\n",
        "                # 타깃 Q 네트워크 soft update\n",
        "                soft_update(critic1_target, critic1, tau)\n",
        "                soft_update(critic2_target, critic2, tau)\n",
        "\n",
        "            # ----------------------\n",
        "            # 6-2. Actor(정책) 업데이트\n",
        "            # ----------------------\n",
        "            for (s_b, a_b, r_b, ns_b, d_b) in batch:\n",
        "                # 상태를 one-hot 벡터로 변환\n",
        "                s_vec_b = np.zeros(state_dim)\n",
        "                s_vec_b[s_b] = 1.0\n",
        "\n",
        "                # 현재 정책에서 행동 샘플 a_policy 및 log π(a_policy|s)\n",
        "                mu_b, log_std_b, std_b, eps_b, z_b, a_b_samp, log_pi_b, cache_actor_b = actor.forward(s_vec_b)\n",
        "\n",
        "                # Q1, Q2에서 Qmin(s, a_policy) 계산\n",
        "                x_actor = np.concatenate([s_vec_b, np.array([a_b_samp])])\n",
        "                q1_val, cache_q1_for_actor = critic1.forward(x_actor)\n",
        "                q2_val, cache_q2_for_actor = critic2.forward(x_actor)\n",
        "\n",
        "                if q1_val <= q2_val:\n",
        "                    q_min = q1_val\n",
        "                    cache_q_min = cache_q1_for_actor\n",
        "                else:\n",
        "                    q_min = q2_val\n",
        "                    cache_q_min = cache_q2_for_actor\n",
        "\n",
        "                # Actor 손실: L_actor = α logπ - Qmin\n",
        "                # Qmin 항에 대한 gradient: dL/dQmin = -1\n",
        "                dx = critic1.backward_input(-1.0, cache_q_min) if q1_val <= q2_val else critic2.backward_input(-1.0, cache_q_min)\n",
        "                da_from_Q = dx[-1]   # 연속 행동 a에 대한 gradient (Q 항에서 기여)\n",
        "\n",
        "                # logπ(a|s)에 대한 analytic gradient (Gaussian)\n",
        "                # logπ = -0.5 * ((z-μ)^2 / σ^2 + 2logσ + const)\n",
        "                # ∂logπ/∂μ = (z - μ) / σ^2\n",
        "                # ∂logπ/∂logσ = -1 + ((z - μ)/σ)^2\n",
        "                diff = z_b - mu_b\n",
        "                grad_logpi_mu = diff / (std_b ** 2)\n",
        "                grad_logpi_logstd = -1.0 + (diff / std_b) ** 2\n",
        "\n",
        "                # z = μ + σ ε, 여기서 ε는 eps_b\n",
        "                # ∂z/∂μ = 1, ∂z/∂logσ = σ * ε\n",
        "                dz_dmu = 1.0\n",
        "                dz_dlogstd = std_b * eps_b\n",
        "\n",
        "                # Q 항에서 오는 μ, logσ에 대한 gradient\n",
        "                dmu_Q = da_from_Q * dz_dmu\n",
        "                dlogstd_Q = da_from_Q * dz_dlogstd\n",
        "\n",
        "                # 엔트로피(α logπ) 항에서 오는 gradient\n",
        "                dmu_ent = alpha * grad_logpi_mu\n",
        "                dlogstd_ent = alpha * grad_logpi_logstd\n",
        "\n",
        "                # 최종 dL/dμ, dL/dlogσ\n",
        "                dmu_total = dmu_Q + dmu_ent\n",
        "                dlogstd_total = dlogstd_Q + dlogstd_ent\n",
        "\n",
        "                # Actor 파라미터 업데이트\n",
        "                actor.backward(dmu_total, dlogstd_total, cache_actor_b, actor_lr)\n",
        "\n",
        "        # 7) 에피소드 Return(G_0) 누적 (할인 보상)\n",
        "        G0 += discount * reward\n",
        "        discount *= gamma\n",
        "\n",
        "        # 8) 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # 목표 상태 도달 시 에피소드 종료\n",
        "            break\n",
        "\n",
        "    # 에피소드 종료 후 Return 기록\n",
        "    return_history.append(G0)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 Return 평균 출력\n",
        "    if episode % 50 == 0:\n",
        "        avg_ret = np.mean(return_history[-50:])\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_ret:.3f}\")\n",
        "\n",
        "print(\"\\n=== SAC 학습 종료 ===\\n\")\n",
        "\n",
        "# ==============================\n",
        "# 9. 학습된 정책(평균 μ 기준) 출력\n",
        "# ==============================\n",
        "print(\"▶ 학습된 Actor의 평균 정책 μ(s) (상태별 연속 행동 평균 값)\")\n",
        "for s in range(n_states):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[s] = 1.0\n",
        "    mu_s, log_std_s, std_s, eps_s, z_s, a_s, log_pi_s, cache_s = actor.forward(s_vec)\n",
        "    print(f\"상태 {s}: μ = {mu_s:.4f}\")\n",
        "\n",
        "# Greedy 기준 이산 정책 (μ의 부호를 기준으로 ←/→)\n",
        "print(\"\\n▶ Greedy 기준 이산 정책(μ(s)의 부호 기준 ←/→)\")\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[s] = 1.0\n",
        "    mu_s, log_std_s, std_s, eps_s, z_s, a_s, log_pi_s, cache_s = actor.forward(s_vec)\n",
        "    env_action = 0 if mu_s < 0.0 else 1\n",
        "    if s == n_states - 1:\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        policy_str += f\" {action_symbols[env_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "# ==============================\n",
        "# 10. 학습된 정책으로 1회 테스트 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 평균 정책(μ)을 이용한 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    s_vec = np.zeros(state_dim)\n",
        "    s_vec[state] = 1.0\n",
        "\n",
        "    mu_s, log_std_s, std_s, eps_s, z_s, a_s, log_pi_s, cache_s = actor.forward(s_vec)\n",
        "    env_action = 0 if mu_s < 0.0 else 1\n",
        "\n",
        "    next_state, reward, done = step(state, env_action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4a6b6685-b629-434e-814d-e45e5c369947",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6b6685-b629-434e-814d-e45e5c369947",
        "outputId": "229c7779-5b2f-4210-aa56-89587c6cccbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1차원 선형 월드에서의 Behavioral Cloning(BC, NumPy) 학습 시작 ===\n",
            "[Epoch  200] Loss = 0.0209, Training Accuracy = 1.000\n",
            "[Epoch  400] Loss = 0.0103, Training Accuracy = 1.000\n",
            "[Epoch  600] Loss = 0.0068, Training Accuracy = 1.000\n",
            "[Epoch  800] Loss = 0.0051, Training Accuracy = 1.000\n",
            "[Epoch 1000] Loss = 0.0041, Training Accuracy = 1.000\n",
            "[Epoch 1200] Loss = 0.0034, Training Accuracy = 1.000\n",
            "[Epoch 1400] Loss = 0.0029, Training Accuracy = 1.000\n",
            "[Epoch 1600] Loss = 0.0025, Training Accuracy = 1.000\n",
            "[Epoch 1800] Loss = 0.0022, Training Accuracy = 1.000\n",
            "[Epoch 2000] Loss = 0.0020, Training Accuracy = 1.000\n",
            "\n",
            "=== Behavioral Cloning 학습 종료 ===\n",
            "\n",
            "▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→] 확률)\n",
            "상태 0: [0.0020, 0.9980]\n",
            "상태 1: [0.0020, 0.9980]\n",
            "상태 2: [0.0020, 0.9980]\n",
            "상태 3: [0.0020, 0.9980]\n",
            "상태 4: [0.0069, 0.9931]\n",
            "\n",
            "▶ Greedy 기준 학습된 정책(Policy)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 정책(BC, Greedy)으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-13) BCBC(Behavioral Cloning) : 데이터를 기반으로 정책을 모방하는 방식\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산용 NumPy\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태: 0,1,2,3,4 (4가 목표 상태)\n",
        "n_actions = 2       # 이산 행동: 0=왼쪽, 1=오른쪽\n",
        "\n",
        "def step(state, action):\n",
        "    # 주어진 상태 state에서 action(0=왼쪽, 1=오른쪽)을 수행\n",
        "    # 다음 상태(next_state), 보상(reward), 종료 여부(done)를 반환\n",
        "\n",
        "    # 행동에 따른 상태 이동\n",
        "    if action == 0:   # 왼쪽\n",
        "        next_state = max(0, state - 1)\n",
        "    else:             # 오른쪽\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    # 목표 상태(4)에 도달하면 보상 1.0, 종료\n",
        "    if next_state == n_states - 1:\n",
        "        reward = 1.0\n",
        "        done = True\n",
        "    else:\n",
        "        reward = -0.01   # 그 외에는 시간 패널티\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 상태 초기화\n",
        "    return 0            # 항상 0에서 시작\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. 난수 시드 및 Behavioral Cloning 설정\n",
        "# ==============================\n",
        "np.random.seed(42)          # 시드 고정 (결과 재현성 확보)\n",
        "\n",
        "n_demo_episodes = 50        # 전문가 데모 에피소드 수\n",
        "max_steps       = 20        # 에피소드당 최대 스텝 수\n",
        "\n",
        "lr = 0.1                    # BC(Behavioral Cloning) 학습률\n",
        "n_epochs = 2000             # 학습 반복(epoch) 수\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 전문가 정책 정의 (Expert Policy)\n",
        "# ==============================\n",
        "def expert_policy(state):\n",
        "    # 전문가 정책: 목표 상태(4)에 도달할 때까지 항상 오른쪽(1)으로 이동\n",
        "    if state < n_states - 1:\n",
        "        return 1           # 오른쪽\n",
        "    else:\n",
        "        return 0           # 목표 상태에서는 의미 없음 (실제로는 사용 안 됨)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. 전문가 Demonstration 데이터 수집\n",
        "# ==============================\n",
        "demo_states = []   # 전문가가 방문한 상태들\n",
        "demo_actions = []  # 전문가가 선택한 행동들\n",
        "\n",
        "for ep in range(n_demo_episodes):\n",
        "    # 에피소드마다 초기화\n",
        "    state = reset()\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 전문가 정책으로 행동 선택\n",
        "        action = expert_policy(state)\n",
        "\n",
        "        # 상태, 행동을 데모 데이터에 기록\n",
        "        demo_states.append(state)\n",
        "        demo_actions.append(action)\n",
        "\n",
        "        # 환경 한 스텝 진행\n",
        "        next_state, reward, done = step(state, action)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # 목표 상태에 도달하면 에피소드 종료\n",
        "            break\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "demo_states = np.array(demo_states)   # (N,)\n",
        "demo_actions = np.array(demo_actions) # (N,)\n",
        "\n",
        "# 데모 데이터 크기 확인\n",
        "N = demo_states.shape[0]\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. 입력(X)과 레이블(y) 구성\n",
        "# ==============================\n",
        "# 입력 X: 상태를 one-hot 벡터로 표현 (형태: (N, n_states))\n",
        "X = np.zeros((N, n_states))\n",
        "for i, s in enumerate(demo_states):\n",
        "    X[i, s] = 1.0\n",
        "\n",
        "# 레이블 y: 행동(0 또는 1)\n",
        "y = demo_actions.copy()   # (N,)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 정책 모델 정의 (Softmax Regression)\n",
        "# ==============================\n",
        "# π(a|s; θ)를 softmax(W^T s + b)로 표현\n",
        "# W: (n_states, n_actions), b: (n_actions,)\n",
        "W = np.random.randn(n_states, n_actions) * 0.01   # 작은 랜덤 값으로 초기화\n",
        "b = np.zeros(n_actions)                           # 편향은 0으로 초기화\n",
        "\n",
        "def softmax(logits):\n",
        "    # 입력 logits: (N, n_actions)\n",
        "    # 출력: 각 행에 대해 softmax 적용 (확률 분포)\n",
        "    max_logits = np.max(logits, axis=1, keepdims=True)   # overflow 방지용\n",
        "    exp_logits = np.exp(logits - max_logits)\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "    return probs\n",
        "\n",
        "def forward(X):\n",
        "    # 순전파: X → logits → softmax 확률\n",
        "    logits = X @ W + b      # (N, n_actions)\n",
        "    probs = softmax(logits) # (N, n_actions)\n",
        "    return logits, probs\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. Behavioral Cloning 학습 루프 (지도학습)\n",
        "# ==============================\n",
        "print(\"=== 1차원 선형 월드에서의 Behavioral Cloning(BC, NumPy) 학습 시작 ===\")\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # 1) 순전파: 현재 정책으로 확률 계산\n",
        "    logits, probs = forward(X)   # probs: (N, n_actions)\n",
        "\n",
        "    # 2) 교차 엔트로피 손실 계산\n",
        "    #    L = - (1/N) * sum_i log π(a_i | s_i)\n",
        "    #    정답 인덱스에 해당하는 확률을 모아서 log 취함\n",
        "    correct_log_probs = -np.log(probs[np.arange(N), y] + 1e-12)\n",
        "    loss = np.mean(correct_log_probs)\n",
        "\n",
        "    # 3) 역전파: gradient 계산\n",
        "    #    softmax + cross-entropy의 gradient:\n",
        "    #    dL/dlogits = (probs - y_onehot) / N\n",
        "    grad_logits = probs.copy()\n",
        "    grad_logits[np.arange(N), y] -= 1.0\n",
        "    grad_logits /= N\n",
        "\n",
        "    # 4) 파라미터 W, b에 대한 gradient 계산\n",
        "    #    dL/dW = X^T @ grad_logits\n",
        "    #    dL/db = row-wise sum(grad_logits)\n",
        "    dW = X.T @ grad_logits              # (n_states, n_actions)\n",
        "    db = np.sum(grad_logits, axis=0)    # (n_actions,)\n",
        "\n",
        "    # 5) 파라미터 업데이트 (Gradient Descent)\n",
        "    W -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "    # 6) 학습 과정 모니터링 (간단히 200 epoch마다 출력)\n",
        "    if epoch % 200 == 0:\n",
        "        # 현재 정책으로 전문가 데이터에서의 정확도 계산\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        acc = np.mean(preds == y)\n",
        "        print(f\"[Epoch {epoch:4d}] Loss = {loss:.4f}, Training Accuracy = {acc:.3f}\")\n",
        "\n",
        "print(\"\\n=== Behavioral Cloning 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 8. 학습된 정책 π(a|s; θ) 확인\n",
        "# ==============================\n",
        "print(\"▶ 학습된 정책 π(a|s; θ) (행: 상태, 열: 행동[←,→] 확률)\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    # 상태 s를 one-hot 벡터로 변환\n",
        "    s_vec = np.zeros((1, n_states))\n",
        "    s_vec[0, s] = 1.0\n",
        "\n",
        "    # 정책 확률 계산\n",
        "    _, probs_s = forward(s_vec)   # (1, n_actions)\n",
        "    p_left  = probs_s[0, 0]\n",
        "    p_right = probs_s[0, 1]\n",
        "\n",
        "    print(f\"상태 {s}: [{p_left:.4f}, {p_right:.4f}]\")\n",
        "\n",
        "# Greedy 정책(가장 확률이 높은 행동) 출력\n",
        "print(\"\\n▶ Greedy 기준 학습된 정책(Policy)\")\n",
        "action_symbols = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        # 목표 상태는 G 로 표시\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        s_vec = np.zeros((1, n_states))\n",
        "        s_vec[0, s] = 1.0\n",
        "        _, probs_s = forward(s_vec)\n",
        "        a_greedy = int(np.argmax(probs_s[0]))   # 확률 최대 행동 선택\n",
        "        policy_str += f\" {action_symbols[a_greedy]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 9. 학습된 정책으로 1회 에피소드 실행 (Greedy)\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 정책(BC, Greedy)으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    # 상태를 one-hot으로 변환\n",
        "    s_vec = np.zeros((1, n_states))\n",
        "    s_vec[0, state] = 1.0\n",
        "\n",
        "    # 정책 확률 계산 후 Greedy 행동 선택\n",
        "    _, probs_s = forward(s_vec)\n",
        "    a_greedy = int(np.argmax(probs_s[0]))\n",
        "\n",
        "    # 환경에 적용\n",
        "    next_state, reward, done = step(state, a_greedy)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c82f2e0c-996f-45f5-ad5f-b5a6afc34212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c82f2e0c-996f-45f5-ad5f-b5a6afc34212",
        "outputId": "d7421ed8-35b8-48fd-956f-12a8fb80e936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 전문가 Demonstration 수집 및 리플레이 버퍼 프리필 시작 ===\n",
            "전문가 Demonstration으로 프리필된 transition 수: 120\n",
            "=== Demonstration 프리필 종료 ===\n",
            "\n",
            "=== 1차원 선형 월드에서의 DDPGfD(DDPG from Demonstrations, NumPy) 학습 시작 ===\n",
            "[Episode   50] 최근 50 에피소드 평균 Return = 0.907, noise_std = 0.233\n",
            "[Episode  100] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.182\n",
            "[Episode  150] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.141\n",
            "[Episode  200] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.110\n",
            "[Episode  250] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.086\n",
            "[Episode  300] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.067\n",
            "[Episode  350] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.052\n",
            "[Episode  400] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.050\n",
            "[Episode  450] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.050\n",
            "[Episode  500] 최근 50 에피소드 평균 Return = 0.970, noise_std = 0.050\n",
            "\n",
            "=== DDPGfD 학습 종료 ===\n",
            "\n",
            "▶ 학습된 Actor의 결정론적 정책 μ(s) (상태별 연속 행동 값)\n",
            "상태 0: 행동 a = 0.8633\n",
            "상태 1: 행동 a = 0.8625\n",
            "상태 2: 행동 a = 0.8588\n",
            "상태 3: 행동 a = 0.8511\n",
            "상태 4: 행동 a = 0.7715\n",
            "\n",
            "▶ Greedy 기준 이산 정책(연속 a의 부호 기준 ←/→)\n",
            "상태 0  1  2  3  4\n",
            "      →  →  →  →  G \n",
            "\n",
            "▶ 학습된 결정론적 정책(DDPGfD)으로 1회 에피소드 실행 예시\n",
            "방문한 상태들: [0, 1, 2, 3, 4]\n",
            "스텝 수: 4\n",
            "마지막 상태가 목표(4)면 학습 성공!\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "## (2-14) DDPGfD(DDPG from Demonstrations) : 전문가의 시범을 사용해 DDPG 성능을 개선\n",
        "###################################################################\n",
        "import numpy as np  # 수치 계산을 위한 NumPy\n",
        "\n",
        "# ==============================\n",
        "# 1. 환경 정의 (1차원 선형 월드)\n",
        "# ==============================\n",
        "n_states = 5        # 상태: 0, 1, 2, 3, 4 (4가 목표 상태)\n",
        "# DDPG 내부에서는 연속 행동 a ∈ [-1, 1] 을 쓰고,\n",
        "# 실제 환경에는 a>0 이면 오른쪽(1), a<=0 이면 왼쪽(0) 으로 이산 행동으로 변환한다.\n",
        "\n",
        "def step(state, env_action):\n",
        "    # 이산 행동 env_action(0=왼쪽, 1=오른쪽)을 받아 다음 상태, 보상, 종료 여부를 반환\n",
        "    if env_action == 0:\n",
        "        next_state = max(0, state - 1)\n",
        "    else:\n",
        "        next_state = min(n_states - 1, state + 1)\n",
        "\n",
        "    if next_state == n_states - 1:\n",
        "        # 목표 상태(4)에 도달하면 보상 1.0, 종료\n",
        "        reward = 1.0\n",
        "        done = True\n",
        "    else:\n",
        "        # 그 외에는 시간 패널티 -0.01\n",
        "        reward = -0.01\n",
        "        done = False\n",
        "\n",
        "    return next_state, reward, done\n",
        "\n",
        "def reset():\n",
        "    # 에피소드 시작 시 항상 상태 0에서 시작\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. 난수 시드 및 하이퍼파라미터\n",
        "# ==============================\n",
        "np.random.seed(42)          # 결과 재현을 위한 시드 고정\n",
        "\n",
        "gamma = 0.99                # 할인율\n",
        "tau   = 0.01                # 타깃 네트워크 소프트 업데이트 계수\n",
        "\n",
        "actor_lr  = 0.01            # Actor 학습률\n",
        "critic_lr = 0.05            # Critic 학습률\n",
        "\n",
        "n_episodes = 500            # 전체 학습 에피소드 수\n",
        "max_steps  = 20             # 한 에피소드 최대 스텝 수\n",
        "\n",
        "buffer_capacity = 10000     # 리플레이 버퍼 최대 크기\n",
        "batch_size      = 32        # 미니배치 크기\n",
        "\n",
        "# 행동 탐험용 가우시안 노이즈 (연속 행동 a에 추가)\n",
        "noise_std_init = 0.3        # 초기 노이즈 표준편차\n",
        "noise_std_min  = 0.05       # 최소 노이즈 표준편차\n",
        "noise_decay    = 0.995      # 에피소드마다 노이즈 감소 비율\n",
        "\n",
        "# DDPGfD 용 전문가 데모 수\n",
        "n_demo_episodes = 30        # 전문가 데모 에피소드 수 (버퍼 프리필용)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. 전문가 정책 정의 (Expert Policy)\n",
        "# ==============================\n",
        "def expert_policy(state):\n",
        "    # 전문가 정책: 목표에 도달할 때까지 항상 오른쪽으로 이동\n",
        "    # 연속 행동 a ∈ [-1, 1] 을 출력한다고 가정하면, +1.0 을 사용\n",
        "    return 1.0   # 항상 오른쪽\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. 상태를 one-hot 벡터로 변환하는 함수\n",
        "# ==============================\n",
        "def state_to_one_hot(state_idx):\n",
        "    # 상태 인덱스를 길이 n_states인 one-hot 벡터로 변환\n",
        "    vec = np.zeros(n_states)\n",
        "    vec[state_idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. Actor / Critic 네트워크 (선형 모델, NumPy)\n",
        "# ==============================\n",
        "# Actor: μ(s) = tanh( s_onehot^T * Wa + ba )\n",
        "#  - Wa: (n_states,)  -> 상태별 weight\n",
        "#  - ba: 스칼라        -> bias\n",
        "# Critic: Q(s,a) = s_onehot^T * Wc_s + a * Wc_a + bc\n",
        "#  - Wc_s: (n_states,) -> 상태 weight\n",
        "#  - Wc_a: 스칼라      -> 행동 weight\n",
        "#  - bc:   스칼라      -> bias\n",
        "\n",
        "# Actor 파라미터 (메인)\n",
        "Wa = np.random.randn(n_states) * 0.01\n",
        "ba = 0.0\n",
        "\n",
        "# Critic 파라미터 (메인)\n",
        "Wc_s = np.random.randn(n_states) * 0.01\n",
        "Wc_a = 0.0\n",
        "bc   = 0.0\n",
        "\n",
        "# 타깃 네트워크 파라미터 (Actor / Critic)\n",
        "Wa_tgt = Wa.copy()\n",
        "ba_tgt = ba\n",
        "Wc_s_tgt = Wc_s.copy()\n",
        "Wc_a_tgt = Wc_a\n",
        "bc_tgt   = bc\n",
        "\n",
        "def actor_forward(state_idx, use_target=False):\n",
        "    # 상태 인덱스를 받아 Actor 네트워크를 통해 연속 행동 μ(s)를 계산\n",
        "    s = state_to_one_hot(state_idx)  # one-hot 상태 벡터\n",
        "    if use_target:\n",
        "        # 타깃 Actor 사용\n",
        "        u = np.dot(s, Wa_tgt) + ba_tgt\n",
        "    else:\n",
        "        # 메인 Actor 사용\n",
        "        u = np.dot(s, Wa) + ba\n",
        "    # 출력에 tanh를 적용해 [-1, 1] 범위로 제한\n",
        "    a = np.tanh(u)\n",
        "    return a\n",
        "\n",
        "def critic_forward(state_idx, action, use_target=False):\n",
        "    # 상태 인덱스와 연속 행동 a를 받아 Q(s,a)를 계산\n",
        "    s = state_to_one_hot(state_idx)\n",
        "    if use_target:\n",
        "        q = np.dot(s, Wc_s_tgt) + Wc_a_tgt * action + bc_tgt\n",
        "    else:\n",
        "        q = np.dot(s, Wc_s) + Wc_a * action + bc\n",
        "    return q\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. 리플레이 버퍼 구현 (Demonstration 포함)\n",
        "# ==============================\n",
        "# 버퍼에는 (s, a, r, s_next, done, is_demo) 를 저장\n",
        "replay_s      = []\n",
        "replay_a      = []\n",
        "replay_r      = []\n",
        "replay_s_next = []\n",
        "replay_done   = []\n",
        "replay_is_demo = []   # 데모 여부 플래그 (True/False)\n",
        "\n",
        "def add_transition(s, a, r, s_next, done, is_demo):\n",
        "    # 새 transition을 리플레이 버퍼에 추가\n",
        "    if len(replay_s) >= buffer_capacity:\n",
        "        # 버퍼가 꽉 찬 경우 FIFO 방식으로 가장 오래된 transition 삭제\n",
        "        replay_s.pop(0)\n",
        "        replay_a.pop(0)\n",
        "        replay_r.pop(0)\n",
        "        replay_s_next.pop(0)\n",
        "        replay_done.pop(0)\n",
        "        replay_is_demo.pop(0)\n",
        "\n",
        "    replay_s.append(s)\n",
        "    replay_a.append(a)\n",
        "    replay_r.append(r)\n",
        "    replay_s_next.append(s_next)\n",
        "    replay_done.append(done)\n",
        "    replay_is_demo.append(is_demo)\n",
        "\n",
        "def sample_minibatch(batch_size):\n",
        "    # 리플레이 버퍼에서 무작위로 미니배치 샘플링\n",
        "    size = len(replay_s)\n",
        "    indices = np.random.choice(size, size=batch_size, replace=False)\n",
        "\n",
        "    batch = {\n",
        "        \"s\":      np.array([replay_s[i] for i in indices], dtype=np.int64),\n",
        "        \"a\":      np.array([replay_a[i] for i in indices], dtype=np.float32),\n",
        "        \"r\":      np.array([replay_r[i] for i in indices], dtype=np.float32),\n",
        "        \"s_next\": np.array([replay_s_next[i] for i in indices], dtype=np.int64),\n",
        "        \"done\":   np.array([replay_done[i] for i in indices], dtype=np.bool_),\n",
        "        \"is_demo\":np.array([replay_is_demo[i] for i in indices], dtype=np.bool_),\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 7. DDPGfD용 전문가 Demonstration 프리필\n",
        "# ==============================\n",
        "print(\"=== 전문가 Demonstration 수집 및 리플레이 버퍼 프리필 시작 ===\")\n",
        "\n",
        "for ep in range(n_demo_episodes):\n",
        "    state = reset()\n",
        "    for step_idx in range(max_steps):\n",
        "        # 전문가 정책으로 연속 행동 생성\n",
        "        a_cont = expert_policy(state)  # 여기서는 항상 +1.0\n",
        "\n",
        "        # 환경에는 이산 행동으로 전달 (a>0 → 오른쪽=1)\n",
        "        env_action = 1 if a_cont > 0.0 else 0\n",
        "        next_state, reward, done = step(state, env_action)\n",
        "\n",
        "        # 데모 transition을 버퍼에 추가 (is_demo=True)\n",
        "        add_transition(state, a_cont, reward, next_state, done, is_demo=True)\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "print(f\"전문가 Demonstration으로 프리필된 transition 수: {len(replay_s)}\")\n",
        "print(\"=== Demonstration 프리필 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 8. DDPGfD 학습 루프\n",
        "# ==============================\n",
        "noise_std = noise_std_init           # 탐험용 노이즈 표준편차\n",
        "returns_history = []                 # 에피소드별 Return 기록\n",
        "\n",
        "print(\"=== 1차원 선형 월드에서의 DDPGfD(DDPG from Demonstrations, NumPy) 학습 시작 ===\")\n",
        "\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    state = reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    for step_idx in range(max_steps):\n",
        "        # 1) 현재 상태에서 Actor의 결정론적 행동 μ(s) 계산\n",
        "        mu = actor_forward(state, use_target=False)\n",
        "\n",
        "        # 2) 탐험을 위해 가우시안 노이즈 추가\n",
        "        noise = np.random.randn() * noise_std\n",
        "        a_cont = mu + noise\n",
        "\n",
        "        # 3) 연속 행동을 [-1, 1]로 클리핑\n",
        "        a_cont = np.clip(a_cont, -1.0, 1.0)\n",
        "\n",
        "        # 4) 환경에 전달할 이산 행동으로 변환\n",
        "        env_action = 1 if a_cont > 0.0 else 0\n",
        "\n",
        "        # 5) 환경에서 한 스텝 진행\n",
        "        next_state, reward, done = step(state, env_action)\n",
        "\n",
        "        # 6) transition을 리플레이 버퍼에 추가 (is_demo=False)\n",
        "        add_transition(state, a_cont, reward, next_state, done, is_demo=False)\n",
        "\n",
        "        # 7) 리플레이 버퍼 기반으로 파라미터 업데이트\n",
        "        if len(replay_s) >= batch_size:\n",
        "            # 미니배치 샘플링\n",
        "            batch = sample_minibatch(batch_size)\n",
        "\n",
        "            s_batch      = batch[\"s\"]\n",
        "            a_batch      = batch[\"a\"]\n",
        "            r_batch      = batch[\"r\"]\n",
        "            s_next_batch = batch[\"s_next\"]\n",
        "            done_batch   = batch[\"done\"]\n",
        "\n",
        "            # --------------------------\n",
        "            # 7-1) Critic 업데이트\n",
        "            # --------------------------\n",
        "            # 타깃 Actor로 next_action 계산\n",
        "            a_next_batch = np.array([actor_forward(s_next_batch[i], use_target=True)\n",
        "                                     for i in range(batch_size)], dtype=np.float32)\n",
        "\n",
        "            # 타깃 Critic으로 target Q 계산\n",
        "            q_next_batch = np.array([critic_forward(s_next_batch[i], a_next_batch[i], use_target=True)\n",
        "                                     for i in range(batch_size)], dtype=np.float32)\n",
        "\n",
        "            # TD target: y = r + γ (1-done) * Q_tgt(s', μ_tgt(s'))\n",
        "            td_target = r_batch + gamma * (1.0 - done_batch.astype(np.float32)) * q_next_batch\n",
        "\n",
        "            # 현재 Critic Q값\n",
        "            q_batch = np.array([critic_forward(s_batch[i], a_batch[i], use_target=False)\n",
        "                                for i in range(batch_size)], dtype=np.float32)\n",
        "\n",
        "            # Critic 손실: MSE = mean( (q - y)^2 )\n",
        "            td_error = q_batch - td_target\n",
        "            critic_loss = np.mean(td_error ** 2)\n",
        "\n",
        "            # Critic 파라미터에 대한 gradient 계산\n",
        "            # dL/dQ = 2 * (Q - y) / N\n",
        "            dL_dQ = 2.0 * td_error / batch_size  # shape: (batch_size,)\n",
        "\n",
        "            # 상태 one-hot 벡터 묶기\n",
        "            S_onehot = np.stack([state_to_one_hot(int(s_batch[i])) for i in range(batch_size)], axis=0)\n",
        "\n",
        "            # dL/dWc_s = sum_i dL/dQ_i * s_i\n",
        "            grad_Wc_s = S_onehot.T @ dL_dQ      # shape: (n_states,)\n",
        "\n",
        "            # dL/dWc_a = sum_i dL/dQ_i * a_i\n",
        "            grad_Wc_a = np.sum(dL_dQ * a_batch)\n",
        "\n",
        "            # dL/dbc = sum_i dL/dQ_i\n",
        "            grad_bc = np.sum(dL_dQ)\n",
        "\n",
        "            # 파라미터 업데이트 (Gradient Descent)\n",
        "            Wc_s -= critic_lr * grad_Wc_s\n",
        "            Wc_a -= critic_lr * grad_Wc_a\n",
        "            bc   -= critic_lr * grad_bc\n",
        "\n",
        "            # --------------------------\n",
        "            # 7-2) Actor 업데이트\n",
        "            # --------------------------\n",
        "            # Actor는 Q(s, μ(s))를 최대화하도록 업데이트\n",
        "            # 여기서는 정책 gradient를 analytic하게 계산\n",
        "            mu_batch = np.array([actor_forward(int(s_batch[i]), use_target=False)\n",
        "                                 for i in range(batch_size)], dtype=np.float32)\n",
        "\n",
        "            # Actor loss = - mean Q(s, μ(s))\n",
        "            # Q(s, μ(s)) = s^T Wc_s + Wc_a * μ(s) + bc\n",
        "            Q_mu_batch = np.sum(S_onehot * Wc_s, axis=1) + Wc_a * mu_batch + bc\n",
        "            actor_loss = -np.mean(Q_mu_batch)\n",
        "\n",
        "            # dL_actor/dμ = - Wc_a / N (스칼라)\n",
        "            dL_dmu = -Wc_a / batch_size\n",
        "\n",
        "            # μ(s) = tanh(u), u = s^T Wa + ba\n",
        "            # dμ/du = 1 - tanh(u)^2 = 1 - μ^2\n",
        "            dmu_du = (1.0 - mu_batch ** 2)  # shape: (batch_size,)\n",
        "\n",
        "            # dL/du = dL/dμ * dμ/du\n",
        "            dL_du = dL_dmu * dmu_du         # shape: (batch_size,)\n",
        "\n",
        "            # dL/dWa = sum_i dL/du_i * s_i\n",
        "            grad_Wa = S_onehot.T @ dL_du    # shape: (n_states,)\n",
        "\n",
        "            # dL/dba = sum_i dL/du_i\n",
        "            grad_ba = np.sum(dL_du)\n",
        "\n",
        "            # Actor 파라미터 업데이트\n",
        "            Wa -= actor_lr * grad_Wa\n",
        "            ba -= actor_lr * grad_ba\n",
        "\n",
        "            # --------------------------\n",
        "            # 7-3) 타깃 네트워크 소프트 업데이트\n",
        "            # --------------------------\n",
        "            Wa_tgt   = (1.0 - tau) * Wa_tgt   + tau * Wa\n",
        "            ba_tgt   = (1.0 - tau) * ba_tgt   + tau * ba\n",
        "            Wc_s_tgt = (1.0 - tau) * Wc_s_tgt + tau * Wc_s\n",
        "            Wc_a_tgt = (1.0 - tau) * Wc_a_tgt + tau * Wc_a\n",
        "            bc_tgt   = (1.0 - tau) * bc_tgt   + tau * bc\n",
        "\n",
        "        # 8) Return 누적 및 상태 갱신\n",
        "        episode_return += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 노이즈 표준편차 감소 (탐험 → 이용 전환)\n",
        "    noise_std = max(noise_std_min, noise_std * noise_decay)\n",
        "\n",
        "    # 에피소드 Return 기록\n",
        "    returns_history.append(episode_return)\n",
        "\n",
        "    # 50 에피소드마다 최근 50개 평균 Return 및 노이즈 출력\n",
        "    if episode % 50 == 0:\n",
        "        recent_returns = returns_history[-50:]\n",
        "        avg_return = np.mean(recent_returns)\n",
        "        print(f\"[Episode {episode:4d}] 최근 50 에피소드 평균 Return = {avg_return:.3f}, noise_std = {noise_std:.3f}\")\n",
        "\n",
        "print(\"\\n=== DDPGfD 학습 종료 ===\\n\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 9. 학습된 Actor 정책으로 상태별 행동 확인\n",
        "# ==============================\n",
        "print(\"▶ 학습된 Actor의 결정론적 정책 μ(s) (상태별 연속 행동 값)\")\n",
        "\n",
        "for s in range(n_states):\n",
        "    a_det = actor_forward(s, use_target=False)\n",
        "    print(f\"상태 {s}: 행동 a = {a_det:.4f}\")\n",
        "\n",
        "print(\"\\n▶ Greedy 기준 이산 정책(연속 a의 부호 기준 ←/→)\")\n",
        "action_symbol = {0: \"←\", 1: \"→\"}\n",
        "\n",
        "policy_str = \"\"\n",
        "for s in range(n_states):\n",
        "    if s == n_states - 1:\n",
        "        # 목표 상태는 G 로 표기\n",
        "        policy_str += \" G \"\n",
        "    else:\n",
        "        a_det = actor_forward(s, use_target=False)\n",
        "        env_action = 1 if a_det > 0.0 else 0\n",
        "        policy_str += f\" {action_symbol[env_action]} \"\n",
        "\n",
        "print(\"상태 0  1  2  3  4\")\n",
        "print(\"     \" + policy_str)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 10. 학습된 결정론적 정책으로 1회 에피소드 실행\n",
        "# ==============================\n",
        "print(\"\\n▶ 학습된 결정론적 정책(DDPGfD)으로 1회 에피소드 실행 예시\")\n",
        "\n",
        "state = reset()\n",
        "trajectory = [state]\n",
        "\n",
        "for step_idx in range(max_steps):\n",
        "    # Actor의 결정론적 행동 계산\n",
        "    a_det = actor_forward(state, use_target=False)\n",
        "    env_action = 1 if a_det > 0.0 else 0\n",
        "\n",
        "    next_state, reward, done = step(state, env_action)\n",
        "    trajectory.append(next_state)\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"방문한 상태들:\", trajectory)\n",
        "print(\"스텝 수:\", len(trajectory) - 1)\n",
        "print(\"마지막 상태가 목표(4)면 학습 성공!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6d9f3529-fc66-44fb-919f-93cff4c9e921",
      "metadata": {
        "id": "6d9f3529-fc66-44fb-919f-93cff4c9e921"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2ac68542-578c-488e-9968-b18b93a742d5",
      "metadata": {
        "id": "2ac68542-578c-488e-9968-b18b93a742d5"
      },
      "outputs": [],
      "source": [
        "###################################################################\n",
        "##\n",
        "###################################################################"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}