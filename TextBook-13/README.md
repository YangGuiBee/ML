#  13 : íšŒê·€ í‰ê°€ ì§€í‘œ

---

	[1] í‰ê·  ì˜¤ì°¨ (Mean Error, ME)
 	[2] í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (Mean Absolute Error, MAE)
	[3] í‰ê·  ì œê³± ì˜¤ì°¨ (Mean Squared Error, MSE)
	[4] í‰ê·  ì œê³± ì˜¤ì°¨(ë¡œê·¸ì ìš©) (Mean Squared Log Error, MSLE)
	[5] í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (Root Mean Squared Error, RMSE)
	[6] í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(ë¡œê·¸ì ìš©) (Root Mean Squared Log Error, RMSLE)
 	[7] í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨ (Mean Percentage Error, MPE)
	[8] í‰ê·  ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨ (Mean Absolute Percentage Error, MAPE)
	[9] í‰ê·  ì ˆëŒ€ ê·œëª¨ ì˜¤ì°¨ (MASE(Mean Absolute Scaled Error)
	[10] R2 score
	  
---

# [1] í‰ê·  ì˜¤ì°¨ (Mean Error, ME)
![](./images/ME.svg)
<br>
â–£ ì •ì˜: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ í‰ê· ë‚¸ ê°’ìœ¼ë¡œ, ì˜¤ì°¨ì˜ ë°©í–¥ì„±ì„ í¬í•¨í•œ ì˜ˆì¸¡ì˜¤ì°¨ì˜ ì‚°ìˆ í‰ê· ì„ ì˜ë¯¸<br>
â–£ í•„ìš”ì„±: ì˜¤ì°¨ê°€ ì–‘ìˆ˜ì¸ì§€ ìŒìˆ˜ì¸ì§€, í‰ê· ì ìœ¼ë¡œ ê³¼ëŒ€í‰ê°€ ë˜ëŠ” ê³¼ì†Œí‰ê°€ë˜ëŠ”ì§€ë¥¼ íŒŒì•…<br>
â–£ ì¥ì : ì˜¤ì°¨ì˜ ë°©í–¥ì„±ì„ ë°˜ì˜í•˜ì—¬ ëª¨ë¸ì˜ í¸í–¥(bias)ì„ ë¶„ì„<br>
â–£ ë‹¨ì : ì–‘ìˆ˜ì™€ ìŒìˆ˜ê°€ ìƒì‡„ë˜ë¯€ë¡œ, ì‹¤ì œ ì˜¤ì°¨ì˜ í¬ê¸°ë¥¼ íŒë‹¨í•˜ê¸° ì–´ë µë‹¤<br>

	def ME(y, t):
		return (y-t).mean(axis=None)

<br>

# [2] í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (Mean Absolute Error, MAE)
![](./images/MAE.svg)
<br>
â–£ ì •ì˜: ì‹¤ì œ ì •ë‹µ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ“ê°’ìœ¼ë¡œ ë³€í™˜í•œ ë’¤ í•©ì‚°í•˜ì—¬ í‰ê· ì„ êµ¬í•œë‹¤.<br>
â–£ í•„ìš”ì„±: ëª¨ë¸ì˜ í‰ê· ì ì¸ ì˜ˆì¸¡ ì˜¤ì°¨ í¬ê¸°ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…, íŠ¹ì´ê°’ì´ ë§ì€ ê²½ìš°ì— ì£¼ë¡œ ì‚¬ìš©<br>
â–£ ì¥ì : ì§ê´€ì ì´ê³  ì •ë‹µ ë° ì˜ˆì¸¡ ê°’ê³¼ ê°™ì€ ë‹¨ìœ„ë¥¼ ê°€ì§€ê³ , MSE, RMSEì— ë¹„í•´ ê·¹ë‹¨ê°’(outlier)ì— ëœ ë¯¼ê°<br>
â–£ ë‹¨ì : ì ˆëŒ“ê°’ì„ ì·¨í•˜ë¯€ë¡œ underestimates/overestimatesì¸ì§€ì— ëŒ€í•œ íŒë‹¨ì„ í•  ìˆ˜ ì—†ìœ¼ë©°, ìŠ¤ì¼€ì¼ ì˜ì¡´ì (scal dependency)ìœ¼ë¡œ ëª¨ë¸ë§ˆë‹¤ ì—ëŸ¬ í¬ê¸°ê°€ ë™ì¼í•´ë„ ì—ëŸ¬ìœ¨ì€ ë™ì¼í•˜ì§€ ì•Šê³ , ì˜¤ì°¨ì˜ ë°©í–¥ì„±ì„ ì•Œ ìˆ˜ ì—†ë‹¤<br>

	def MAE(y, t):
    	return (abs(y - t)).mean(axis=None)
   
<br>

# [3] í‰ê·  ì œê³± ì˜¤ì°¨ (Mean Squared Error, MSE)
![](./images/MSE.svg)
<br>
â–£ ì •ì˜: ì‹¤ì œ ì •ë‹µ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´ë¥¼ ì œê³±(ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì°¨ì´ì˜ ë©´ì )í•œ ë’¤ í‰ê· ì„ êµ¬í•œë‹¤.<br>
â–£ í•„ìš”ì„±: ì˜¤ì°¨ì˜ í¬ê¸°ë¥¼ ê°•ì¡°í•´ í° ì˜¤ì°¨ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘<br>
â–£ ì¥ì : ì§ê´€ì ì´ë©°, ëª¨ë¸ì˜ í° ì˜¤ì°¨ë¥¼ ë” ì˜ ì‹ë³„ ê°€ëŠ¥<br>
â–£ ë‹¨ì : ì˜ˆì¸¡ ë³€ìˆ˜ì™€ ë‹¨ìœ„ê°€ ë‹¤ë¥´ë©°, ì˜¤ì°¨ë¥¼ ì œê³±í•˜ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜ì— ë¯¼ê°(ì œê³±í•˜ê¸° ë•Œë¬¸ì— 1ë¯¸ë§Œì˜ ì—ëŸ¬ëŠ” ì‘ì•„ì§€ê³  ê·¸ ì´ìƒì˜ ì—ëŸ¬ëŠ” ì»¤ì§), ì œê³±ì„ ì”Œìš°ê²Œ ë˜ì–´ underestimates/overestimatesì¸ì§€ íŒŒì•…í•˜ê¸° í˜ë“¤ë©°, ìŠ¤ì¼€ì¼ ì˜ì¡´ì (scal dependency)ì´ë¼ ëª¨ë¸ë§ˆë‹¤ ì—ëŸ¬ëŸ¬ í¬ê¸°ê°€ ë™ì¼í•´ë„ ì—ëŸ¬ìœ¨ì€ ë™ì¼í•˜ì§€ ì•Šì€ ë‹¨ì . ì˜¤ì°¨ì œê³±í•©(SSE)ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ì˜¤ì°¨ì œê³±í•©ìœ¼ë¡œëŠ” ì‹¤ì œ ì˜¤ì°¨ê°€ ì»¤ì„œ ê°’ì´ ì»¤ì§€ëŠ” ê²ƒì¸ì§€ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì„œ ê°’ì´ ì»¤ì§€ëŠ” ê²ƒì¸ì§€ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ê²Œ ëœë‹¤.<br>

	def MSE(y, t):
    	return ((y-t)**2).mean(axis=None)

	def SSE(y, t):
    	return 0.5*np.sum((y-t)**2)

<br>

# [4] í‰ê·  ì œê³± ì˜¤ì°¨(ë¡œê·¸ì ìš©) (Mean Squared Log Error, MSLE)
![](./images/MSLE.svg)
<br>
â–£ ì •ì˜: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ë¡œê·¸ ì°¨ì´ì— ëŒ€í•œ ì œê³± í‰ê· ìœ¼ë¡œ MSEì— ë¡œê·¸ë¥¼ ì ìš©<br> 
â–£ í•„ìš”ì„±: ì‘ì€ ê°’ì˜ ìƒëŒ€ì ì¸ ì°¨ì´ë¥¼ ê°•ì¡°í•˜ë©° í° ê°’ì˜ ì°¨ì´ë¥¼ ì™„í™”<br>
â–£ ì¥ì : ì‘ì€ ê°’ì— ì§‘ì¤‘í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì— ì í•©(ê²°ì • ê°’ì´ í´ìˆ˜ë¡ ì˜¤ë¥˜ê°’ë„ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì¼ë¶€ í° ì˜¤ë¥˜ê°’ë“¤ë¡œ ì¸í•´ ì „ì²´ ì˜¤ë¥˜ê°’ì´ ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ì•„ì¤€ë‹¤)<br>
â–£ ë‹¨ì : ë¡œê·¸ ì—°ì‚°ìœ¼ë¡œ ì¸í•´ ìŒìˆ˜ ê°’ì€ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥<br>

	def MSLE(y, t):
		return np.log((y-t)**2).mean(axis=None)

<br>

# [5] í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (Root Mean Squared Error, RMSE)
![](./images/RMSE.svg)
<br>
â–£ ì •ì˜: MSEì˜ ì œê³±ê·¼ìœ¼ë¡œ, ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ê°„ì˜ í‰ê· ì  ì°¨ì´ë¥¼ ì› ë‹¨ìœ„ë¡œ í™˜ì‚°<br>
â–£ í•„ìš”ì„±: ë‹¨ìœ„ë¥¼ ì‹¤ì œ ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ ì¡°ì •<br>
â–£ ì¥ì : í•´ì„ì´ ì‰¬ìš°ë©°, í° ì˜¤ì°¨ë¥¼ ê°•ì¡°(MSEì— ë£¨íŠ¸ëŠ” ì”Œì›Œì„œ ì—ëŸ¬ë¥¼ ì œê³±í•´ì„œ ìƒê¸°ëŠ” ê°’ì˜ ì™œê³¡ì´ ì¤„ì–´ë“ ë‹¤)<br>
â–£ ë‹¨ì : ê·¹ë‹¨ê°’ì— ë¯¼ê°í•˜ê³ , ì œê³± í›„ ë£¨íŠ¸ë¥¼ ì”Œìš°ê¸° ë•Œë¬¸ì— MAEì²˜ëŸ¼ ì‹¤ì œ ê°’ì— ëŒ€í•´ underestimates/overestimatesì¸ì§€ íŒŒì•…í•˜ê¸° í˜ë“¤ê³ , ìŠ¤ì¼€ì¼ ì˜ì¡´ì (scal dependency)ìœ¼ë¡œ ëª¨ë¸ë§ˆë‹¤ ì—ëŸ¬ í¬ê¸°ê°€ ë™ì¼í•´ë„ ì—ëŸ¬ìœ¨ì€ ë™ì¼í•˜ì§€ ì•Šì€ ë‹¨ì <br>

	def RMSE(y, t):
		return np.sqrt(((y - t) ** 2).mean(axis=None))

<br>

# [6] í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(ë¡œê·¸ì ìš©) (Root Mean Squared Log Error, RMSLE)
![](./images/RMSLE.svg)
<br>
â–£ ì •ì˜: MSLEì˜ ì œê³±ê·¼ ê°’ìœ¼ë¡œ, ë¡œê·¸ ì°¨ì´ë¥¼ ì› ë‹¨ìœ„ë¡œ í™˜ì‚°í•œ ê°’ìœ¼ë¡œ RMSEê°’ì— ë¡œê·¸ë¥¼ ì·¨í•œ ê°’<br>
â–£ í•„ìš”ì„±: ì‘ì€ ê°’ì˜ ìƒëŒ€ì  ì°¨ì´ë¥¼ í‰ê°€<br>
â–£ ì¥ì : ìƒëŒ€ì  ì˜¤ì°¨ì— ê°•ì (ê²°ì • ê°’ì´ í´ ìˆ˜ë¡ ì˜¤ë¥˜ ê°’ë„ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì¼ë¶€ í° ì˜¤ë¥˜ ê°’ë“¤ë¡œì¸í•´ ì „ì²´ ì˜¤ë¥˜ê°’ì´ ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ì•„ì¤€ë‹¤)<br>
â–£ ë‹¨ì : ìŒìˆ˜ ê°’ ë¶ˆê°€<br>

	def RMSLE(y, t):
		return np.log(np.sqrt(((y - t) ** 2).mean(axis=None)))

<br>

# [7] í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨ (Mean Percentage Error, MPE)
![](./images/MPE.svg)
<br>
â–£ ì •ì˜: ì˜¤ì°¨ë¥¼ ì‹¤ì œê°’ì— ëŒ€í•œ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°í•´ í‰ê· <br>
â–£ í•„ìš”ì„±: ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ë°©í–¥ì„±ê³¼ ë°±ë¶„ìœ¨ í¬ê¸°ë¥¼ íŒŒì•…, ì ˆëŒ€ì ì¸ ì˜ë¯¸ì˜ ì˜ˆì¸¡ì˜¤ì°¨ë¿ ì•„ë‹ˆë¼ ìƒëŒ€ì ì¸ ì˜ë¯¸ì˜ ì˜ˆì¸¡ì˜¤ì°¨ê°€ í•„ìš”í•  ê²½ìš°ì— ê³„ì‚°<br>
â–£ ì¥ì : ìƒëŒ€ì  í¬ê¸° ë¹„êµ ê°€ëŠ¥(ìŒìˆ˜ë©´ overperformance, ì–‘ìˆ˜ë©´ underperformanceìœ¼ë¡œ íŒë‹¨), ëª¨ë¸ì´ underestimates/overestimatesì¸ì§€ íŒë‹¨í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì <br>
â–£ ë‹¨ì : ì‹¤ì œê°’ì´ 0ì¼ ë•Œ ê³„ì‚° ë¶ˆê°€<br>

	def MPE(y, t):
		return (((y-t)/y)*100).mean(axis=None)

<br>

# [8] í‰ê·  ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨ (Mean Absolute Percentage Error, MAPE)
![](./images/MAPE.svg)
<br>
â–£ ì •ì˜: ì ˆëŒ€ ì˜¤ì°¨ë¥¼ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°í•´ í‰ê· (MAEë¥¼ ë¹„ìœ¨, í¼ì„¼íŠ¸ë¡œ í‘œí˜„í•˜ì—¬ ìŠ¤ì¼€ì¸ ì˜ì¡´ì  ì—ëŸ¬ì˜ ë¬¸ì œì ì„ ê°œì„ )<br>
â–£ í•„ìš”ì„±: ì˜¤ì°¨ë¥¼ ìƒëŒ€ì ìœ¼ë¡œ í‰ê°€<br>
â–£ ì¥ì  : ì§ê´€ì ì´ê³ , ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì—ëŸ¬ìœ¨ ë¹„êµê°€ ì‰¬ìš´ ì¥ì <br>
â–£ ë‹¨ì : 0ê°’ ë¬¸ì œì™€ ê·¹ë‹¨ê°’ì— ë¯¼ê°(ì‹¤ì œ ì •ë‹µë³´ë‹¤ ë‚®ê²Œ ì˜ˆì¸¡í–ˆëŠ”ì§€, ë†’ê²Œ í–ˆëŠ”ì§€ë¥¼ íŒŒì•…í•˜ê¸° í˜ë“¤ê³  ì‹¤ì œ ì •ë‹µì´ 1ë³´ë‹¤ì‘ì„ ê²½ìš°,ë¬´í•œëŒ€ì˜ ê°’ìœ¼ë¡œ ìˆ˜ë ´)<br>

	def MAPE(y, t):
		return ((abs((y-t)/y))*100).mean(axis=None)

<br>

# [9] í‰ê·  ì ˆëŒ€ ê·œëª¨ ì˜¤ì°¨ (MASE(Mean Absolute Scaled Error)
<!-- ![](./images/MASE.svg) ![](./images/MASE1.svg) -->
![](./images/[9].png)
<br>
â–£ ì •ì˜: ì˜ˆì¸¡ê°’ì˜ ì˜¤ì°¨ë¥¼ ê¸°ì¤€ ì‹œì ì˜ ì˜¤ì°¨ì™€ ë¹„êµí•˜ì—¬ ë¹„ìœ¨í™”(ë°ì´í„°ë¥¼ ì²™ë„í™”í•˜ì—¬ ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ì˜¤ì°¨ì˜ ì ˆëŒ€ê°’ì— ëŒ€í•œ í‰ê· )<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì— ë”°ë¼ ìœ ì—°í•œ í‰ê°€ ê°€ëŠ¥<br>
â–£ ì¥ì : ìŠ¤ì¼€ì¼ì— ëŒ€í•œ ì˜ì¡´ì„±ì´ ë‚®ì•„ì„œ ì•ˆì •ì ìœ¼ë¡œ ë¹„êµ ê°€ëŠ¥<br>
â–£ ë‹¨ì : í•´ì„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì–´ë µë‹¤<br>

	def MASE(y, t):
		n = len(y)
		d = np.abs(np.diff(y)).sum() / (n - 1)
		errors = abs(y-t)
		return errors.mean(axis=None)/d

<br>

# [10] R2 score
â–£ ì •ì˜: ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ **SST(Total Sum of Squares)** ê´€ì¸¡ê°’ì—ì„œ ê´€ì¸¡ê°’ì˜ í‰ê· (í˜¹ì€ ì¶”ì •ì¹˜ì˜ í‰ê· )ì„ ëº€ ê²°ê³¼ì˜ ì´í•©ì¸ ì´ ì œê³±í•©<br>
â–£ í•„ìš”ì„±: ì „ì²´(Total)ì— ëŒ€í•œ ë³€ë™ì„±ì„ ë‚˜íƒ€ëƒ„ìœ¼ë¡œì¨ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ íŒŒì•…<br>
â–£ ì¥ì : ì§ê´€ì <br>
â–£ ë‹¨ì : ëª¨ë¸ì´ ìµœì†Œí•œì˜ ê¸°ì¤€ë„ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ëŠ” ìŒìˆ˜ê°€ ë  ìˆ˜ ìˆìŒ<br>
![](./images/f_R2.png)
<br>
![](./images/ff_SST.png)
<br>

## SST(Total Sum of Squares) : ì´ ë³€ë™
â–£ ì •ì˜ : ë°ì´í„°ì˜ ì´ ë³€ë™ëŸ‰ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ, ì‹¤ì œê°’($ğ‘¦_ğ‘–$)ê³¼ í‰ê· ê°’($\overline{y}$) ê°„ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í•©í•œ ê°’ìœ¼ë¡œ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ë¶„ì‚°ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„<br>
â–£ í•„ìš”ì„± : íšŒê·€ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ê¸°ì¤€ì„ ì´ ë˜ê³ , ëª¨ë¸ ì—†ì´ë„ ë°ì´í„°ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë³€ë™ì„±ì„ ê³„ì‚°<br>
â–£ ì¥ì  : ë°ì´í„°ì˜ ì „ì²´ ë³€ë™ëŸ‰ì„ ëª…í™•íˆ ê³„ì‚°í•˜ì—¬ ëª¨ë¸ í‰ê°€ì˜ ê¸°ë³¸ ì²™ë„ê°€ ë˜ë©°, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ SSR, SSEì™€ í•¨ê»˜ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨<br>
â–£ ë‹¨ì  : ì‹¤ì œê°’ì˜ ë³€ë™ëŸ‰ë§Œ ì¸¡ì •í•  ë¿, ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ëŠ” ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ì—†ìœ¼ë©°, ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì´ë‚˜ ë‹¨ìœ„ì— ë¯¼ê°í•˜ì—¬ í¬ê¸°ê°€ í¬ê²Œ ë³€í•  ìˆ˜ ìˆìŒ<br>
![](./images/f_SST.png)
<br>


## SSR(Sum of Squares due to Regression) : íšŒê·€ì— ì˜í•œ ë³€ë™
â–£ ì •ì˜ : ì˜ˆì¸¡ê°’(ğ‘¦^ğ‘–)ê³¼ í‰ê· ê°’($\overline{y}$)ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í•©í•œ ê°’ìœ¼ë¡œ ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„°ì˜ ë³€ë™ ì¤‘ì—ì„œ ì„¤ëª…í•œ ë³€ë™ëŸ‰ì„ ë‚˜íƒ€ëƒ„<br>
â–£ í•„ìš”ì„± : ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë°ì´í„°ë¥¼ ì„¤ëª…í–ˆëŠ”ì§€ íŒë‹¨í•˜ëŠ” ì²™ë„ë¡œ, RÂ² Score ê³„ì‚°ì—ì„œ í•µì‹¬ ì—­í• ì„ í•˜ë©°, ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ<br>
â–£ ì¥ì  : ëª¨ë¸ì´ ë°ì´í„° íŒ¨í„´ì„ ì–¼ë§ˆë‚˜ ì˜ íŒŒì•…í–ˆëŠ”ì§€ ëª…í™•íˆ ë‚˜íƒ€ë‚´ë©°, ğ‘†ğ‘†ğ‘…ì´ í´ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨ì„ ì˜ë¯¸<br>
â–£ ë‹¨ì  : ë‹¨ë…ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ê³¼ì í•© ì—¬ë¶€ë¥¼ í‰ê°€í•˜ê¸° ì–´ë µê³ , ë°ì´í„°ì— ì¡ìŒ(noise)ì´ ë§ì„ ê²½ìš° ì˜ëª»ëœ ì„¤ëª…ë ¥ì„ ì¸¡ì •<br>
![](./images/f_SSR.png)
<br>


## SSE(Sum of Squares Residual of Error) : ì”ì°¨ ì œê³±í•©
â–£ ì •ì˜ : ì‹¤ì œê°’($ğ‘¦_ğ‘–$)ê³¼ ì˜ˆì¸¡ê°’($\widehat{y_i}$)ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í•©í•œ ê°’ìœ¼ë¡œ ëª¨ë¸ì´ ì„¤ëª…í•˜ì§€ ëª»í•œ ë°ì´í„°ì˜ ë³€ë™ëŸ‰(ì”ì°¨)ì„ ë‚˜íƒ€ëƒ„<br>
â–£ í•„ìš”ì„± : ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ë©°, ì”ì°¨ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ì œì‹œ<br>
â–£ ì¥ì  : ëª¨ë¸ì˜ í•œê³„ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìœ¼ë©°, SSEê°€ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ì˜ë¯¸<br>
â–£ ë‹¨ì  : SSEì˜ í¬ê¸°ê°€ ë°ì´í„°ì˜ í¬ê¸°ì™€ ë‹¨ìœ„ì— ë”°ë¼ í¬ê²Œ ë³€í•˜ë©°, ê·¹ë‹¨ê°’(outliers)ì— ë¯¼ê°í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì´ ì™œê³¡ë  ìˆ˜ ìˆìŒ<br>
![](./images/f_SSE.png)
<br>

---

![](./images/SST.svg)
<br>


SSE (Sum of Squares Residual of Error, ì”ì°¨ ì œê³±í•©)
â–£ ì •ì˜
SSEëŠ” ëª¨ë¸ì´ ì„¤ëª…í•˜ì§€ ëª»í•œ ë°ì´í„°ì˜ ë³€ë™ëŸ‰(ì”ì°¨)ì„ ë‚˜íƒ€ëƒ…


**SSR(Sum of Squares due to Regression) :** íšŒê·€ì‹ ì¶”ì • ê°’ê³¼ ê´€ì¸¡ê°’ì˜ í‰ê·  ê°„ ì°¨ì´ì¸ íšŒê·€ ì œê³±í•©<br>
íšŒê·€ì‹ìœ¼ë¡œ ë¶€í„° ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì—ì„œ ê´€ì¸¡ê°’(y)ì˜ í‰ê· (í˜¹ì€ ì¶”ì •ì¹˜ì˜ í‰ê· )ì„ ëº€ ê²°ê³¼ì˜ ì´í•©<br>
ESS(Explained Sum of Squares)ë¡œ ë¶„ì„ì„ í†µí•´ ì„¤ëª… ê°€ëŠ¥í•œ ìˆ˜ì¹˜ë¡œ ì§ì„ (Regression)ì— ëŒ€í•œ ë³€ë™ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤.<br>

![](./images/SSR.svg)
<br>

![](./images/SST_SSE.png)
<br>
ì¶œì²˜ : https://devhwi.tistory.com/13


**SSE(Sum of Squares Residual of Error) :** ì‹¤ì œ ê´€ì¸¡ê°’(y)ê³¼ ì˜ˆì¸¡ê°’ ì‚¬ì´ì˜ ì°¨ì¸ ì”ì°¨(Residual)ì˜ ì´í•©<br>
ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê´€ì¸¡ê°’ì˜ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” íšŒê·€ì‹ìœ¼ë¡œëŠ” ì„¤ëª…í•  ìˆ˜ ì—†ëŠ” ì„¤ëª… ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¹˜ì´ë‹¤.<br> 
SSEê°’ì€ ì˜¤ì°¨(Error)ì— ëŒ€í•œ ë³€ë™ì„±ì„ ë‚˜íƒ€ë‚´ëŠ”ë°, ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤. ìì£¼ ì‚¬ìš©ë˜ëŠ” MSE(Mean Squared Error)ëŠ” SSEë¥¼ í‘œì¤€í™”í•œ ê°œë…ì´ë‹¤.<br>

![](./images/SSE.svg)
<br>

ìœ„ì˜ ìˆ˜ì‹ì— ë”°ë¼ ê²°êµ­ SST=SSR+SSEì´ë©°, SSRê³¼ SSEëŠ” ë°˜ë¹„ë¡€ ê´€ê³„ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.<br>
ì´ ë³€ë™ ì¤‘ ì„¤ëª… ê°€ëŠ¥í•œ ë³€ë™ì˜ ë¹„ìœ¨ì„ ëœ»í•˜ëŠ” ê²°ì •ê³„ìˆ˜(R2)ëŠ” ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ì„±ë¦½í•œë‹¤.<br>


![](./images/R2.svg)
<br>
ë‹¤ë¥¸ ì§€í‘œ(MAE, MSE, RMSE)ë“¤ì€ ëª¨ë¸ë§ˆë‹¤ ê°’ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì ˆëŒ€ ê°’ë§Œ ë³´ê³  ì„ ëŠ¥ì„ íŒë‹¨í•˜ê¸° ì–´ë ¤ìš´ ë°˜ë©´, $R^2$ scoreëŠ” ìƒëŒ€ì ì¸ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ê¸° ë¹„êµê°€ ì‰½ë‹¤.<br>
ì‹¤ì œ ê°’ì˜ ë¶„ì‚° ëŒ€ë¹„ ì˜ˆì¸¡ê°’ì˜ ë¶„ì‚° ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤.<br>
0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì¢‹ë‹¤.<br>

<br><br>


![](./images/SST.png)
<br>
ì¶œì²˜ : https://medium.com/coders-mojo/data-science-and-machine-learning-projects-mega-compilation-part-5-e50baa2faa85


	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.linear_model import LinearRegression
 	from sklearn.metrics import r2_score

	#í•´ë‹¹ êµ¬ë¬¸ì´ ì‚¬ìš©ëœ íŒŒì´ì¬ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰
	if __name__ == '__main__':
    
	    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
	    x = np.random.rand(1000)*100
	    y = 0.8*x+np.random.randn(1000)*30

	    # Linear Regrssion model ìƒì„±
	    model = LinearRegression() 
    
	    # Linear Regression model í•™ìŠµ
	    model.fit(x.reshape(-1,1), y) 
    
	    # Prediction
	    y_new = model.predict(np.array([6]).reshape((-1, 1))) 
	    print("Data Prediction: ", y_new)
    
	    # Linear Regression model í‰ê°€
	    r_sq = model.score(x.reshape(-1,1), y)  
    	    print("ê²°ì • ê³„ìˆ˜ (model.score): ", r_sq)
    	    r2 = r2_score(y, model.predict(x.reshape(-1,1)))  
    	    print("ê²°ì • ê³„ìˆ˜ (r2_score): ", r2)
    
	    # Linear Model ì‹ 
	    b0,b1 = model.intercept_, model.coef_[0]   
	    print("ê¸°ìš¸ê¸°", model.coef_[0])
	    print("ì ˆí¸", model.intercept_)

	    # ì‹œê°í™”
	    plt.scatter(x, y, s=5)
	    plt.plot(x, model.predict(x.reshape(-1,1)), color='red', linewidth=2)
	    plt.annotate(f'y = {b1:.2f}x + {b0:.2f}', xy=(0.7*max(x), 0.8*max(y)))
	    plt.show()



# ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì„ í™œìš©í•œ íšŒê·€ëª¨ë¸ ìµœì í™” ê¸°ë²•

**(1) LinearRegression :**  solver ë§¤ê°œë³€ìˆ˜ì—ì„œ sag(Stochastic Average Gradient), lsqr ë³€í˜• ì‚¬ìš©<br>

	from sklearn.linear_model import LinearRegression

	model = LinearRegression(solver='sag')  # ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ í•´ë²•
	model.fit(X_train, y_train)
	predictions = model.predict(X_test)


**(2) SGDRegressor (Stochastic Gradient Descent Regressor) :** ëŒ€ê·œëª¨ ë°ì´í„°ì— ì í•©, ì •ê·œí™”ë¥¼ ìœ„í•œ L2, L1 ë° ElasticNet ê·œì œë¥¼ ì§€ì›<br>

 	from sklearn.linear_model import SGDRegressor

	model = SGDRegressor()
	model.fit(X_train, y_train)
	predictions = model.predict(X_test)


**(3) Ridge, Lasso, ElasticNet :** ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ solver ë§¤ê°œë³€ìˆ˜ì—ì„œ sag(Stochastic Average Gradient) ì„ íƒ<br>

	from sklearn.linear_model import Ridge

	model = Ridge(solver='saga')  # ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ í•´ë²•
	model.fit(X_train, y_train)
	predictions = model.predict(X_test)


   **(4) PassiveAggressiveRegressor :** ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê¸° ìœ„í•´ ê²½ì‚¬í•˜ê°•ë²•ì˜ ë³€í˜• ì¤‘ í•˜ë‚˜ì¸ ìˆ˜ë™ ê³µê²©ì  í•™ìŠµ(passive-aggressive learning)ì„ ì‚¬ìš©<br>
   
	from sklearn.linear_model import PassiveAggressiveRegressor

	model = PassiveAggressiveRegressor()
	model.fit(X_train, y_train)
	predictions = model.predict(X_test)

---

(ë°ì´í„°êµ¬ì¡°) ![](./images/db.png)
<br>
(ë°ì´í„°ì…‹) https://github.com/YangGuiBee/ML/blob/main/TextBook-13/insurance.csv
<br>

	################################################################################
	# ë°ì´í„° ì‹œê°í™”
	################################################################################
	
	import pandas as pd
	import seaborn as sns
	import matplotlib.pyplot as plt
	
	# Load dataset from URL
	data_url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-13/insurance.csv"
	df = pd.read_csv(data_url)
	
	# Ensure categorical columns are treated as categories
	df['sex'] = df['sex'].astype('category')
	df['smoker'] = df['smoker'].astype('category')
	df['region'] = df['region'].astype('category')
	
	# Basic information about the dataset
	print("Dataset Information:")
	print(df.info())
	print("\nBasic Statistics:")
	print(df.describe())
	
	# Visualizations
	plt.figure(figsize=(14, 10))
	
	# Distribution of Charges
	plt.subplot(3, 2, 1)
	sns.histplot(df['charges'], kde=True, bins=30, color='blue')
	plt.title('Distribution of Charges')
	plt.xlabel('Charges')
	plt.ylabel('Frequency')
	
	# Age Distribution
	plt.subplot(3, 2, 2)
	sns.histplot(df['age'], kde=True, bins=20, color='green')
	plt.title('Age Distribution')
	plt.xlabel('Age')
	plt.ylabel('Frequency')
	
	# BMI vs Charges
	plt.subplot(3, 2, 3)
	sns.scatterplot(x='bmi', y='charges', data=df, hue='smoker', palette='Set1', alpha=0.7)
	plt.title('BMI vs Charges')
	plt.xlabel('BMI')
	plt.ylabel('Charges')
	
	# Region Counts
	plt.subplot(3, 2, 4)
	sns.countplot(x='region', data=df, palette='Set2')
	plt.title('Region Counts')
	plt.xlabel('Region')
	plt.ylabel('Count')
	
	# Charges by Smoker
	plt.subplot(3, 2, 5)
	sns.boxplot(x='smoker', y='charges', data=df, palette='Set1')
	plt.title('Charges by Smoker')
	plt.xlabel('Smoker')
	plt.ylabel('Charges')
	
	# Children Distribution
	plt.subplot(3, 2, 6)
	sns.countplot(x='children', data=df, palette='Set3')
	plt.title('Children Distribution')
	plt.xlabel('Number of Children')
	plt.ylabel('Count')
	
	plt.tight_layout()
	plt.show()
	
	# Correlation Heatmap (Numerical Columns Only)
	plt.figure(figsize=(10, 8))
	numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
	sns.heatmap(df[numerical_columns].corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
	plt.title('Correlation Heatmap')
	plt.show()

<br>

![](./images/data.png)
<br>

![](./images/data2.png)
<br>

	################################################################################
	# Multiple Linear Regression 
	# Decision Tree Regression
	################################################################################
	
	# Check and install necessary packages
	import subprocess
	import sys
	
	def install(package):
	    try:
	        __import__(package)
	    except ImportError:
	        print(f"Installing {package}...")
	        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
	
	# List of required packages
	required_packages = ['pandas', 'scikit-learn', 'xgboost', 'lightgbm', 'numpy']
	for package in required_packages:
	    install(package)
	
	# Import libraries
	import pandas as pd
	import numpy as np
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import OneHotEncoder
	from sklearn.linear_model import LinearRegression
	from sklearn.tree import DecisionTreeRegressor
	from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
	
	# Load dataset from URL
	data_url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-13/insurance.csv"
	df = pd.read_csv(data_url)
	
	# Check and handle missing values
	print("Checking for missing values...")
	print(df.isnull().sum())  # Display the count of NaN values per column
	
	# Ensure no missing values
	assert not df.isnull().values.any(), "Data contains missing values!"
	
	# Preprocessing
	X = df.drop("charges", axis=1)
	y = df["charges"]
	categorical_features = ["sex", "smoker", "region"]
	numerical_features = ["age", "bmi", "children"]
	
	# Updated sparse_output instead of sparse
	encoder = OneHotEncoder(sparse_output=False, drop="first")
	X_encoded = encoder.fit_transform(X[categorical_features])
	X_numerical = X[numerical_features]
	
	X_preprocessed = pd.DataFrame(
	    np.hstack([X_numerical, X_encoded]),
	    columns=numerical_features + encoder.get_feature_names_out().tolist()
	)
	
	# Train-test split
	X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)
	
	# Evaluation metrics
	def evaluate_model(y_true, y_pred):
	    me = np.mean(y_pred - y_true)  # í‰ê·  ì˜¤ì°¨ (ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’)
	    mae = mean_absolute_error(y_true, y_pred)  # í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
	    mse = mean_squared_error(y_true, y_pred)  # í‰ê·  ì œê³± ì˜¤ì°¨
	    rmse = np.sqrt(mse)  # í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨
	
	    # Conditional MSLE calculation
	    if (y_true > 0).all() and (y_pred > 0).all():
	        msle = mean_squared_error(np.log1p(y_true), np.log1p(y_pred))  # í‰ê·  ì œê³± ì˜¤ì°¨ (ë¡œê·¸ ì ìš©)
	        rmsle = np.sqrt(msle)  # í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (ë¡œê·¸ ì ìš©)
	    else:
	        msle = np.nan
	        rmsle = np.nan
	
	    mpe = np.mean((y_pred - y_true) / y_true) * 100  # í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨
	    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # í‰ê·  ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨
	    r2 = r2_score(y_true, y_pred)  # R2 ì ìˆ˜
	
	    return {
	        "ME": me,
	        "MAE": mae,
	        "MSE": mse,
	        "MSLE": msle,
	        "RMSE": rmse,
	        "RMSLE": rmsle,
	        "MPE": mpe,
	        "MAPE": mape,
	        "R2": r2,
	    }
	
	# Initialize models
	models = {
	    "Multiple Linear Regression": LinearRegression(),
	    "Decision Tree Regression": DecisionTreeRegressor(),
	}
	
	# Train and evaluate models
	results = {}
	for name, model in models.items():
	    model.fit(X_train, y_train)
	    y_pred = model.predict(X_test)
	
	    # Check for invalid prediction values
	    if (y_pred < 0).any():
	        print(f"Warning: Model {name} produced negative predictions. Adjusting values to zero.")
	        y_pred = np.maximum(y_pred, 0)  # Replace negative predictions with 0
	
	    results[name] = evaluate_model(y_test, y_pred)
	
	# Format evaluation results for consistent decimal places
	evaluation_results = pd.DataFrame(results)
	evaluation_results = evaluation_results.applymap(lambda x: f"{x:.6f}" if pd.notnull(x) else "NaN")
	
	# Display formatted results
	print("\nModel Evaluation Results:")
	print(evaluation_results)
	
	# Add explanations for each metric in Korean
	metric_explanations = {
	    "ME": "í‰ê·  ì˜¤ì°¨ (Mean Error): ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ í‰ê·  ì°¨ì´. 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ.",
	    "MAE": "í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (Mean Absolute Error): ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì ˆëŒ€ì  ì°¨ì´ì˜ í‰ê· . ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.",
	    "MSE": "í‰ê·  ì œê³± ì˜¤ì°¨ (Mean Squared Error): ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì œê³± ì°¨ì´ í‰ê· . ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.",
	    "MSLE": "í‰ê·  ì œê³± ì˜¤ì°¨ (ë¡œê·¸ ì ìš©, Mean Squared Log Error): ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œì˜ í‰ê·  ì œê³± ì˜¤ì°¨. ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.",
	    "RMSE": "í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (Root Mean Squared Error): í‰ê·  ì œê³± ì˜¤ì°¨ì˜ ì œê³±ê·¼. ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.",
	    "RMSLE": "í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (ë¡œê·¸ ì ìš©, Root Mean Squared Log Error): ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œì˜ ì œê³±ê·¼ ì˜¤ì°¨. ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.",
	    "MPE": "í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨ (Mean Percentage Error): ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ë¹„ìœ¨ ì˜¤ì°¨ í‰ê· . 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ.",
	    "MAPE": "í‰ê·  ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨ (Mean Absolute Percentage Error): ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨ì˜ í‰ê· . ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.",
	    "R2": "R2 ì ìˆ˜ (Coefficient of Determination): ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ë‚˜íƒ€ëƒ„. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ.",
	}
	
	# Append explanations to results
	print("\nModel Evaluation Results with Explanations:")
	for metric, explanation in metric_explanations.items():
	    print(f"{metric}: {explanation}")
	    print(evaluation_results.loc[metric])
	    print()
	
	# Prediction
	test_input = pd.DataFrame(
	    [[55, 21, 2, "female", "no", "northeast"]],
	    columns=["age", "bmi", "children", "sex", "smoker", "region"]
	)
	
	# Encode and predict
	test_encoded = encoder.transform(test_input[categorical_features])
	test_numerical = test_input[numerical_features]
	test_preprocessed = pd.DataFrame(
	    np.hstack([test_numerical, test_encoded]),
	    columns=numerical_features + encoder.get_feature_names_out().tolist()
	)
	
	# Predictions for test input
	predictions = {}
	for name, model in models.items():
	    predictions[name] = model.predict(test_preprocessed)[0]
	
	# Format predictions for consistent decimal places
	predictions_df = pd.DataFrame(predictions, index=["Predicted Charges"]).applymap(lambda x: f"{x:.6f}")
	
	# Display predictions
	print("\nPredicted Charges for Input:")
	print(predictions_df)
	
<br>

	################################################################################
	# Ridge Regression
	# Lasso Regression
	# Elastic Net Regression
	# Random Forest Regression
	# XGBoost
	# LightGBM
	################################################################################
	
	# Check and install necessary packages
	import subprocess
	import sys
	
	def install(package):
	    try:
	        __import__(package)
	    except ImportError:
	        print(f"Installing {package}...")
	        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
	
	# List of required packages
	required_packages = ['pandas', 'scikit-learn', 'xgboost', 'lightgbm', 'numpy']
	for package in required_packages:
	    install(package)
	
	# Import libraries
	import pandas as pd
	import numpy as np
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import OneHotEncoder
	from sklearn.linear_model import Ridge, Lasso, ElasticNet
	from sklearn.ensemble import RandomForestRegressor
	from xgboost import XGBRegressor
	from lightgbm import LGBMRegressor
	from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
	
	# Load dataset from URL
	data_url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-13/insurance.csv"
	df = pd.read_csv(data_url)
	
	# Check and handle missing values
	print("Checking for missing values...")
	print(df.isnull().sum())  # Display the count of NaN values per column
	
	# Ensure no missing values
	assert not df.isnull().values.any(), "Data contains missing values!"
	
	# Preprocessing
	X = df.drop("charges", axis=1)
	y = df["charges"]
	categorical_features = ["sex", "smoker", "region"]
	numerical_features = ["age", "bmi", "children"]
	
	# Updated sparse_output instead of sparse
	encoder = OneHotEncoder(sparse_output=False, drop="first")
	X_encoded = encoder.fit_transform(X[categorical_features])
	X_numerical = X[numerical_features]
	
	X_preprocessed = pd.DataFrame(
	    np.hstack([X_numerical, X_encoded]),
	    columns=numerical_features + encoder.get_feature_names_out().tolist()
	)
	
	# Train-test split
	X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)
	
	# Evaluation metrics
	def evaluate_model(y_true, y_pred):
	    me = np.mean(y_pred - y_true)  # í‰ê·  ì˜¤ì°¨ (ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’)
	    mae = mean_absolute_error(y_true, y_pred)  # í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
	    mse = mean_squared_error(y_true, y_pred)  # í‰ê·  ì œê³± ì˜¤ì°¨
	    rmse = np.sqrt(mse)  # í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨
	
	    # Conditional MSLE calculation
	    if (y_true > 0).all() and (y_pred > 0).all():
	        msle = mean_squared_error(np.log1p(y_true), np.log1p(y_pred))  # í‰ê·  ì œê³± ì˜¤ì°¨ (ë¡œê·¸ ì ìš©)
	        rmsle = np.sqrt(msle)  # í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (ë¡œê·¸ ì ìš©)
	    else:
	        msle = np.nan
	        rmsle = np.nan
	
	    mpe = np.mean((y_pred - y_true) / y_true) * 100  # í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨
	    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # í‰ê·  ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨
	    r2 = r2_score(y_true, y_pred)  # R2 ì ìˆ˜
	
	    return {
	        "ME": me,
	        "MAE": mae,
	        "MSE": mse,
	        "MSLE": msle,
	        "RMSE": rmse,
	        "RMSLE": rmsle,
	        "MPE": mpe,
	        "MAPE": mape,
	        "R2": r2,
	    }
	
	# Initialize models
	models = {
	    "Ridge Regression": Ridge(),
	    "Lasso Regression": Lasso(),
	    "Elastic Net Regression": ElasticNet(),
	    "Random Forest Regression": RandomForestRegressor(random_state=42),
	    "XGBoost": XGBRegressor(random_state=42),
	    "LightGBM": LGBMRegressor(random_state=42),
	}
	
	# Train and evaluate models
	results = {}
	for name, model in models.items():
	    model.fit(X_train, y_train)
	    y_pred = model.predict(X_test)
	
	    # Check for invalid prediction values
	    if (y_pred < 0).any():
	        print(f"Warning: Model {name} produced negative predictions. Adjusting values to zero.")
	        y_pred = np.maximum(y_pred, 0)  # Replace negative predictions with 0
	
	    results[name] = evaluate_model(y_test, y_pred)
	
	# Format evaluation results for consistent decimal places
	evaluation_results = pd.DataFrame(results)
	evaluation_results = evaluation_results.applymap(lambda x: f"{x:.6f}" if pd.notnull(x) else "NaN")
	
	# Display formatted results
	print("\nModel Evaluation Results:")
	print(evaluation_results)
	
	# Prediction
	test_input = pd.DataFrame(
	    [[55, 21, 2, "female", "no", "northeast"]],
	    columns=["age", "bmi", "children", "sex", "smoker", "region"]
	)
	
	# Encode and predict
	test_encoded = encoder.transform(test_input[categorical_features])
	test_numerical = test_input[numerical_features]
	test_preprocessed = pd.DataFrame(
	    np.hstack([test_numerical, test_encoded]),
	    columns=numerical_features + encoder.get_feature_names_out().tolist()
	)
	
	# Predictions for test input
	predictions = {}
	for name, model in models.items():
	    predictions[name] = model.predict(test_preprocessed)[0]
	
	# Format predictions for consistent decimal places
	predictions_df = pd.DataFrame(predictions, index=["Predicted Charges"]).applymap(lambda x: f"{x:.6f}")
	
	# Display predictions
	print("\nPredicted Charges for Input:")
	print(predictions_df)

---

![](./images/tscore.png)
<br>



