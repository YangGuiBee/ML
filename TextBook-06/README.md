#  06 : ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning, UL) : ì—°ê´€ê·œì¹™, ì°¨ì›ì¶•ì†Œ

---

## ì—°ê´€ ê·œì¹™(Association Rule)
<br>

	í•µì‹¬ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜
	[AR-1] Apriori : ì„ í—˜ì  ì•Œê³ ë¦¬ì¦˜
	[AR-2] FP-Growth(Frequent Pattern Growth) : ë¹ˆë°œ íŒ¨í„´ ì„±ì¥
	[AR-3] Eclat(Equivalence Class Transformation) : ë™ë“± í´ë˜ìŠ¤ ë³€í™˜

	ê·œì¹™ í™•ì¥/ë³€í˜• ì•Œê³ ë¦¬ì¦˜
	[AR-4] Multi-level Association Rules : ë‹¤ê³„ì¸µ ì—°ê´€ê·œì¹™
	[AR-5] Multi-dimensional Association Rules : ë‹¤ì°¨ì› ì—°ê´€ê·œì¹™

	ì¶”ë¡ /ìµœì í™” ì•Œê³ ë¦¬ì¦˜
	[AR-6] Artificial Immune System : ì¸ê³µë©´ì—­ì‹œìŠ¤í…œ

	   
    [ì—°ê´€ ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]
    â–£ ì§€ì§€ë„(Support) : íŠ¹ì • í•­ëª© ì§‘í•©ì´ ì „ì²´ ê±°ë˜ì—ì„œ ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ ì‹ ë¢°ë„(Confidence) : Aê°€ ì£¼ì–´ì¡Œì„ ë•Œ Bê°€ ë°œìƒí•  í™•ë¥ 
    â–£ í–¥ìƒë„(Lift) : Aì™€ Bê°€ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê²½ìš°ì— ë¹„í•´ Aê°€ ë°œìƒí–ˆì„ ë•Œ Bê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ ë ˆë²„ë¦¬ì§€(Leverage) : Aì™€ Bì˜ ê²°í•© ë¹ˆë„ê°€ ë‘ í•­ëª©ì´ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¹ˆë„ì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ Conviction(í™•ì‹ ë„) : Aê°€ ë°œìƒí•  ë•Œ Bê°€ ë°œìƒí•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë…ë¦½ì ì¸ ê²½ìš°ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“œëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    â–£ ìƒê´€ê³„ìˆ˜(Correlation Coefficient)ëŠ” ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ì˜ ê°•ë„ì™€ ë°©í–¥

    

## ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction)
<br>

	ì „í†µ í†µê³„Â·ì„ í˜• ì•Œê³ ë¦¬ì¦˜
	[DR-1] PCA(Principal Component Analysis) : ì£¼ì„±ë¶„ ë¶„ì„	
	[DR-2] SVD(Singular Value Decomposition) : íŠ¹ì´ê°’ ë¶„í•´
	[DR-3] ICA(Independent Component Analysis) : ë…ë¦½ì„±ë¶„ ë¶„ì„
	[DR-4] LDA(Linear Discriminant Analysis) : ì„ í˜•íŒë³„ ë¶„ì„(ì§€ë„í•™ìŠµ ê¸°ë°˜)
	[DR-5] NMF(Non-negative Matrix Factorization)  : ë¹„ìŒìˆ˜ í–‰ë ¬ ë¶„í•´

	ë¹„ì„ í˜•/ë§¤ë‹ˆí´ë“œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
	[DR-6] t-SNE(t-distributed Stochastic Neighbor Embedding) : t-ë¶„í¬ í™•ë¥ ì  ì´ì›ƒ ì„ë² ë”©
	[DR-7] UMAP(Uniform Manifold Approximation and Projection) : ê· ì¼ ë§¤ë‹ˆí´ë“œ ê·¼ì‚¬ì  ì‚¬ì˜
	[DR-8] Isomap : ë“±ê±°ë¦¬ ë§¤í•‘
	[DR-9] MDS(Multidimensional Scaling) : ë‹¤ì°¨ì› ì²™ë„

	ì‹ ê²½ë§/ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜
	[DR-10] SOM(Self-Organizing Maps) : ìê¸° ì¡°ì§í™”
	
	
    [ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]
    â–£ ì¬êµ¬ì„± ì˜¤ë¥˜(Reconstruction Error) : ë³µì›ëœ ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„° ê°„ì˜ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)
    â–£ ë¶„ì‚° ìœ ì§€ìœ¨(Explained Variance Ratio) : ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨ë¡œ ë°ì´í„°ì˜ ì •ë³´ ì†ì‹¤ì •ë„ íŒŒì•…
    â–£ ìƒí˜¸ ì •ë³´ëŸ‰(Mutual Information) :  ì°¨ì› ì¶•ì†Œ ì „í›„ ë°ì´í„°ì˜ ì •ë³´ëŸ‰ì„ ë¹„êµ
    â–£ êµ°ì§‘ í‰ê°€ ì§€í‘œ : Silhouette Score, Davies-Bouldin Index, ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ(ARI, NMI)


**ì—°ê´€ ê·œì¹™ ì¶”ì²œ(Assocication Rule based Recommendation) :** ë¹…ë°ì´í„° ê¸°ë°˜ì˜ ë°ì´í„° ë§ˆì´ë‹ê¸°ë²•<br>
"Aë¥¼ ì„ íƒí•˜ë©´(antecedent), Bë„ ì„ íƒí•œë‹¤(Consequent)"ëŠ” ê·œì¹™ì„ ì°¾ëŠ”ë‹¤.<br>
<br>

<!--
![](./images/data.PNG)
<br>
-->

# [AR-1] Apriori : ì„ í—˜ì  ì•Œê³ ë¦¬ì¦˜

![](./images/apriori.png)
<br>
https://nyamin9.github.io/data_mining/Data-Mining-Pattern-3/#-31-apriori-algorithm---example<br><br>
â–£ ì •ì˜ : ì—°ê´€ê·œì¹™ í•™ìŠµì„ ìœ„í•œ ê³ ì „ì ì¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë¹ˆë°œí•­ëª© ì§‘í•©(frequent itemsets)ì„ ì°¾ì•„ë‚´ê³  ê·¸ ì§‘í•© ê°„ ì—°ê´€ì„±ì„ ì¶”ì¶œ<br>
â–£ í•„ìš”ì„± : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ì—°ê´€ì„±ì„ ë°œê²¬í•˜ëŠ” ì‘ì—…ì€ ê³„ì‚° ë¹„ìš©ì´ ë†’ì„ ìˆ˜ ìˆëŠ”ë°, AprioriëŠ” ë¹ˆë°œí•˜ì§€ ì•Šì€ í•­ëª© ì§‘í•©ì„ ë¨¼ì € ì œê±°í•´ ê²€ìƒ‰ ê³µê°„ì„ ì¤„ì—¬ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ íš¨ìœ¨ì ì¸ íƒìƒ‰<br>
â–£ ì¥ì  : ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì´í•´í•˜ê¸° ì‰½ê³ , ê³„ì‚° ê³µê°„ì„ ì¤„ì´ê¸° ìœ„í•œ ì‚¬ì „ ë‹¨ê³„ë¥¼ ê°€ì§€ê³  ìˆì–´, íš¨ìœ¨ì ì¸ íƒìƒ‰ì´ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ íƒìƒ‰ ê³µê°„ì´ ì»¤ì§€ë©´ ì„±ëŠ¥ì´ ì €í•˜ë˜ê³  ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë©°, ë§¤ë²ˆ ìƒˆë¡œìš´ í›„ë³´ì§‘í•© ìƒì„±ì— ë”°ë¥¸ í° ê³„ì‚°ë¹„ìš©<br>
â–£ ì‘ìš©ë¶„ì•¼ : ì‹œì¥ ë°”êµ¬ë‹ˆ ë¶„ì„(ì¥ë°”êµ¬ë‹ˆ ë°ì´í„°ì—ì„œ ìì£¼ í•¨ê»˜ êµ¬ë§¤ë˜ëŠ” ìƒí’ˆì„ ì°¾ìŒ), ì¶”ì²œ ì‹œìŠ¤í…œ, ì›¹ í˜ì´ì§€ ì—°ê²°ì„± ë¶„ì„<br>
â–£ ëª¨ë¸ì‹ : ì§€ì§€ë„(Support): íŠ¹ì • í•­ëª© ì§‘í•©ì´ ì „ì²´ ê±°ë˜ì—ì„œ ë°œìƒí•˜ëŠ” ë¹ˆë„, ì‹ ë¢°ë„(Confidence): íŠ¹ì • í•­ëª©ì´ ë°œìƒí•œ ê²½ìš° ë‹¤ë¥¸ í•­ëª©ì´ í•¨ê»˜ ë°œìƒí•  í™•ë¥ , í–¥ìƒë„(Lift): í•­ëª© ê°„ì˜ ìƒí˜¸ì˜ì¡´ì„±ì„ ì¸¡ì •<br>

	import pandas as pd	
	import matplotlib.pyplot as plt
 	from mlxtend.frequent_patterns import apriori
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='blue')
	plt.title('Item Frequency')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# apriori ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)
	
	# ìˆ˜ë™ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ ê³„ì‚°
	rules = []
	for itemset in frequent_itemsets['itemsets']:
	    if len(itemset) > 1:
	        for antecedent in combinations(itemset, len(itemset) - 1):
	            antecedent = frozenset(antecedent)
	            consequent = itemset - antecedent
	            
	            # ì§€ì§€ë„ ê³„ì‚°
	            support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	            
	            # ì‹ ë¢°ë„ ê³„ì‚°
	            antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	            confidence = support / antecedent_support
	            
	            # í–¥ìƒë„ ê³„ì‚°
	            consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	            lift = confidence / consequent_support
	            
	            # ê·œì¹™ ì €ì¥
	            rules.append({
	                'antecedents': antecedent,
	                'consequents': consequent,
	                'support': support,
	                'confidence': confidence,
	                'lift': lift
	            })
	
	# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
	rules_df = pd.DataFrame(rules)
	print("Association Rules:")
 	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

![](./images/1-1.png)
<br>

**ì§€ì§€ë„(support):** ê·œì¹™ ì „ì²´(AâˆªB)ê°€ ê±°ë˜ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨, {butter}â†’{bread}: 0.6 (ê±°ë˜ì˜ 60%ì—ì„œ butterì™€ bread ë™ì‹œ ë“±ì¥)<br>
**ì‹ ë¢°ë„(confidence):** ì„ í–‰í•­ì´ ë“±ì¥í–ˆì„ ë•Œ, ê²°ê³¼í•­ì´ í•¨ê»˜ ë“±ì¥í•  í™•ë¥ , {butter}â†’{bread}: 0.75 (butterê°€ ìˆìœ¼ë©´ 75% í™•ë¥ ë¡œ breadë„ í•¨ê»˜ êµ¬ë§¤)<br>
**í–¥ìƒë„(lift):** ë‘ í•­ëª©ì´ ë…ë¦½ì¼ ë•Œ ê¸°ëŒ€ë˜ëŠ” í™•ë¥  ëŒ€ë¹„ í•¨ê»˜ ë“±ì¥í•  í™•ë¥ , {butter}â†’{bread}: 0.9375 < 1 â†’ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•  ë•Œë³´ë‹¤ ê°™ì´ ë‚˜íƒ€ë‚  í™•ë¥ ì´ ì˜¤íˆë ¤ ë‚®ìŒ<br>

	support â‰¥ 0.4: ê·œì¹™ ìì²´ëŠ” ì¶©ë¶„íˆ ìì£¼ ë“±ì¥
	confidence â‰¥ 0.7: ê·œì¹™ ì‹ ë¢°ë„ê°€ ê½¤ ë†’ìŒ
	lift > 1: ê¸ì •ì  ì—°ê´€ì„±ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥

<br>

# [AR-2] FP-Growth(Frequent Pattern Growth) : ë¹ˆë°œ íŒ¨í„´ ì„±ì¥
â–£ ì •ì˜: Apriori ì•Œê³ ë¦¬ì¦˜ì˜ ëŒ€ì•ˆìœ¼ë¡œ FP-Tree(Frequent Pattern Tree)ë¥¼ í†µí•´ ë¹ˆë°œí•­ëª© ì§‘í•©ì„ ìƒì„±í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, Aprioriì™€ ë‹¬ë¦¬ ë§¤ë²ˆ í›„ë³´ì§‘í•©ì„ ìƒì„±í•˜ì§€ ì•Šìœ¼ë©°, ë°ì´í„°ì˜ íŠ¸ëœì­ì…˜ì„ ì§ì ‘ íƒìƒ‰í•˜ì—¬ ë¹ˆë°œí•­ëª© ì§‘í•©ì„ êµ¬í•œë‹¤.<br>
â–£ í•„ìš”ì„±: Aprioriì˜ ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆ<br>
â–£ ì¥ì : ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ê³ , ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë¹ ë¥´ê²Œ ì‘ë™<br>
â–£ ë‹¨ì : FP-íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë©°, êµ¬í˜„ì´ ë³µì¡í•˜ê³  FP-Tree ìƒì„±ì„ ìœ„í•œ í•™ìŠµì´ í•„ìš”<br>
â–£ ì‘ìš©ë¶„ì•¼: ëŒ€ê·œëª¨ ë°ì´í„° ë¶„ì„, ì „ììƒê±°ë˜ ì¶”ì²œ ì‹œìŠ¤í…œ<br>

	import pandas as pd	
	import matplotlib.pyplot as plt
 	from mlxtend.frequent_patterns import fpgrowth
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='green')
	plt.title('Item Frequency (FP-Growth)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# FP-Growth ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)
	
	# ìˆ˜ë™ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ ê³„ì‚°
	rules = []
	for itemset in frequent_itemsets['itemsets']:
	    if len(itemset) > 1:
	        for antecedent in combinations(itemset, len(itemset) - 1):
	            antecedent = frozenset(antecedent)
	            consequent = itemset - antecedent
	            
	            # ì§€ì§€ë„ ê³„ì‚°
	            support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	            
	            # ì‹ ë¢°ë„ ê³„ì‚°
	            antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	            confidence = support / antecedent_support
	            
	            # í–¥ìƒë„ ê³„ì‚°
	            consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	            lift = confidence / consequent_support
	            
	            # ê·œì¹™ ì €ì¥
	            rules.append({
	                'antecedents': antecedent,
	                'consequents': consequent,
	                'support': support,
	                'confidence': confidence,
	                'lift': lift
	            })
	
	# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
	rules_df = pd.DataFrame(rules)
	print("Association Rules (FP-Growth):")
	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

![](./images/1-2.png)   
<br>

# [AR-3] Eclat(Equivalence Class Transformation) : ë™ë“± í´ë˜ìŠ¤ ë³€í™˜
![](./images/eclat.png)  
<br>
chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.philippe-fournier-viger.com/COURSES/Pattern_mining/Eclat.pdf
<br>
ë…¸ë€ìƒ‰ ë¹ˆë°œ ì§‘í•© : ì‚¬ì „ ì •ì˜ëœ ìµœì†Œ ì§€ì§€ë„(minimum support) ì´ìƒì˜ ì§€ì§€ë„ë¥¼ ê°€ì§€ëŠ” í•­ëª©ì˜ ì¡°í•©<br><br>
â–£ ì •ì˜: Aprioriì™€ FP-Growthì˜ ëŒ€ì•ˆìœ¼ë¡œ, íŠ¸ëœì­ì…˜ ê°„ì˜ ê³µí†µí•­ëª©(êµì§‘í•©)ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆë°œí•­ëª©ì„ ì¶”ì¶œí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ì•„ë„ íŠ¸ëœì­ì…˜ ê°„ êµì°¨ ê³„ì‚°ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ì„ ë„ì¶œ<br>
â–£ ì¥ì  : ìˆ˜í‰ì  ë°ì´í„° êµ¬ì¡°ë¥¼ ì´ìš©í•˜ì—¬ íŠ¸ëœì­ì…˜ ë°ì´í„°ì—ì„œ ë¹ˆë°œ í•­ëª© ì§‘í•©ì„ ë¹ ë¥´ê²Œ ì°¾ê³ , ì €ì¥ ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, êµì°¨ ì—°ì‚°ì„ í†µí•´ ë¹ˆë°œ í•­ëª©ì„ ì¶”ì¶œ<br>
â–£ ë‹¨ì  : íŠ¸ëœì­ì…˜ ID ì§‘í•©ì„ ê³„ì† ì—…ë°ì´íŠ¸í•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ì¦ê°€í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œëŠ” íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼ : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ë¹ˆë°œ íŒ¨í„´ ë¶„ì„, ì›¹ í´ë¦­ ë¡œê·¸ ë¶„ì„, í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© ë¶„ì„<br>
â–£ ëª¨ë¸ì‹ : í•­ëª© ì§‘í•©ì˜ ì§€ì§€ë„ ê³„ì‚°ì„ ìœ„í•´ íŠ¸ëœì­ì…˜ ID ì§‘í•©ì˜ êµì§‘í•©ì„ ì‚¬ìš©í•˜ë©° ë¹ˆë°œí•­ëª© ì§‘í•©ì˜ ì§€ì§€ë„ë¥¼ ê³„ì‚°í•  ë•Œ êµì§‘í•©ì„ í†µí•´ ë¹ˆë°œ í•­ëª©ì„ ì°¾ì•„ë‚¸ë‹¤<br>

	import pandas as pd
	import matplotlib.pyplot as plt
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='purple')
	plt.title('Item Frequency (Eclat)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# Eclat ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
	def eclat(data, min_support=0.4):
	    # í•­ëª©ë³„ ì§€ì§€ë„ ê³„ì‚°
	    itemsets = {}
	    for col in data.columns:
	        support = data[col].sum() / len(data)
	        if support >= min_support:
	            itemsets[frozenset([col])] = support
	    
	    # ë‘ ê°œ ì´ìƒì˜ í•­ëª© ì§‘í•©ì— ëŒ€í•´ ì§€ì§€ë„ ê³„ì‚°
	    for length in range(2, len(data.columns) + 1):
	        for comb in combinations(data.columns, length):
	            comb_set = frozenset(comb)
	            support = (data[list(comb)].sum(axis=1) == length).mean()
	            if support >= min_support:
	                itemsets[comb_set] = support
	
	    return itemsets
	
	# Eclat ì•Œê³ ë¦¬ì¦˜ ì ìš©í•˜ì—¬ ë¹ˆë°œ í•­ëª© ì§‘í•© ìƒì„±
	frequent_itemsets = eclat(df, min_support=0.4)
	
	# ë¹ˆë°œ í•­ëª© ì§‘í•©ì—ì„œ ì—°ê´€ ê·œì¹™ ê³„ì‚°
	rules = []
	for itemset, support in frequent_itemsets.items():
	    if len(itemset) > 1:
	        for antecedent in combinations(itemset, len(itemset) - 1):
	            antecedent = frozenset(antecedent)
	            consequent = itemset - antecedent
	            
	            # ì‹ ë¢°ë„ ê³„ì‚°
	            antecedent_support = frequent_itemsets[antecedent]
	            confidence = support / antecedent_support
	            
	            # í–¥ìƒë„ ê³„ì‚°
	            consequent_support = frequent_itemsets[consequent]
	            lift = confidence / consequent_support
	            
	            # ê·œì¹™ ì €ì¥
	            rules.append({
	                'antecedents': antecedent,
	                'consequents': consequent,
	                'support': support,
	                'confidence': confidence,
	                'lift': lift
	            })
	
	# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
	rules_df = pd.DataFrame(rules)
	print("Association Rules (Eclat):")
	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
    
![](./images/1-3.png)
<br>

# [AR-4] Multi-level Association Rules : ë‹¤ê³„ì¸µ ì—°ê´€ê·œì¹™
â–£ ì •ì˜: Aprioriì™€ FP-Growth í™•ì¥ë²„ì „ìœ¼ë¡œ ì—°ê´€ ê·œì¹™ì„ ê³„ì¸µì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë‹¤ì¤‘ ìˆ˜ì¤€ì—ì„œ ê·œì¹™ì„ ìƒì„±í•˜ëŠ” ë°©ì‹<br>
â–£ í•„ìš”ì„±: ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°ì— ì í•©<br>
â–£ ì¥ì : ë” ì •êµí•œ ê·œì¹™ì„ ìƒì„±<br>
â–£ ë‹¨ì : ë³µì¡ì„±ì´ ì¦ê°€í•˜ë©°, í•´ì„ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼: ì „ììƒê±°ë˜, ì¶”ì²œ ì‹œìŠ¤í…œ, ë§ˆì¼€íŒ… ë¶„ì„<br>

	import pandas as pd
	from mlxtend.frequent_patterns import apriori
	import matplotlib.pyplot as plt
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„± (Multi-level êµ¬ì¡°)
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'Dairy_Milk': [1, 1, 0, 1, 0],
	    'Bakery_Bread': [1, 1, 1, 0, 1],
	    'Bakery_Butter': [0, 1, 1, 1, 1]
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ìƒìœ„ ê³„ì¸µ ë°ì´í„° ìƒì„±
	df['Dairy'] = df['Dairy_Milk']
	df['Bakery'] = df[['Bakery_Bread', 'Bakery_Butter']].max(axis=1)
	
	# ì›ë³¸ ë°ì´í„° ì‹œê°í™”
	item_counts = df[['Dairy_Milk', 'Bakery_Bread', 'Bakery_Butter']].sum()
	item_counts.plot(kind='bar', color='purple')
	plt.title('Item Frequency (Multi-level Association Rules)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# ìƒìœ„ ê³„ì¸µì—ì„œ apriori ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets_upper = apriori(df[['Dairy', 'Bakery']], min_support=0.4, use_colnames=True)
	frequent_itemsets_upper['length'] = frequent_itemsets_upper['itemsets'].apply(lambda x: len(x))
	
	# í•˜ìœ„ ê³„ì¸µì—ì„œ apriori ì•Œê³ ë¦¬ì¦˜ ì ìš©
	frequent_itemsets_lower = apriori(df[['Dairy_Milk', 'Bakery_Bread', 'Bakery_Butter']], min_support=0.4, use_colnames=True)
	frequent_itemsets_lower['length'] = frequent_itemsets_lower['itemsets'].apply(lambda x: len(x))
	
	# ì—°ê´€ ê·œì¹™ ìˆ˜ë™ ê³„ì‚°
	def generate_rules(frequent_itemsets):
	    rules = []
	    for itemset in frequent_itemsets['itemsets']:
	        if len(itemset) > 1:
	            for antecedent in combinations(itemset, len(itemset) - 1):
	                antecedent = frozenset(antecedent)
	                consequent = itemset - antecedent
	                
	                # ì§€ì§€ë„ ê³„ì‚°
	                support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	                
	                # ì‹ ë¢°ë„ ê³„ì‚°
	                antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	                confidence = support / antecedent_support
	                
	                # í–¥ìƒë„ ê³„ì‚°
	                consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	                lift = confidence / consequent_support
	                
	                # ê·œì¹™ ì €ì¥
	                rules.append({
	                    'antecedents': antecedent,
	                    'consequents': consequent,
	                    'support': support,
	                    'confidence': confidence,
	                    'lift': lift
	                })
	    return rules
	
	# ìƒìœ„ ê³„ì¸µ ì—°ê´€ ê·œì¹™ ìƒì„±
	rules_upper = generate_rules(frequent_itemsets_upper)
	rules_df_upper = pd.DataFrame(rules_upper)
	print("Association Rules (Upper Level):")
	print(rules_df_upper[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
	
	# í•˜ìœ„ ê³„ì¸µ ì—°ê´€ ê·œì¹™ ìƒì„±
	rules_lower = generate_rules(frequent_itemsets_lower)
	rules_df_lower = pd.DataFrame(rules_lower)
	print("\nAssociation Rules (Lower Level):")
	print(rules_df_lower[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

![](./images/1-4.png)
<br>

# [AR-5] Multi-dimensional Association Rules : ë‹¤ê³„ì¸µ ì—°ê´€ê·œì¹™
â–£ ì •ì˜: ì—¬ëŸ¬ ì†ì„±ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ì°¨ì›ì˜ ê·œì¹™ì„ ìƒì„±<br>
â–£ í•„ìš”ì„±: ì—°ê´€ ê·œì¹™ì„ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì³ ë¶„ì„í•˜ê³ ì í•  ë•Œ ìœ ìš©í•˜ë©°, íŠ¹ì • ì§‘ë‹¨ì— ëŒ€í•œ íŠ¹ì • íŒ¨í„´ì„ íƒì§€í•˜ëŠ” ë° ì í•©<br>
â–£ ì¥ì : ê·œì¹™ì˜ ë²”ìœ„ë¥¼ í™•ì¥í•  ìˆ˜ ìˆì–´ ë” ì„¸ë°€í•œ ê·œì¹™ ë„ì¶œ ê°€ëŠ¥.<br>
â–£ ë‹¨ì : ë³µì¡ì„±ê³¼ í•´ì„ì˜ ì–´ë ¤ì›€<br>
â–£ ì‘ìš©ë¶„ì•¼: ì‚¬ìš©ì ì†ì„± ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ, ë§ˆì¼€íŒ… ì¸í…”ë¦¬ì „ìŠ¤<br>

	import pandas as pd
	from mlxtend.frequent_patterns import apriori
	import matplotlib.pyplot as plt
	from itertools import combinations
	
	# ë°ì´í„°ì…‹ ìƒì„± (Multi-dimensional êµ¬ì¡°)
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	    'Gender_Male': [1, 0, 0, 1, 1],
	    'Gender_Female': [0, 1, 1, 0, 0],
	    'Category_Dairy': [1, 1, 0, 1, 0],
	    'Category_Bakery': [1, 1, 1, 0, 1]
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ì›ë³¸ ë°ì´í„° ì‹œê°í™”
	item_counts = df[['milk', 'bread', 'butter']].sum()
	item_counts.plot(kind='bar', color='orange')
	plt.title('Item Frequency (Multi-dimensional Association Rules)')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# apriori ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¹ˆë°œ í•­ëª© ì§‘í•© ìƒì„±
	frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)
	frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
	
	# ì—°ê´€ ê·œì¹™ ìˆ˜ë™ ê³„ì‚°
	def generate_rules(frequent_itemsets):
	    rules = []
	    for itemset in frequent_itemsets['itemsets']:
	        if len(itemset) > 1:
	            for antecedent in combinations(itemset, len(itemset) - 1):
	                antecedent = frozenset(antecedent)
	                consequent = itemset - antecedent
	                
	                # ì§€ì§€ë„ ê³„ì‚°
	                support = frequent_itemsets[frequent_itemsets['itemsets'] == itemset]['support'].values[0]
	                
	                # ì‹ ë¢°ë„ ê³„ì‚°
	                antecedent_support = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]
	                confidence = support / antecedent_support
	                
	                # í–¥ìƒë„ ê³„ì‚°
	                consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == consequent]['support'].values[0]
	                lift = confidence / consequent_support
	                
	                # ê·œì¹™ ì €ì¥
	                rules.append({
	                    'antecedents': antecedent,
	                    'consequents': consequent,
	                    'support': support,
	                    'confidence': confidence,
	                    'lift': lift
	                })
	    return rules
	
	# Multi-dimensional ì—°ê´€ ê·œì¹™ ìƒì„±
	rules = generate_rules(frequent_itemsets)
	rules_df = pd.DataFrame(rules)
	print("Association Rules (Multi-dimensional):")
	print(rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
	
![](./images/1-5.png)
<br>

# [AR-6] AIS(Artificial Immune System) : ì¸ê³µë©´ì—­ì‹œìŠ¤í…œ
â–£ ì •ì˜: ê±°ë˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë¹ˆë²ˆí•œ í•­ëª© ì§‘í•©ì„ ì°¾ëŠ” ì´ˆê¸° ì—°ê´€ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜<br>
â–£ í•„ìš”ì„±: ì´ˆê¸° ì—°ê´€ ê·œì¹™ ì—°êµ¬ì—ì„œ í™œìš©ë˜ì—ˆìœ¼ë‚˜, ì„±ëŠ¥ì˜ í•œê³„ë¡œ í˜„ì¬ëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ<br>
â–£ ì¥ì : ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì´í•´í•˜ê¸° ì‰½ê³ , ë³µì¡í•œ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ì´ìƒ íŒ¨í„´ì„ ê°ì§€í•˜ëŠ” ë° ê°•ì <br>
â–£ ë‹¨ì : ë¹„íš¨ìœ¨ì ì´ë©°, Apriorië³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§<br>
â–£ ì‘ìš©ë¶„ì•¼: ì´ˆê¸° ì—°ê´€ ê·œì¹™ ì—°êµ¬, ì´ìƒíƒì§€<br>

	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt
	
	# ë°ì´í„°ì…‹ ìƒì„±
	data = {
	    'TID': [1, 2, 3, 4, 5],
	    'milk': [1, 1, 0, 1, 0],
	    'bread': [1, 1, 1, 0, 1],
	    'butter': [0, 1, 1, 1, 1],
	}
	df = pd.DataFrame(data).set_index('TID')
	
	# ë°ì´í„° ì‹œê°í™”
	item_counts = df.sum()
	item_counts.plot(kind='bar', color='blue')
	plt.title('Item Frequency')
	plt.xlabel('Items')
	plt.ylabel('Frequency')
	plt.show()
	
	# AIS ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
	population_size = 10         # ì´ˆê¸° í•­ì²´(í•´) ê°œìˆ˜
	num_generations = 10         # ë°˜ë³µí•  ì„¸ëŒ€ ìˆ˜
	mutation_rate = 0.1          # ëŒì—°ë³€ì´ìœ¨
	selection_rate = 0.5         # ì„ íƒë¥ 
	
	# ì í•©ë„ í•¨ìˆ˜ ì •ì˜ (ì˜ˆ: milk, bread, butter êµ¬ë§¤ ì¡°í•©ì˜ ì ìˆ˜í™”)
	def fitness(antibody):
	    # í•­ì²´ì˜ ì í•©ë„ë¥¼ milk, bread, butterì˜ í•©ìœ¼ë¡œ ì •ì˜
	    return antibody['milk'] * 1 + antibody['bread'] * 1.2 + antibody['butter'] * 0.8
	
	# ì´ˆê¸° í•­ì²´(í•´) ìƒì„± (milk, bread, butter êµ¬ë§¤ ìœ ë¬´ì— ë”°ë¼ í•­ì²´ êµ¬ì„±)
	population = [df.sample(1, replace=True).squeeze() for _ in range(population_size)]
	fitness_scores = np.array([fitness(antibody) for antibody in population])
	
	# AIS ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
	for generation in range(num_generations):
	    # ì„ íƒ: ìƒìœ„ selection_rate ë¹„ìœ¨ì˜ í•­ì²´ë§Œ ìœ ì§€
	    num_selected = int(selection_rate * population_size)
	    selected_indices = np.argsort(fitness_scores)[-num_selected:]
	    selected_population = [population[i] for i in selected_indices]
	    
	    # ë³µì œ ë° ëŒì—°ë³€ì´
	    offspring = []
	    for antibody in selected_population:
	        # ë³µì œ
	        cloned = antibody.copy()
	        # ëŒì—°ë³€ì´ ì ìš©
	        for item in ['milk', 'bread', 'butter']:
	            if np.random.rand() < mutation_rate:
	                cloned[item] = 1 - cloned[item]  # 0ì´ë©´ 1ë¡œ, 1ì´ë©´ 0ìœ¼ë¡œ ë³€ê²½
	        offspring.append(cloned)
	    
	    # ìƒˆ ì„¸ëŒ€ ìƒì„±
	    population = offspring
	    fitness_scores = np.array([fitness(antibody) for antibody in population])
	
	# ìµœì ì˜ í•­ì²´ ì„ íƒ
	best_solution = population[np.argmax(fitness_scores)]
	best_fitness = fitness(best_solution)
	
	# ê²°ê³¼ ì¶œë ¥
	print("ìµœì ì˜ í•´:", best_solution)
	print("ìµœì ì˜ ì í•©ë„:", best_fitness)
	
	# í‰ê°€ ê²°ê³¼ (ìœ ì‚¬ ì§€ì§€ë„, ì‹ ë¢°ë„, í–¥ìƒë„)
	support = sum(best_solution) / len(best_solution)
	confidence = best_fitness / max(fitness_scores)
	lift = confidence / (support if support != 0 else 1)
	
	print("\ní‰ê°€ ê²°ê³¼:")
	print(f"ì§€ì§€ë„(Support): {support}")
	print(f"ì‹ ë¢°ë„(Confidence): {confidence}")
	print(f"í–¥ìƒë„(Lift): {lift}")
	
	# ìµœì¢… í•­ì²´ ì í•©ë„ ë¶„í¬ ì‹œê°í™”
	plt.plot(range(len(fitness_scores)), fitness_scores, 'bo')
	plt.xlabel("Antibody Index")
	plt.ylabel("Fitness Score")
	plt.title("AIS Antibody Fitness Distribution")
	plt.show()

![](./images/1-6.png)
<br>


# ì—°ê´€ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì‹ ìš”ì•½

| êµ¬ë¶„ | ì•Œê³ ë¦¬ì¦˜ | í•µì‹¬ ì•„ì´ë””ì–´ | ì£¼ìš” ìˆ˜í•™ì‹ (ëª¨ë¸ í•¨ìˆ˜ì‹ ì—­í• ) | ëª©ì í•¨ìˆ˜ / í‰ê°€í•¨ìˆ˜ |
|------|-----------|----------------|----------------------------------|----------------------|
| **[AR-1]** | **Apriori**<br>(ì„ í—˜ì  ì•Œê³ ë¦¬ì¦˜) | ë‹¨ì¡°ì„±(Apriori Property)ì„ ì´ìš©í•´ ë¹ˆë°œ í•­ëª©ì§‘í•© íƒìƒ‰ | $Support(X)=\frac{count(X)}{N}$<br>$Confidence(X\Rightarrow Y)=\frac{Support(X\cup Y)}{Support(X)}$<br>$Lift(X\Rightarrow Y)=\frac{Support(X\cup Y)}{Support(X)\,Support(Y)}$ | $\max Support(X)$, $Confidence(X\Rightarrow Y)\ge min\_conf$ |
| **[AR-2]** | **FP-Growth**<br>(Frequent Pattern Growth) | FP-Treeë¡œ íŠ¸ëœì­ì…˜ì„ ì••ì¶• ì €ì¥ í›„ ì¡°ê±´ë¶€ íŒ¨í„´ í™•ì¥ | $FP=\{(I,Support(I))\mid Support(I)\ge min\_sup\}$<br>$FP(I)=\bigcup\_{i\in I}FP(\mathrm{CondBase}(i))$ | ì¡°ê±´ë¶€ íŒ¨í„´ë² ì´ìŠ¤ë¥¼ í†µí•´ $Support\ge min\_sup$ ì¸ ë¹ˆë°œíŒ¨í„´ íš¨ìœ¨ì  íƒìƒ‰ |
| **[AR-3]** | **Eclat**<br>(Equivalence Class Transformation) | TID ì§‘í•©ì˜ êµì§‘í•©ìœ¼ë¡œ Support ê³„ì‚° | $Support(X)=\left|\bigcap\_{i\in X}T(i)\right|$<br>(ì—¬ê¸°ì„œ $T(i)$ëŠ” í•­ëª© $i$ê°€ í¬í•¨ëœ íŠ¸ëœì­ì…˜ ID ì§‘í•©) | $\displaystyle\max\_X\left|\bigcap\_{i\in X}T(i)\right|\quad\text{s.t.}\quad\left|\bigcap\_{i\in X}T(i)\right|\ge min\_sup$ |
| **[AR-4]** | **Multi-level Association Rules**<br>(ë‹¤ê³„ì¸µ ì—°ê´€ê·œì¹™) | ë°ì´í„°ì˜ ê³„ì¸µ(Level) êµ¬ì¡° ë°˜ì˜ | $Support\_l(X)=\frac{count\_l(X)}{N\_l}$<br>$Confidence\_l(X\Rightarrow Y)=\frac{Support\_l(X\cup Y)}{Support\_l(X)}$<br>$min\_sup\_1>min\_sup\_2>min\_sup\_3$ | ê³„ì¸µ ìˆ˜ì¤€ë³„ë¡œ ë‹¤ë¥¸ $min\_support$ ì¡°ê±´ ë§Œì¡± íƒìƒ‰ |
| **[AR-5]** | **Multi-dimensional Association Rules**<br>(ë‹¤ì°¨ì› ì—°ê´€ê·œì¹™) | ë‹¤ì¤‘ ì†ì„±(Attribute) ê°„ ì—°ê´€ ë¶„ì„ | $Support(A\_1=a\_1,\dots,A\_k=a\_k)=\frac{count(A\_1=a\_1,\dots,A\_k=a\_k)}{N}$<br>$Confidence((A\_1=a\_1,\dots,A\_i=a\_i)\Rightarrow(A\_j=a\_j))=\frac{Support(A\_1=a\_1,\dots,A\_i=a\_i,A\_j=a\_j)}{Support(A\_1=a\_1,\dots,A\_i=a\_i)}$ | ë‹¤ì°¨ì› ì†ì„± ì¡°í•© ê°„ ì—°ê´€ê·œì¹™ ë„ì¶œ |
| **[AR-6]** | **Artificial Immune System (AIS)**<br>(ì¸ê³µë©´ì—­ì‹œìŠ¤í…œ) | í•­ì›â€“í•­ì²´ ì¹œí™”ë„(Affinity) ê¸°ë°˜ì˜ ì§„í™” íƒìƒ‰ | $Affinity(Ab,Ag)=\frac{match(Ab,Ag)}{|Ag|}$<br>$P(Ab\_i)=\frac{Affinity(Ab\_i,Ag)}{\sum\_jAffinity(Ab\_j,Ag)}$<br>$P\_{clone}=\alpha\cdot Affinity(Ab\_i,Ag)$<br>$P\_{mutation}=e^{-\beta\cdot Affinity(Ab\_i,Ag)}$ | $\max\_{Ab}Affinity(Ab,Ag)$ subject to $P\_{clone}$, $P\_{mutation}$ ì¡°ê±´ ë§Œì¡± |


<br>

# ğŸ“˜ ì—°ê´€ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì‹ ìš”ì•½ (GitHub ìˆ˜ì‹ ì´ë¯¸ì§€ ìµœì¢…ë³¸)

| êµ¬ë¶„ | ì•Œê³ ë¦¬ì¦˜ | í•µì‹¬ ì•„ì´ë””ì–´ | ì£¼ìš” ìˆ˜í•™ì‹ (ì´ë¯¸ì§€ ë Œë”ë§) | ëª©ì í•¨ìˆ˜ / í‰ê°€í•¨ìˆ˜ (ì´ë¯¸ì§€ ë Œë”ë§) |
|------|-----------|----------------|-------------------------------|----------------------|
| **[AR-1]** | **Apriori** (ì„ í—˜ì  ì•Œê³ ë¦¬ì¦˜) | ë‹¨ì¡°ì„±(Apriori Property)ì„ ì´ìš©í•´ ë¹ˆë°œ í•­ëª©ì§‘í•© íƒìƒ‰ | ![](https://latex.codecogs.com/png.image?\dpi{140}Support(X)=\frac{count(X)}{N})<br>![](https://latex.codecogs.com/png.image?\dpi{140}Confidence(X\Rightarrow%20Y)=\frac{Support(X\cup%20Y)}{Support(X)})<br>![](https://latex.codecogs.com/png.image?\dpi{140}Lift(X\Rightarrow%20Y)=\frac{Support(X\cup%20Y)}{Support(X)\cdot%20Support(Y)}) | ![](https://latex.codecogs.com/png.image?\dpi{140}\max%20Support(X),\;Confidence(X\Rightarrow%20Y)\ge%20min\_conf) |
| **[AR-2]** | **FP-Growth** (Frequent Pattern Growth) | FP-Treeë¡œ íŠ¸ëœì­ì…˜ì„ ì••ì¶• ì €ì¥ í›„ ì¡°ê±´ë¶€ íŒ¨í„´ í™•ì¥ | ![](https://latex.codecogs.com/png.image?\dpi{140}FP=\{(I,Support(I))\mid%20Support(I)\ge%20min\_sup\})<br>![](https://latex.codecogs.com/png.image?\dpi{140}FP(I)=\bigcup_{i\in%20I}FP(\mathrm{CondBase}(i))) | ![](https://latex.codecogs.com/png.image?\dpi{140}Support\ge%20min\_sup\;\text{patterns}) |
| **[AR-3]** | **Eclat** (Equivalence Class Transformation) | TID ì§‘í•©ì˜ êµì§‘í•©ìœ¼ë¡œ Support ê³„ì‚° | ![](https://latex.codecogs.com/png.image?\dpi{140}Support(X)=\left%7C\bigcap_{i\in%20X}T(i)\right%7C) | ![](https://latex.codecogs.com/png.image?\dpi{140}\max_{X}\left%7C\bigcap_{i\in%20X}T(i)\right%7C\;\text{s.t.}\;\left%7C\bigcap_{i\in%20X}T(i)\right%7C\ge%20min\_sup) |
| **[AR-4]** | **Multi-level Association Rules** (ë‹¤ê³„ì¸µ ì—°ê´€ê·œì¹™) | ë°ì´í„°ì˜ ê³„ì¸µ(Level) êµ¬ì¡° ë°˜ì˜ | ![](https://latex.codecogs.com/png.image?\dpi{140}Support\_l(X)=\frac{count\_l(X)}{N\_l})<br>![](https://latex.codecogs.com/png.image?\dpi{140}Confidence\_l(X\Rightarrow%20Y)=\frac{Support\_l(X\cup%20Y)}{Support\_l(X)})<br>![](https://latex.codecogs.com/png.image?\dpi{140}min\_sup\_1>min\_sup\_2>min\_sup\_3) | ![](https://latex.codecogs.com/png.image?\dpi{140}\text{Level-wise thresholds satisfied}) |
| **[AR-5]** | **Multi-dimensional Association Rules** (ë‹¤ì°¨ì› ì—°ê´€ê·œì¹™) | ë‹¤ì¤‘ ì†ì„±(Attribute) ê°„ ì—°ê´€ ë¶„ì„ | ![](https://latex.codecogs.com/png.image?\dpi{140}Support(A\_1=a\_1,\dots,A\_k=a\_k)=\frac{count(A\_1=a\_1,\dots,A\_k=a\_k)}{N})<br>![](https://latex.codecogs.com/png.image?\dpi{140}Confidence((A\_1=a\_1,\dots,A\_i=a\_i)\Rightarrow(A\_j=a\_j))=\frac{Support(A\_1=a\_1,\dots,A\_i=a\_i,A\_j=a\_j)}{Support(A\_1=a\_1,\dots,A\_i=a\_i)}) | ![](https://latex.codecogs.com/png.image?\dpi{140}\text{Rules across attribute combinations}) |
| **[AR-6]** | **Artificial Immune System (AIS)** (ì¸ê³µë©´ì—­ì‹œìŠ¤í…œ) | í•­ì›â€“í•­ì²´ ì¹œí™”ë„(Affinity) ê¸°ë°˜ì˜ ì§„í™” íƒìƒ‰ | ![](https://latex.codecogs.com/png.image?\dpi{140}Affinity(Ab,Ag)=\frac{match(Ab,Ag)}{%7CAg%7C})<br>![](https://latex.codecogs.com/png.image?\dpi{140}P(Ab\_i)=\frac{Affinity(Ab\_i,Ag)}{\sum_{j}Affinity(Ab\_j,Ag)})<br>![](https://latex.codecogs.com/png.image?\dpi{140}P\_{clone}=\alpha\cdot%20Affinity(Ab\_i,Ag))<br>![](https://latex.codecogs.com/png.image?\dpi{140}P\_{mutation}=e^{-\beta\cdot%20Affinity(Ab\_i,Ag)}) | ![](https://latex.codecogs.com/png.image?\dpi{140}\max_{Ab}Affinity(Ab,Ag)) |



---
## [ì—°ê´€ ê·œì¹™ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]

**â–£ ì§€ì§€ë„(Support):** íŠ¹ì • í•­ëª© ì§‘í•©ì´ ì „ì²´ ê±°ë˜ì—ì„œ ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.<br>
Support(A) = (ê±°ë˜ì—ì„œ Aê°€ ë°œìƒí•œ íšŸìˆ˜)/(ì „ì²´ ê±°ë˜ ìˆ˜)<br>

**â–£ ì‹ ë¢°ë„(Confidence):** Aê°€ ì£¼ì–´ì¡Œì„ ë•Œ Bê°€ ë°œìƒí•  í™•ë¥ <br>
Confidence(A â‡’ B) = Support(A âˆ© B)/Support(A)<br>

**â–£ í–¥ìƒë„(Lift):** Aì™€ Bê°€ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê²½ìš°ì— ë¹„í•´ Aê°€ ë°œìƒí–ˆì„ ë•Œ Bê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 1ì´ë©´ ë‘ í•­ëª©ì´ ë…ë¦½ì , 1ë³´ë‹¤ í¬ë©´ ì–‘ì˜ ìƒê´€ê´€ê³„, 1ë³´ë‹¤ ì‘ìœ¼ë©´ ìŒì˜ ìƒê´€ê´€ê³„<br>
Lift(A â‡’ B) = Confidence(A â‡’ B)/Support(B)<br>

**â–£ ë ˆë²„ë¦¬ì§€(Leverage):** Aì™€ Bì˜ ê²°í•© ë¹ˆë„ê°€ ë‘ í•­ëª©ì´ ë…ë¦½ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¹ˆë„ì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤. 0ì´ë©´ ë‘ í•­ëª©ì´ ë…ë¦½ì <br>
Leverage(A â‡’ B) =  Support(A âˆ© B) - (Support(A) Ã— Support(B))<br>

**â–£ Conviction(í™•ì‹ ë„):** Aê°€ ë°œìƒí•  ë•Œ Bê°€ ë°œìƒí•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë…ë¦½ì ì¸ ê²½ìš°ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“œëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 1ì— ê°€ê¹Œìš°ë©´ Aì™€ BëŠ” ì„œë¡œ ë…ë¦½ì <br>
Conviction(A â‡’ B) = (1-Support(B))/(1-Confidence(A â‡’ B))<br>

**â–£ ìƒê´€ê³„ìˆ˜(Correlation Coefficient):** 0ì— ê°€ê¹Œìš°ë©´ ë‘ í•­ëª© ê°„ì— ìƒê´€ê´€ê³„ê°€ ì—†ê³ , ì–‘ìˆ˜ë‚˜ ìŒìˆ˜ë¡œ ê°ˆìˆ˜ë¡ ìƒê´€ê´€ê³„ê°€ ê°•í•˜ë‹¤.<br>

<br>

---
**ì°¨ì›ì¶•ì†Œì˜ í•„ìš”ì„± :** ë°ì´í„°ì— í¬í•¨ëœ ë…¸ì´ì¦ˆ(noise)ë¥¼ ì œê±°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ì°¨ì›ì¶•ì†Œ(dimension reduction)ì´ë‹¤. ì°¨ì›ì¶•ì†ŒëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì˜ ì •ë³´ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. ì°¨ì›ì¶•ì†Œë¥¼ í†µí•´ ì°¨ì›ì´ ëŠ˜ì–´ë‚  ìˆ˜ë¡ í•„ìš”í•œ ë°ì´í„°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë§ì•„ì§€ëŠ” ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality) ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ì§€ë„í•™ìŠµì˜ ëŒ€í‘œì ì¸ ì°¨ì›ì¶•ì†Œ ë°©ë²•ì€ ì„ í˜•íŒë³„ë¶„ì„(Linear Discriminant Analysis)ì´ ìˆê³ , ë¹„ì§€ë„í•™ìŠµì˜ ëŒ€í‘œì ì¸ ì°¨ì›ì¶•ì†Œ ë°©ë²•ì€ ì£¼ì„±ë¶„ë¶„ì„(Principal Component Anaysis)ì´ ìˆë‹¤.<br>

# [DR-1] PCA(Principal Component Analysis) : ì£¼ì„±ë¶„ ë¶„ì„
![](./images/PCA_1.png)
<br>
â–£ ì •ì˜ : ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ë°ì´í„°ì˜ ì£¼ìš” ì„±ë¶„(ì£¼ì„±ë¶„)ì„ ì°¾ê¸° ìœ„í•´ ì„ í˜• ë³€í™˜ì„ ì ìš©í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜. ì—¬ëŸ¬ íŠ¹ì„±(Feature) ë³€ìˆ˜ë“¤ì´ í†µê³„ì ìœ¼ë¡œ ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ë„ë¡ ë³€í™˜ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²•. ì£¼ì„±ë¶„ë¶„ì„ì€ ì˜¤ì§ ê³µë¶„ì‚°í–‰ë ¬(convariance matrix) $\sum$ ì—ë§Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.<br> 
â–£ ì¥ì  : ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ, ë°ì´í„°ì˜ ì¡ìŒì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°, ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‰½ê²Œ ì´í•´í•˜ê³  ë¶„ì„<br>
â–£ ë‹¨ì  : ì„ í˜• ë³€í™˜ë§Œì„ ê°€ì •(ì»¤ë„PCA ê°™ì€ ë¹„ì„ í˜• ë³€í˜• ê¸°ë²•ì´ í•„ìš”), ê° ì£¼ì„±ë¶„ì´ ì›ë˜ ë°ì´í„°ì˜ ì–´ë–¤ íŠ¹ì„±ì„ ì„¤ëª…í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ í•´ì„í•˜ê¸° ì–´ë µë‹¤. ë¶„ì‚°ì— ì¤‘ìš”í•œ ì •ë³´ê°€ ìˆì„ ê²½ìš° ì´ë¥¼ ë†“ì¹  ìˆ˜ ìˆë‹¤.<br>
â–£ ì‘ìš©ë¶„ì•¼ : ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2D ë˜ëŠ” 3Dë¡œ ë³€í™˜í•´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”, ì¡ìŒ ì œê±°, ì–¼êµ´ ì¸ì‹ì—ì„œ ì–¼êµ´ ì´ë¯¸ì§€ì˜ ì£¼ìš” íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì–¼êµ´ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë¥˜<br>
â–£ ëª¨ë¸ì‹ : ì£¼ì„±ë¶„ì€ ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°<br>
ë°ì´í„° í–‰ë ¬ ğ‘‹ì˜ ê³µë¶„ì‚° í–‰ë ¬ ğ¶ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ í†µí•´ ìƒˆë¡œìš´ ì£¼ì„±ë¶„ì„ ê³„ì‚° : $C=\frac{1}{n-1}X^TX$<br>
ê³ ìœ ê°’ ë¶„í•´(v_iëŠ” ië²ˆì§¸ ê³ ìœ ë²¡í„°, \lambda_iëŠ” ië²ˆì§¸ ê³ ìœ ê°’) : $Cv_i = \lambda_iv_i$<br>
â–£ PCAì˜ ì ˆì°¨ : ë¶„ì‚°ì˜ ìµœëŒ€í™”: ì£¼ì„±ë¶„ì€ ë°ì´í„°ì˜ ë¶„ì‚°(ë³€ë™ì„±)ì„ ìµœëŒ€í•œ ë§ì´ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ë‹¤. ë°ì´í„°ì˜ ì£¼ìš”í•œ ë³€ë™ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì¶•ì„ ë¨¼ì € ì°¾ê³ , ê·¸ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ íˆ¬ì˜í•œë‹¤. ì§êµì„±: ê° ì£¼ì„±ë¶„ì€ ì„œë¡œ ì§êµ(orthogonal)í•´ì•¼ í•˜ëŠ”ë° ì´ëŠ” ê° ì£¼ì„±ë¶„ì´ ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ëŠ” ë…ë¦½ì ì¸ ì¶•ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.<br>
(1) ë°ì´í„° í‘œì¤€í™” : PCAë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì„ ë§ì¶”ê¸° ìœ„í•´ ê° ë³€ìˆ˜ì˜ í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“¤ê³  ë¶„ì‚°ì„ 1ë¡œ ë§ì¶”ëŠ” z-ì ìˆ˜ ì •ê·œí™” ê³¼ì •<br>
(2) ê³µë¶„ì‚° í–‰ë ¬ê³„ì‚° : ê³µë¶„ì‚°(ë‘ ë³€ìˆ˜ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„) í–‰ë ¬ ê³„ì‚°ì„ í†µí•´ ë°ì´í„°ì˜ ë¶„ì‚°ì´ ì–´ë–»ê²Œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ í™•ì¸<br>
  $Cov(X,Y)=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})$<br>
(3) ê³ ìœ ê°’ ë¶„í•´(Eigenvalue Decomposition) : ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ë²¡í„°(eigenvector)ëŠ” PCAì˜ ì£¼ì„±ë¶„ì— í•´ë‹¹, ê³ ìœ ê°’(eigenvalue)ì€ ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì˜ ì–‘ì„ ë‚˜íƒ€ëƒ„<br>
(4) ì£¼ì„±ë¶„ ì„ íƒ: ê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ì£¼ì„±ë¶„ì„ ì„ íƒ(ê°€ì¥ í° ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ ë²¡í„°ê°€ ì œ1ì£¼ì„±ë¶„, ê·¸ë‹¤ìŒ ê³ ìœ ê°’ì´ ì œ2ì£¼ì„±ë¶„ : ê³ ìœ ê°’ì´ í° ì£¼ì„±ë¶„ì¼ìˆ˜ë¡ ë°ì´í„°ì˜ ë¶„ì‚°ì„¤ëª…ë ¬ì´ ë†’ë‹¤)<br>
(5) ì°¨ì› ì¶•ì†Œ: ì„ íƒëœ ì£¼ì„±ë¶„ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜. ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì„±(ë¶„ì‚°)ì„ ìœ ì§€í•˜ë©´ì„œ ë¶ˆí•„ìš”í•œ ì°¨ì›ì„ ì œê±°í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œ<br>
 
<br>

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # PCA ì ìš©
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data.target)
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.title("PCA on Iris Dataset")
    plt.colorbar()
    plt.show()

    # ë¶„ì‚° ìœ ì§€ìœ¨ ì¶œë ¥
    print("Explained Variance Ratio:", pca.explained_variance_ratio_)
    print("Total Variance Retained:", sum(pca.explained_variance_ratio_))

![](./images/PCA.png)
<br>


# [DR-2] SVD(Singular Value Decomposition) : íŠ¹ì´ê°’ ë¶„í•´
![](./images/SVD_1.png)
<br>
â–£ ì •ì˜: ì„ì˜ì˜ í–‰ë ¬ì„ ì„¸ ê°œì˜ í–‰ë ¬ë¡œ ë¶„í•´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í–‰ë ¬ì˜ íŠ¹ì´ê°’ê³¼ íŠ¹ì´ë²¡í„°ë¥¼ í†µí•´ í–‰ë ¬ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µí•´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì°¾ê±°ë‚˜ ì••ì¶•í•˜ëŠ” ë° ì‚¬ìš©<br> 
â–£ ì¥ì  : ì •ë°©/ë¹„ì •ë°©/ë¹„ëŒ€ì¹­ í–‰ë ¬ ë“± ì–´ë–¤ í˜•íƒœì˜ í–‰ë ¬ì—ë„ ì ìš© ê°€ëŠ¥, ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ë©´ì„œë„ ì¤‘ìš”í•œ íŒ¨í„´ì„ ìœ ì§€, ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¸¸ ìˆ˜ ìˆìŒ<br>
â–£ ë‹¨ì  : íŠ¹íˆ ë§¤ìš° í° í–‰ë ¬ì˜ ê²½ìš° ê³„ì‚°ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë©°, ë¶„í•´ëœ í–‰ë ¬ë“¤ì´ ì›ë³¸ ë°ì´í„°ì™€ ì§ê´€ì ì¸ ê´€ê³„ë¥¼ ê°€ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì— ê²°ê³¼ë¥¼ í•´ì„í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼ : ë‹¨ì–´-ë¬¸ì„œ í–‰ë ¬ì˜ ì°¨ì› ì¶•ì†Œ, ë°ì´í„° ì••ì¶•, ë…¸ì´ì¦ˆ ì œê±°, ì¶”ì²œ ì‹œìŠ¤í…œ, ì´ë¯¸ì§€ ì••ì¶•<br> 
â–£ ëª¨ë¸ì‹ : $X=UÎ£V^T$<br>
ğ‘‹ëŠ” ğ‘šÃ—ğ‘› í¬ê¸°ì˜ ì›ë³¸ í–‰ë ¬, ğ‘ˆëŠ” ğ‘šÃ—ğ‘š í¬ê¸°ì˜ ì¢Œì¸¡ ì§êµ í–‰ë ¬, Î£ëŠ” ğ‘šÃ—ğ‘› í¬ê¸°ì˜ ëŒ€ê° í–‰ë ¬ë¡œ íŠ¹ì´ê°’ì´ ëŒ€ê° ì›ì†Œë¡œ ë°°ì¹˜, $ğ‘‰^ğ‘‡$ëŠ” ğ‘›Ã—ğ‘› í¬ê¸°ì˜ ìš°ì¸¡ ì§êµ í–‰ë ¬<br>

<br>

    import numpy as np
    from sklearn.decomposition import TruncatedSVD
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # SVD ì ìš©
    svd = TruncatedSVD(n_components=2)
    X_svd = svd.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_svd[:, 0], X_svd[:, 1], c=data.target)
    plt.xlabel("SVD Component 1")
    plt.ylabel("SVD Component 2")
    plt.title("SVD on Iris Dataset")
    plt.colorbar()
    plt.show()

    # ë¶„ì‚° ìœ ì§€ìœ¨ ì¶œë ¥
    print("Explained Variance Ratio:", svd.explained_variance_ratio_)
    print("Total Variance Retained:", sum(svd.explained_variance_ratio_))

![](./images/SVD.png)
<br>


# [DR-3] ICA(Independent Component Analysis) : ë…ë¦½ì„±ë¶„ ë¶„ì„
â–£ ì •ì˜ : ë‹¤ë³€ëŸ‰ ì‹ í˜¸ì—ì„œ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ì„±ë¶„ì„ ì¶”ì¶œí•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•. PCAëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ëŠ” ì¶•ì„ ì°¾ëŠ” ë°˜ë©´, ICAëŠ” ì‹ í˜¸ ê°„ì˜ ë…ë¦½ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ë¶„ì„ ì°¾ëŠ”ë‹¤. ë˜í•œ PCAëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ë°ì´í„°ì˜ ìƒê´€ê´€ê³„ë§Œì„ ì´ìš©í•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ê±°ë‚˜ ì„±ë¶„ì„ ì°¾ëŠ” ë°˜ë©´, ICAëŠ” ì‹ í˜¸ë“¤ ê°„ì˜ ê³ ì°¨ì›ì  í†µê³„ì  ë…ë¦½ì„±ì— ì´ˆì ì„ ë§ì¶”ê¸° ë•Œë¬¸ì— ë” ë³µì¡í•œ êµ¬ì¡°ì˜ ì‹ í˜¸ë¶„ë¦¬ ë¬¸ì œë¥¼ í•´ê²°<br>
â–£ í•„ìš”ì„± : ê´€ì¸¡ëœ ì‹ í˜¸ê°€ ì—¬ëŸ¬ ë…ë¦½ì ì¸ ì›ì²œ ì‹ í˜¸ì˜ í˜¼í•©ìœ¼ë¡œ êµ¬ì„±ë  ë•Œ ê° ë…ë¦½ì ì¸ ì‹ í˜¸ë¥¼ ë³µì›í•˜ëŠ” ë° í•„ìš”í•˜ë©° íŠ¹íˆ ì‹ í˜¸ ì²˜ë¦¬ ë° ìŒì„± ë¶„ë¦¬ì— ìœ ìš©<br>
â–£ ì‘ìš©ë¶„ì•¼ : ë‡ŒíŒŒ(EEG) ì‹ í˜¸ ë¶„ì„, ìŒì„± ì‹ í˜¸ ë¶„ë¦¬, ì´ë¯¸ì§€ ì²˜ë¦¬<br>
â–£ ì¥ì  : í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ì‹ í˜¸ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì‹ í˜¸ ì²˜ë¦¬, ì´ë¯¸ì§€ ë¶„í• , ìŒì„± ë¶„ë¦¬ ë“±ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜<br>
â–£ ë‹¨ì  : ì¡ìŒì— ë¯¼ê°í•˜ê³ , ì›ë˜ ì‹ í˜¸ì˜ ìˆœì„œë¥¼ ë³´ì¥í•˜ì§€ ì•Šìœ¼ë©°, ì„±ë¶„ì˜ í¬ê¸°ë„ ì›ë˜ ì‹ í˜¸ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ì¶”ê°€ì ì¸ í›„ì²˜ë¦¬ê°€ í•„ìš”<br>
â–£ ëª¨ë¸ì‹ : ê´€ì¸¡ ë°ì´í„° ğ‘‹=ğ´ğ‘†ì—ì„œ ğ´ëŠ” í˜¼í•© í–‰ë ¬, ğ‘†ëŠ” ë…ë¦½ ì„±ë¶„ í–‰ë ¬ì´ë©°, ğ´ì™€ ğ‘†ë¥¼ ì¶”ì •í•˜ì—¬ ğ‘†ë¥¼ ì¶”ì¶œ<br>
â–£ ì•Œê³ ë¦¬ì¦˜ : ë¹„ì„ í˜•ì„±ì„ ì´ìš©í•´ ë…ë¦½ ì„±ë¶„ì„ ë¹ ë¥´ê²Œ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ ì‹ í˜¸ì˜ ë¹„ì •ê·œì„±ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì„±ë¶„ì„ ì¶”ì •í•˜ëŠ” Fast ICAê³¼ ì •ë³´ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°©ë²•ìœ¼ë¡œ, ê´€ì¸¡ëœ ë°ì´í„°ì—ì„œ ì •ë³´ëŸ‰ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë…ë¦½ ì„±ë¶„ì„ ì¶”ì •í•˜ëŠ” Infomax ICA<br>

<br>

    import numpy as np
    from sklearn.decomposition import FastICA
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # ICA ì ìš©
    ica = FastICA(n_components=2, random_state=42)
    X_ica = ica.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_ica[:, 0], X_ica[:, 1], c=data.target)
    plt.xlabel("ICA Component 1")
    plt.ylabel("ICA Component 2")
    plt.title("ICA on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/ICA.png)
<br> 

# [DR-4] LDA(Linear Discriminant Analysis) : ì„ í˜•íŒë³„ ë¶„ì„(ì§€ë„í•™ìŠµ ê¸°ë°˜)
![](./images/LDA_1.png)
<br>
â–£ ì •ì˜: í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ê³  í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ìœ¼ë¡œ ì£¼ë¡œ ì§€ë„ í•™ìŠµì—ì„œ ì‚¬ìš©<br>
â–£ í•„ìš”ì„±: í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë¶„ë¥˜ ë¬¸ì œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì : í´ë˜ìŠ¤ ë¶„ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ì—¬ ë¶„ë¥˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìœ¼ë©°, ì„ í˜• ë³€í™˜ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œ<br>
â–£ ë‹¨ì : ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ê²½ìš° ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë©°, í´ë˜ìŠ¤ ê°„ ë¶„í¬ê°€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ë” íš¨ê³¼ì <br>
â–£ ì‘ìš©ë¶„ì•¼: ì–¼êµ´ ì¸ì‹, ì´ë¯¸ì§€ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë“±<br>
â–£ ëª¨ë¸ì‹: ë‘ í´ë˜ìŠ¤ ê°„ì˜ ë¶„ì‚° ë¹„ìœ¨ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ íˆ¬ì˜<br>

    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data
    y = data.target

    # LDA ì ìš©
    lda = LinearDiscriminantAnalysis(n_components=2)
    X_lda = lda.fit_transform(X, y)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)
    plt.xlabel("LDA Component 1")
    plt.ylabel("LDA Component 2")
    plt.title("LDA on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/LDA.png)
<br><br>
![](./images/PCA_LDA.png)
<br>
https://nirpyresearch.com/classification-nir-spectra-linear-discriminant-analysis-python/
<br>


# [DR-5] NMF(Non-negative Matrix Factorization) : ë¹„ìŒìˆ˜ í–‰ë ¬ ë¶„í•´
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ ë¹„ìŒìˆ˜ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚´ê³  ì´ë¥¼ ë‘ ê°œì˜ ë¹„ìŒìˆ˜ í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” í–‰ë ¬ ë¶„í•´(Matrix Factorization) ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ì›ë³¸ ë°ì´í„°ë¥¼ ë‘ ê°œì˜ ë¹„ìŒìˆ˜(ì˜ˆ: í”½ì…€ ê°’, ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼, ì‚¬ìš©ì í‰ê°€ ì ìˆ˜ ë“±) í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë¶„í•´í•¨ìœ¼ë¡œì¨ ë¹„ìŒìˆ˜ ë°ì´í„°ë¥¼ ì••ì¶•ì ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ì¤‘ìš”í•œ êµ¬ì¡°ì  íŠ¹ì§•ì„ ë°œê²¬<br>
â–£ ì¥ì  : ëª¨ë“  ìš”ì†Œê°€ ë¹„ìŒìˆ˜ì´ë¯€ë¡œ ê²°ê³¼ë¥¼ ì§ê´€ì ìœ¼ë¡œ í•´ì„, ë°ì´í„°ì˜ ì €ì°¨ì› í‘œí˜„ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ë©°, ê° ë°ì´í„°ì˜ ê¸°ì—¬ ìš”ì†Œë¥¼ ëª…í™•íˆ êµ¬ë¶„<br>
â–£ ë‹¨ì  : ì´ˆê¸°í™” ë¯¼ê°ì„±, ë³µì¡í•œ ë¹„ì„ í˜• ë°ì´í„° í‘œí˜„ì—ëŠ” ë¶€ì í•©, ë¹„ìŒìˆ˜ ì œì•½ìœ¼ë¡œ ì¸í•´ ì œí•œëœ í‘œí˜„ë ¥, ê²°ê³¼ì˜ ë¶ˆí™•ì‹¤ì„±<br>
â–£ ì‘ìš©ë¶„ì•¼ : ì–¼êµ´ ì¸ì‹ì—ì„œ ì´ë¯¸ì§€ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ, í…ìŠ¤íŠ¸ ë§ˆì´ë‹, ìŒì› ë¶„ë¦¬ ë° ì¡ìŒ ì œê±°, ì¶”ì²œ ì‹œìŠ¤í…œ, ìœ ì „ì ë°œí˜„ ë°ì´í„°ì˜ íŠ¹ì§• ì¶”ì¶œ ë° í•´ì„<br>
(ì°¸ê³ ) https://angeloyeo.github.io/2020/10/15/NMF.html<br>

	from sklearn.decomposition import NMF
	import numpy as np
	import matplotlib.pyplot as plt
	
	# 1. ë°ì´í„° ìƒì„± (ì˜ˆ: ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬)
	V = np.array([[1, 2, 3],
	              [4, 5, 6],
	              [7, 8, 9]])
	
	# 2. NMF ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
	model = NMF(n_components=2, init='random', random_state=42)
	W = model.fit_transform(V)
	H = model.components_
	
	# 3. ê·¼ì‚¬ í–‰ë ¬ ê³„ì‚°
	V_approx = np.dot(W, H)
	
	# 4. ì‹œê°í™”
	fig, axs = plt.subplots(2, 2, figsize=(12, 10))
	
	# ì›ë³¸ ë°ì´í„° ì‹œê°í™”
	axs[0, 0].imshow(V, cmap='viridis', aspect='auto')
	axs[0, 0].set_title("ì›ë³¸ í–‰ë ¬ (V)")
	axs[0, 0].set_xticks(range(V.shape[1]))
	axs[0, 0].set_yticks(range(V.shape[0]))
	
	# ê·¼ì‚¬ í–‰ë ¬ ì‹œê°í™”
	axs[0, 1].imshow(V_approx, cmap='viridis', aspect='auto')
	axs[0, 1].set_title("ê·¼ì‚¬ í–‰ë ¬ (V_approx)")
	axs[0, 1].set_xticks(range(V_approx.shape[1]))
	axs[0, 1].set_yticks(range(V_approx.shape[0]))
	
	# ê¸°ì € í–‰ë ¬ (W) ì‹œê°í™”
	axs[1, 0].imshow(W, cmap='viridis', aspect='auto')
	axs[1, 0].set_title("ê¸°ì € í–‰ë ¬ (W)")
	axs[1, 0].set_xticks(range(W.shape[1]))
	axs[1, 0].set_yticks(range(W.shape[0]))
	
	# ê³„ìˆ˜ í–‰ë ¬ (H) ì‹œê°í™”
	axs[1, 1].imshow(H, cmap='viridis', aspect='auto')
	axs[1, 1].set_title("ê³„ìˆ˜ í–‰ë ¬ (H)")
	axs[1, 1].set_xticks(range(H.shape[1]))
	axs[1, 1].set_yticks(range(H.shape[0]))
	
	# ë ˆì´ì•„ì›ƒ ì •ë¦¬
	plt.tight_layout()
	plt.show()
	
	# 5. ì¶œë ¥ ê²°ê³¼
	print("ì›ë³¸ í–‰ë ¬ (V):")
	print(V)
	
	print("\nê¸°ì € í–‰ë ¬ (W):")
	print(W)
	
	print("\nê³„ìˆ˜ í–‰ë ¬ (H):")
	print(H)
	
	print("\nê·¼ì‚¬ í–‰ë ¬ (V_approx):")
	print(V_approx)
	
<br>

	ì›ë³¸ í–‰ë ¬ (V): ì›ë˜ì˜ ë°ì´í„° í–‰ë ¬ë¡œ, NMFë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— ì…ë ¥ëœ ê°’
	[[1 2 3]
 	[4 5 6]
 	[7 8 9]]

	ê¸°ì € í–‰ë ¬ (W): í–‰ë ¬ ğ‘‰ì˜ í–‰(ë°ì´í„° í¬ì¸íŠ¸)ì„ ì €ì°¨ì› ì ì¬ ê³µê°„ì—ì„œ í‘œí˜„
	[[2.41498468 0.        ]
 	[4.83219981 0.36423119]
 	[7.24871414 0.72880911]]

	ê³„ìˆ˜ í–‰ë ¬ (H): ê° ì—´(íŠ¹ì„±)ì„ ì ì¬ ë³€ìˆ˜ì˜ ì¡°í•©ìœ¼ë¡œ í‘œí˜„
	[[0.41443612 0.82883423 1.24166579]
 	[5.48294704 2.73290582 0.        ]]

	ê·¼ì‚¬ í–‰ë ¬ (V_approx): NMFë¥¼ í†µí•´ ì›ë³¸ í–‰ë ¬ ğ‘‰ë¥¼ ê·¼ì‚¬í•œ ê²°ê³¼
	[[1.00085688 2.00162196 2.99860386]
 	[3.99969848 5.00050215 5.99997718]
 	[7.00015069 7.99974905 9.00048035]]
 

$ğ‘‰[0,0]=1, ğ‘‰_{approx}[0,0] = 1.00085688$ : ì˜¤ì°¨ëŠ” ì•½ 0.0009<br>
$ğ‘‰[1,2]=6, ğ‘‰_{approx}[1,2]=5.99997718$ : ì˜¤ì°¨ëŠ” ì•½ 0.00002<br>

![](./images/NMF.PNG)
<br> 
ì›ë³¸ í–‰ë ¬ (ğ‘‰)ì˜ í¬ê¸°: 3Ã—3 â†’ ğ‘š=3, ğ‘›=3 (ë°ì´í„° í¬ì¸íŠ¸ 3ê°œ, íŠ¹ì„± 3ê°œ)<br>
ê¸°ì € í–‰ë ¬ (ğ‘Š)ì˜ í¬ê¸°: 3Ã—2 â†’ ğ‘š=3, ğ‘˜=2 (3ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ 2ê°œì˜ ì ì¬ ìš”ì¸ìœ¼ë¡œ í‘œí˜„)<br>
ê³„ìˆ˜ í–‰ë ¬ (ğ»)ì˜ í¬ê¸°: 2Ã—3 â†’ ğ‘˜=2, ğ‘›=3 (2ê°œì˜ ì ì¬ ìš”ì¸ì„ 3ê°œì˜ íŠ¹ì„±ìœ¼ë¡œ í‘œí˜„)<br>
<br>


# [DR-6] t-SNE(t-distributed Stochastic Neighbor Embedding) : t-ë¶„í¬ í™•ë¥ ì  ì´ì›ƒ ì„ë² ë”©
â–£ ì •ì˜: ê³ ì°¨ì› ë°ì´í„°ì˜ êµ­ì†Œ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ì—¬ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì˜ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ìœ ì§€í•œ ì±„ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë°ì´í„° ê°„ì˜ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì¥ì  : ê³ ì°¨ì› ë°ì´í„°ì˜ êµ°ì§‘ êµ¬ì¡°ë¥¼ ì˜ ë°˜ì˜í•˜ì—¬ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ ì˜ ë“œëŸ¬ë‚´ê³ , ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ê°€ì§„ ë°ì´í„°ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™<br>
â–£ ë‹¨ì  : ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê³„ì‚° ì‹œê°„ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³ , ì´ˆê¸° ë§¤ê°œë³€ìˆ˜(ì˜ˆ: Ïƒ ê°’ ë° í•™ìŠµë¥ )ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘<br> 
â–£ ì‘ìš© ë¶„ì•¼ : ì´ë¯¸ì§€ ë°ì´í„°, í…ìŠ¤íŠ¸ ë°ì´í„°, ìœ ì „ì í‘œí˜„ ë°ì´í„° ë“±ì˜ ì‹œê°í™”, í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„, ë°ì´í„° ì „ì²˜ë¦¬, ì‹ ê²½ë§ ëª¨ë¸ì˜ ì¤‘ê°„ ì¶œë ¥ì„ ì‹œê°í™”<br>
â–£ ëª¨ë¸ì‹: ê³ ì°¨ì› ë°ì´í„°ì˜ ìœ ì‚¬ë„ì™€ ì €ì°¨ì› ë°ì´í„°ì˜ ìœ ì‚¬ë„ ë¶„í¬ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì½”ìŠ¤íŠ¸ í•¨ìˆ˜ ğ¾ğ¿(ğ‘âˆ¥ğ‘)ë¥¼ ìµœì†Œí™”<br>

<br>

    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # t-SNE ì ìš©
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=data.target)
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")
    plt.title("t-SNE on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/tSNE.png)
<br>


# [DR-7] UMAP(Uniform Manifold Approximation and Projection) : ê· ì¼ ë§¤ë‹ˆí´ë“œ ê·¼ì‚¬ì  ì‚¬ì˜
â–£ ì •ì˜: ë°ì´í„°ì˜ êµ­ì†Œ êµ¬ì¡°ì™€ ì „ì—­ êµ¬ì¡°ë¥¼ ë™ì‹œì— ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„±: ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ì—ì„œ ì‹œê°í™”í•˜ë©´ì„œ ë°ì´í„°ì˜ ì „ì²´ì  ë° êµ­ì†Œì  ê´€ê³„ë¥¼ ë™ì‹œì— ë³´ì¡´í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì¥ì : t-SNEë³´ë‹¤ ê³„ì‚°ì´ ë¹ ë¥´ê³ , ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™, ë°ì´í„°ì˜ ì „ì—­ì  ë° êµ­ì†Œì  êµ¬ì¡°ë¥¼ ë™ì‹œì— ë³´ì¡´<br>
â–£ ë‹¨ì : ì¼ë¶€ ë§¤ê°œë³€ìˆ˜ ì¡°ì •ì´ í•„ìš”í•˜ë©°, ê²°ê³¼ê°€ ë§¤ê°œë³€ìˆ˜ì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œê°í™”, ìƒë¬¼ì •ë³´í•™, í…ìŠ¤íŠ¸ ë¶„ì„ ë“±<br>
â–£ ëª¨ë¸ì‹: ì´ë¡ ì ìœ¼ë¡œëŠ” ë¦¬ë§Œ ê±°ë¦¬ì™€ ì´ˆêµ¬ ë©´ì  ê°œë…ì„ ì´ìš©í•˜ì—¬ ë°ì´í„°ì˜ ê·¼ì ‘ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê³ ì°¨ì›ì—ì„œ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜<br>

    !pip install umap-learn
    import umap
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # UMAP ì ìš©
    umap_model = umap.UMAP(n_components=2, random_state=42)
    X_umap = umap_model.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=data.target)
    plt.xlabel("UMAP Component 1")
    plt.ylabel("UMAP Component 2")
    plt.title("UMAP on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/UMAP.png)
<br>


# [DR-8] Isomap : ë“±ê±°ë¦¬ ë§¤í•‘
â–£ ì •ì˜: ë°ì´í„°ì˜ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ì—¬ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•<br>
â–£ í•„ìš”ì„±: ë¹„ì„ í˜•ì ì¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ì €ì°¨ì›ì—ì„œë„ ìœ ì§€í•˜ë©° ì‹œê°í™”í•  ë•Œ ìœ ìš©<br>
â–£ ì¥ì : ê³ ì°¨ì› ë°ì´í„°ì˜ ë§¤ë‹ˆí´ë“œ(ì €ì°¨ì› ë‹¤ì–‘ì²´) êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ë©°, êµ­ì†Œì ì¸ ê±°ë¦¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ìœ ì§€<br>
â–£ ë‹¨ì : ë°ì´í„°ê°€ ê³ ì°¨ì›ì—ì„œ ë§¤ë‹ˆí´ë“œ êµ¬ì¡°ë¥¼ í˜•ì„±í•˜ì§€ ì•ŠëŠ” ê²½ìš° íš¨ê³¼ì ì´ì§€ ì•Šìœ¼ë©°, ê³„ì‚° ë¹„ìš©ì´ ë†’ì•„ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ëŠ” ë¶€ì í•©<br>
â–£ ì‘ìš©ë¶„ì•¼: ì‹œê°í™”, ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„, ìƒë¬¼ì •ë³´í•™<br>
â–£ ëª¨ë¸ì‹: ê·¼ì ‘ ê·¸ë˜í”„ì™€ ë‹¤ì°¨ì› ì²™ë„ë¥¼ ê²°í•©í•˜ì—¬ ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ë³´ì¡´<br>

    from sklearn.manifold import Isomap
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # Isomap ì ìš©
    isomap = Isomap(n_components=2)
    X_isomap = isomap.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=data.target)
    plt.xlabel("Isomap Component 1")
    plt.ylabel("Isomap Component 2")
    plt.title("Isomap on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/ISO.png)
<br>


# [DR-9] MDS(Multidimensional Scaling) : ë‹¤ì°¨ì› ì²™ë„
â–£ ì •ì˜: MDSëŠ” ê³ ì°¨ì› ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ë³´ì¡´í•˜ë©° ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²•<br>
â–£ í•„ìš”ì„±: ë°ì´í„°ì˜ ìœ ì‚¬ì„± ë˜ëŠ” ê±°ë¦¬ ì •ë³´ë¥¼ ì €ì°¨ì›ì—ì„œë„ ìœ ì§€í•˜ì—¬ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì¥ì : ê±°ë¦¬ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë¯€ë¡œ ë°ì´í„°ì˜ ê¸°í•˜í•™ì  ê´€ê³„ë¥¼ ì˜ ìœ ì§€í•˜ë©°, ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ì¼ë¶€ ë³´ì¡´<br>
â–£ ë‹¨ì : ê³„ì‚° ë¹„ìš©ì´ ë†’ê³ , ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ëŠ” ì í•©í•˜ì§€ ì•Šìœ¼ë©°, ì´ˆê¸°í™”ì— ë¯¼ê°í•˜ì—¬ ê²°ê³¼ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼: ì‹¬ë¦¬í•™, ìƒë¬¼ì •ë³´í•™, ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ ë“±<br>
â–£ ëª¨ë¸ì‹: ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ í–‰ë ¬ì„ ìœ ì§€í•˜ë©° ì €ì°¨ì›ì—ì„œ êµ¬ì„±<br>

    from sklearn.manifold import MDS
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data

    # MDS ì ìš©
    mds = MDS(n_components=2, random_state=42)
    X_mds = mds.fit_transform(X)

    # ê²°ê³¼ ì‹œê°í™”
    plt.scatter(X_mds[:, 0], X_mds[:, 1], c=data.target)
    plt.xlabel("MDS Component 1")
    plt.ylabel("MDS Component 2")
    plt.title("MDS on Iris Dataset")
    plt.colorbar()
    plt.show()

![](./images/MDS.png)
<br>


# [DR-10] SOM(Self-Organizing Maps) : ìê¸° ì¡°ì§í™”
â–£ ì •ì˜ : ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›(ì¼ë°˜ì ìœ¼ë¡œ 2ì°¨ì›) ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ëŠ” ë° ì‚¬ìš©. PCAëŠ” ì„ í˜• ë³€í™˜ì„ í†µí•´ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•˜ì§€ë§Œ, SOMì€ ë¹„ì„ í˜• ë³€í™˜ì„ ì‚¬ìš©í•˜ì—¬ ë” ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìœ¼ë©°, k-í‰ê· ì€ ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì„ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ êµ°ì§‘í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°˜ë©´, SOMì€ ë‰´ëŸ°ì´ ê²©ì í˜•íƒœë¡œ ì¡°ì§ë˜ì–´ ìˆì–´ ë” ì§ê´€ì ì¸ ì‹œê°í™”ê°€ ê°€ëŠ¥<br> 
â–£ ì ˆì°¨
(1) ì´ˆê¸°í™”: SOMì˜ ê° ë‰´ëŸ°ì— ì„ì˜ì˜ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ í• ë‹¹(ì´ ê°€ì¤‘ì¹˜ ë²¡í„°ëŠ” ì…ë ¥ ë°ì´í„°ì™€ ê°™ì€ ì°¨ì›)<br>
(2) ì…ë ¥ ë°ì´í„° ì„ íƒ: í•™ìŠµ ê³¼ì •ì—ì„œ ì…ë ¥ ë°ì´í„° ë²¡í„° í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ<br>
(3) ìŠ¹ì ë‰´ëŸ°(BMU, Best Matching Unit) ì°¾ê¸°: SOMì˜ ëª¨ë“  ë‰´ëŸ° ì¤‘ì—ì„œ í˜„ì¬ ì…ë ¥ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ(ê°€ì¤‘ì¹˜ ë²¡í„° ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ì¸¡ì •) ë‰´ëŸ°ì„ ì°¾ëŠ” ê²½ìŸ í•™ìŠµì˜ í•µì‹¬ ë‹¨ê³„<br>
(4) ê°€ì¤‘ì¹˜ ë²¡í„° ê°±ì‹ : ì„ íƒëœ ìŠ¹ì ë‰´ëŸ°ê³¼ ê·¸ ì£¼ë³€ ì´ì›ƒ ë‰´ëŸ°ë“¤ì˜ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ì¡°ì •í•œë‹¤. ì´ë•Œ, ê°€ì¤‘ì¹˜ ë²¡í„°ëŠ” ì…ë ¥ ë°ì´í„°ì— ë” ê°€ê¹ê²Œ ì´ë™<br> 
â–£ ì¥ì  : ë°ì´í„°ì— ëŒ€í•œ ì‚¬ì „ ì •ë³´ê°€ ì—†ì–´ë„ ìœ ìš©í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥, êµ°ì§‘ì˜ ë¶„í¬ë‚˜ ë°ì´í„°ì˜ ê²½í–¥ì„±ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´, ì…ë ¥ ë°ì´í„°ì˜ ì´ì›ƒ ê´€ê³„ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ë¯€ë¡œ ì›ë˜ ë°ì´í„°ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ ìœ ì§€<br> 
â–£ ë‹¨ì  : í•™ìŠµë¥ ê³¼ ì´ì›ƒ í¬ê¸° ë“± ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë¥¼ ì ì ˆíˆ ì„¤ì •í•´ì•¼ í•˜ë©°, ëŒ€ê·œëª¨ ë°ì´í„° í•™ìŠµì— ë¹„íš¨ìœ¨ì , ë³€í™˜ëœ ë§µì„ í•´ì„í•˜ëŠ” ê²ƒì´ PCA ë“±ì˜ ì„ í˜• ë³€í™˜ë³´ë‹¤ ë” ì–´ë ¤ì›€<br> 
â–£ ì‘ìš©ë¶„ì•¼ : ì´ë¯¸ì§€ ë¶„ì„, ë¬¸ì„œ ë¶„ë¥˜, ìŒì„± ì¸ì‹, ìƒë¬¼ì •ë³´í•™<br>
â–£ ëª¨ë¸ì‹ : ë‰´ëŸ°ì˜ ìœ„ì¹˜ ğ‘Ÿì™€ ì…ë ¥ ë²¡í„° ğ‘¥ ê°„ì˜ ê±°ë¦¬ í•¨ìˆ˜ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±(ğœ‚(ğ‘¡)ëŠ” í•™ìŠµë¥ , â„(ğ‘¡)ëŠ” ì´ì›ƒ í•¨ìˆ˜)<br>
$W(t+1)=W(t)+\theta(t)\cdot\eta(t)\cdot(X-W(t))$<br>

    !pip install minisom

    from minisom import MiniSom
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris
    from sklearn.preprocessing import MinMaxScaler

    # ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”
    data = load_iris()
    X = data.data
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    # SOM ì´ˆê¸°í™” ë° í•™ìŠµ
    som = MiniSom(x=10, y=10, input_len=4, sigma=1.0, learning_rate=0.5, random_seed=42)
    som.train_random(X_scaled, 100)  # 100íšŒ ë°˜ë³µ í•™ìŠµ

    # SOM ì‹œê°í™”
    plt.figure(figsize=(10, 10))
    for i, x in enumerate(X_scaled):
        w = som.winner(x)
        plt.text(w[0] + 0.5, w[1] + 0.5, str(data.target[i]),
        color=plt.cm.rainbow(data.target[i] / 2.0),
        fontdict={'weight': 'bold', 'size': 11})

    plt.title("SOM Clustering of Iris Data")
    plt.xlim([0, 10])
    plt.ylim([0, 10])
    plt.grid()
    plt.show()

![](./images/SOM.png)
<br> 



---
## [ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ë°©ë²•]

**â–£ ì¬êµ¬ì„± ì˜¤ë¥˜(Reconstruction Error) :** ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°ë¥¼ ì›ë³¸ ì°¨ì›ìœ¼ë¡œ ë³µì›í•˜ì—¬ ë³µì›ëœ ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„° ê°„ì˜ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)ë¥¼ í†µí•´ ì¬êµ¬ì„± ì˜¤ë¥˜ë¥¼ ê³„ì‚°<br>

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    from sklearn.metrics import mean_squared_error

    # ë°ì´í„° ë¡œë“œ : Iris ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ ì…ë ¥ ë°ì´í„°(X)ë¥¼ ì¤€ë¹„
    data = load_iris()
    X = data.data  # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)  # ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°

    # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚° : ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°ë¥¼ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›í•˜ê³  ì›ë³¸ ë°ì´í„°ì™€ì˜ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)ë¥¼ ê³„ì‚°
    X_reconstructed = pca.inverse_transform(X_reduced)  # ì°¨ì› ì¶•ì†Œ í›„ ë³µì›ëœ ë°ì´í„°
    reconstruction_error = mean_squared_error(X, X_reconstructed)  # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
    print(f"Reconstruction Error (MSE): {reconstruction_error:.3f}")

<br>

**â–£ ë¶„ì‚° ìœ ì§€ìœ¨(Explained Variance Ratio) :** ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨ì„ í†µí•´ ë°ì´í„°ì˜ ì •ë³´ ì†ì‹¤ ì •ë„ë¥¼ íŒŒì•…

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA

    # ë°ì´í„° ë¡œë“œ : Iris ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ ì…ë ¥ ë°ì´í„°(X)ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
    data = load_iris()
    X = data.data  # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=2)
    pca.fit(X)  # PCA í•™ìŠµ

    # ë¶„ì‚° ìœ ì§€ìœ¨ ê³„ì‚° : ê° ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë¹„ìœ¨ë¡œ í™•ì¸
    explained_variance_ratio = pca.explained_variance_ratio_
    print(f"Explained Variance Ratio per Component: {explained_variance_ratio}")
    print(f"Total Variance Retained: {sum(explained_variance_ratio):.3f}")  # ì „ì²´ ë¶„ì‚° ìœ ì§€ìœ¨

<br>

**â–£ ìƒí˜¸ ì •ë³´ëŸ‰(Mutual Information) :** ì°¨ì› ì¶•ì†Œ ì „í›„ ë°ì´í„°ì˜ ì •ë³´ëŸ‰ì„ ë¹„êµ

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    from sklearn.metrics import adjusted_mutual_info_score, normalized_mutual_info_score

    # ë°ì´í„° ë¡œë“œ : Iris ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ ì…ë ¥ ë°ì´í„°(X)ì™€ ì‹¤ì œ ë ˆì´ë¸”(y_true)ë¥¼ ì¤€ë¹„
    data = load_iris()
    X = data.data         # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)
    y_true = data.target  # ì‹¤ì œ ë ˆì´ë¸” (í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ ì‹œ ì‚¬ìš©)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)  # ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°

    # KMeansë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰ : í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜ì™€ ë§ì¶”ê¸°
    kmeans = KMeans(n_clusters=3, random_state=42)
    y_pred = kmeans.fit_predict(X_reduced)  # í´ëŸ¬ìŠ¤í„°ë§ ì˜ˆì¸¡ ë ˆì´ë¸”

    # 4. ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°
    # (1) Adjusted Mutual Information (AMI) : ì‹¤ì œ ë ˆì´ë¸”(y_true)ê³¼ í´ëŸ¬ìŠ¤í„°ë§ ì˜ˆì¸¡ ë ˆì´ë¸”(y_pred) ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •
    ami = adjusted_mutual_info_score(y_true, y_pred)
    print(f"Adjusted Mutual Information (AMI): {ami:.3f}")

    # (2) Normalized Mutual Information (NMI) : ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ê°„ì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ì •ê·œí™”í•˜ì—¬ ì¸¡ì •
    nmi = normalized_mutual_info_score(y_true, y_pred)
    print(f"Normalized Mutual Information (NMI): {nmi:.3f}")

<br>

**â–£ êµ°ì§‘ í‰ê°€ ì§€í‘œ :** ì°¨ì› ì¶•ì†Œ í›„ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ê³  êµ°ì§‘ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ì°¨ì› ì¶•ì†Œì˜ ì„±ëŠ¥ì„ í‰ê°€

    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score
    from sklearn.model_selection import train_test_split

    # ë°ì´í„° ë¡œë“œ
    data = load_iris()
    X = data.data
    y_true = data.target  # ì‹¤ì œ ë ˆì´ë¸” (í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©)

    # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)

    # KMeansë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    kmeans = KMeans(n_clusters=3, random_state=42)
    y_pred = kmeans.fit_predict(X_reduced)

    # êµ°ì§‘ í‰ê°€ ì§€í‘œ ê³„ì‚°
    # (1) Silhouette Score
    silhouette = silhouette_score(X_reduced, y_pred)
    print(f"Silhouette Score: {silhouette:.3f}")

    # (2) Davies-Bouldin Index (DBI) - í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë¦¬ë˜ê³  ì‘ì§‘ë˜ì–´ ìˆëŠ”ì§€ í‰ê°€(DBIê°€ ë‚®ì„ìˆ˜ë¡ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆì´ ë” ì¢‹ìŒ)
    davies_bouldin = davies_bouldin_score(X_reduced, y_pred)
    print(f"Davies-Bouldin Index: {davies_bouldin:.3f}")

    # (3) Adjusted Rand Index (ARI) - ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ(í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì¼ì¹˜ë„ë¥¼ ì¸¡ì •: 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìœ ì‚¬)
    ari = adjusted_rand_score(y_true, y_pred)
    print(f"Adjusted Rand Index (ARI): {ari:.3f}")

    # (4) Normalized Mutual Information (NMI) - ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ(í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì •ë³´ëŸ‰ì˜ ê³µìœ  ì •ë„ë¥¼ ì¸¡ì •: 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìœ ì‚¬)
    nmi = normalized_mutual_info_score(y_true, y_pred)
    print(f"Normalized Mutual Information (NMI): {nmi:.3f}")

<br>

![](./images/s.png)

<br>

---

## [Q&A] t-SNEê°€ ë™ì‹¬ì› ë°ì´í„°ì…‹ì„ ì œëŒ€ë¡œ ë¶„ë¦¬í•˜ì§€ ëª»í•˜ëŠ” ì´ìœ ì™€ í•´ê²° ë°©ì•ˆ
**ì´ìœ ** : t-SNEëŠ” êµ­ì†Œì  êµ¬ì¡°(Local Structure)ë¥¼ ë³´ì¡´í•˜ëŠ” ë° ì§‘ì¤‘í•˜ë¯€ë¡œ ì „ì—­ì  êµ¬ì¡°(Global Structure)ë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ì€ë°, ë™ì‹¬ì› ë°ì´í„°ëŠ” ì „ì—­ì  êµ¬ì¡°(ì›ê³¼ ì› ê°„ì˜ ê±°ë¦¬)ë¥¼ ì˜ ë°˜ì˜í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ë°œìƒ<br>

**í•´ê²°ë°©ì•ˆ**<br>
(1) UMAP ì‚¬ìš©: êµ­ì†Œì  êµ¬ì¡°ì™€ ì „ì—­ì  êµ¬ì¡°ë¥¼ ë™ì‹œì— ë³´ì¡´<br>
(2) t-SNE ë§¤ê°œë³€ìˆ˜ íŠœë‹: Perplexity, í•™ìŠµë¥ , ë°˜ë³µ íšŸìˆ˜ë¥¼ ì¡°ì •<br>
(3) PCAì™€ ê²°í•©: ì „ì—­ì  êµ¬ì¡°ë¥¼ ë¨¼ì € ë°˜ì˜í•œ ë’¤ t-SNE ì ìš©<br>
(4) ë‹¤ë¥¸ ì°¨ì› ì¶•ì†Œ ê¸°ë²•: Kernel PCA, Spectral Embedding ë“± ì‚¬ìš©<br>

	import os
	import matplotlib.pyplot as plt
	from matplotlib import font_manager, rc
	from sklearn.datasets import make_circles
	from sklearn.manifold import TSNE
	from sklearn.decomposition import PCA
	from umap import UMAP
	from sklearn.preprocessing import StandardScaler
	
	# Windows í™˜ê²½ì—ì„œ ì‚¬ìš©í•  í•œê¸€ í°íŠ¸ ì„¤ì •
	font_path = 'C:/Windows/Fonts/malgun.ttf'  # Windowsì˜ 'ë§‘ì€ ê³ ë”•' í°íŠ¸ ê²½ë¡œ
	font_name = font_manager.FontProperties(fname=font_path).get_name()
	rc('font', family=font_name)
	
	# '-' ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
	plt.rcParams['axes.unicode_minus'] = False
	
	# 1. ë°ì´í„° ìƒì„±
	X, y = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)
	
	# 2. ê¸°ë³¸ t-SNE
	tsne_basic = TSNE(n_components=2, random_state=42)
	X_tsne_basic = tsne_basic.fit_transform(X)
	
	# 2-1. UMAP í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ê¸°ì¡´ ì„¤ì • ìµœì í™”)
	umap_optimized = UMAP(n_neighbors=30, min_dist=0.05, n_components=2, random_state=42)
	X_umap_optimized = umap_optimized.fit_transform(X)
	
	# 2-2. UMAP í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ì¡°ì • (í‘œì¤€í™” ë°ì´í„° ì ìš©)
	scaler = StandardScaler()
	X_scaled = scaler.fit_transform(X)  # ë°ì´í„° í‘œì¤€í™”
	umap_revised = UMAP(n_neighbors=20, min_dist=0.1, n_components=2, random_state=42)
	X_umap_revised = umap_revised.fit_transform(X_scaled)
	
	# 3. UMAP (ê¸°ë³¸ ì„¤ì •)
	umap = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
	X_umap = umap.fit_transform(X)
	
	# 4. t-SNE ë§¤ê°œë³€ìˆ˜ íŠœë‹
	tsne_tuned = TSNE(n_components=2, perplexity=50, learning_rate=300, n_iter=5000, random_state=42)
	X_tsne_tuned = tsne_tuned.fit_transform(X)
	
	# 5. PCA + t-SNE
	pca = PCA(n_components=2)  # PCAë¡œ ì°¨ì› ì¶•ì†Œ (ë°ì´í„°ì˜ ì°¨ì› ìˆ˜ ì´í•˜ë¡œ ì„¤ì •)
	X_pca = pca.fit_transform(X)
	tsne_pca = TSNE(n_components=2, perplexity=50, learning_rate=300, n_iter=5000, random_state=42)
	X_tsne_pca = tsne_pca.fit_transform(X_pca)
	
	# 6. ì‹œê°í™”
	fig, axs = plt.subplots(4, 2, figsize=(12, 20))
	
	# ì›ë°ì´í„° ì‹œê°í™”
	axs[0, 0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=10)
	axs[0, 0].set_title("ì› ë°ì´í„° (ë™ì‹¬ì›)")
	
	# ê¸°ë³¸ t-SNE
	axs[0, 1].scatter(X_tsne_basic[:, 0], X_tsne_basic[:, 1], c=y, cmap='viridis', s=10)
	axs[0, 1].set_title("ê¸°ë³¸ t-SNE")
	
	# UMAP (ìµœì í™”: n_neighbors=30, min_dist=0.05)
	axs[1, 0].scatter(X_umap_optimized[:, 0], X_umap_optimized[:, 1], c=y, cmap='viridis', s=10)
	axs[1, 0].set_title("ìµœì í™”ëœ UMAP (n_neighbors=30, min_dist=0.05)")
	
	# UMAP (ì¬ì¡°ì •: n_neighbors=20, min_dist=0.1, í‘œì¤€í™” ì ìš©)
	axs[1, 1].scatter(X_umap_revised[:, 0], X_umap_revised[:, 1], c=y, cmap='viridis', s=10)
	axs[1, 1].set_title("UMAP (í‘œì¤€í™”, n_neighbors=20, min_dist=0.1)")
	
	# UMAP (ê¸°ë³¸ ì„¤ì •)
	axs[2, 0].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis', s=10)
	axs[2, 0].set_title("UMAP (ê¸°ë³¸ ì„¤ì •)")
	
	# t-SNE ë§¤ê°œë³€ìˆ˜ íŠœë‹
	axs[2, 1].scatter(X_tsne_tuned[:, 0], X_tsne_tuned[:, 1], c=y, cmap='viridis', s=10)
	axs[2, 1].set_title("t-SNE (ë§¤ê°œë³€ìˆ˜ íŠœë‹)")
	
	# PCA + t-SNE
	axs[3, 0].scatter(X_tsne_pca[:, 0], X_tsne_pca[:, 1], c=y, cmap='viridis', s=10)
	axs[3, 0].set_title("PCA + t-SNE")
	
	# ë¹ˆ í”Œë¡¯
	axs[3, 1].axis('off')  # ë§ˆì§€ë§‰ ë¹ˆ í”Œë¡¯ ì œê±°
	
	# ë ˆì´ì•„ì›ƒ ì •ë¦¬
	plt.tight_layout()
	plt.show()
	
	# 7. í‰ê°€ ì¶œë ¥ (í•œê¸€ë¡œ ë²ˆì—­)
	print("ê²°ê³¼ ë¶„ì„:")
	print("1. ê¸°ë³¸ t-SNE: ë™ì‹¬ì›ì˜ ì „ì—­ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ì§€ ëª»í•  ìˆ˜ ìˆìœ¼ë©°, ë°ì´í„° í¬ì¸íŠ¸ê°€ ì„ì—¬ ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.")
	print("2. UMAP (ìµœì í™”, n_neighbors=30, min_dist=0.05): ë‘ ì›ì˜ ë¶„ë¦¬ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©°, ì „ì—­ êµ¬ì¡°ê°€ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
	print("3. UMAP (í‘œì¤€í™”, n_neighbors=20, min_dist=0.1): í‘œì¤€í™” ì ìš©ìœ¼ë¡œ ì „ì—­ ë° êµ­ì†Œ êµ¬ì¡°ê°€ ê°œì„ ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.")
	print("4. UMAP (ê¸°ë³¸ ì„¤ì •): ë‘ ì›ì˜ ì „ì—­ êµ¬ì¡° ë³´ì¡´ì´ ë¶€ì¡±í•˜ë©° ì™œê³¡ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
	print("5. t-SNE (ë§¤ê°œë³€ìˆ˜ íŠœë‹): ë‘ ì›ì˜ ì „ì—­ êµ¬ì¡°ì™€ êµ­ì†Œ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ë©°, ë¶„ë¦¬ê°€ ëª…í™•í•©ë‹ˆë‹¤.")
	print("6. PCA + t-SNE: ì „ì—­ êµ¬ì¡°ì™€ êµ­ì†Œ êµ¬ì¡°ê°€ ê· í˜• ìˆê²Œ ë³´ì¡´ë˜ë©°, ë™ì‹¬ì›ì˜ êµ¬ì¡°ë¥¼ ëª…í™•íˆ í‘œí˜„í•©ë‹ˆë‹¤.")


![](./images/result.png)

<br>

---








