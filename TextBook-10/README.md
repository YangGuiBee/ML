#  10 : ë¹„ì§€ë„ í•™ìŠµ (Unsupervised Learning, UL) : êµ°ì§‘í™”

---

	[1] KDE (Kernel Desity Estimation)
 	[2] k-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ (k-Means Clustering)
	[3] ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ (Hierarchical Clustering)
	[4] DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
	[5] ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ (Gaussian Mixture Model, GMM)
   
---  

# [1] KDE (Kernel Desity Estimation)

<br>

# [2] k-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ (k-Means Clustering)
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ Kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ„ê³  ê° êµ°ì§‘ì˜ ì¤‘ì‹¬(centroid)ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í• ë‹¹í•˜ëŠ” êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„± : ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ë°œê²¬í•˜ëŠ” ë° ìœ ìš©<br>
â–£ ì¥ì  : êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ê³„ì‚° ì†ë„ê°€ ë¹ ë¥´ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ì í•©<br>
â–£ ë‹¨ì  : êµ°ì§‘ì˜ ê°œìˆ˜(K)ë¥¼ ì‚¬ì „ì— ì •ì˜í•´ì•¼ í•˜ë©°, êµ¬í˜• êµ°ì§‘ì´ ì•„ë‹ˆê±°ë‚˜ ì´ìƒì¹˜(outliers)ê°€ ìˆì„ ê²½ìš° ì„±ëŠ¥ ì €í•˜<br>
â–£ ì‘ìš©ë¶„ì•¼ : ê³ ê° ì„¸ë¶„í™”, ì´ë¯¸ì§€ ë¶„í• , ì¶”ì²œ ì‹œìŠ¤í…œ<br>
â–£ ëª¨ë¸ì‹ : ğ¾ëŠ” êµ°ì§‘ì˜ ê°œìˆ˜, $ğ¶_ğ‘–$ëŠ” ië²ˆì§¸ êµ°ì§‘, $ğœ‡_ğ‘–$ëŠ” ië²ˆì§¸ êµ°ì§‘ì˜ ì¤‘ì‹¬, ğ‘¥ëŠ” ë°ì´í„° í¬ì¸íŠ¸<br>

	from sklearn.cluster import KMeans
	from sklearn.datasets import load_iris
	import matplotlib.pyplot as plt

	iris = load_iris()
	X = iris.data

	kmeans = KMeans(n_clusters=3, random_state=0)
	kmeans.fit(X)
	labels = kmeans.labels_

	plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
	plt.title("K-Means Clustering on Iris Dataset")
	plt.xlabel("Feature 1")
	plt.ylabel("Feature 2")
	plt.show()

<br>

# [3] ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ (Hierarchical Clustering)
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ ë³‘í•©(bottom-up)í•˜ê±°ë‚˜ ë¶„í• (top-down)í•˜ì—¬ ê³„ì¸µì ì¸ êµ°ì§‘ êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” ë°©ë²•<br>
â–£ í•„ìš”ì„± : êµ°ì§‘ì˜ ê°œìˆ˜ë¥¼ ì‚¬ì „ì— ì •í•  í•„ìš” ì—†ì´ ê³„ì¸µì  ê´€ê³„ë¥¼ íŒŒì•…í•  ë•Œ ì‚¬ìš©<br>
â–£ ì¥ì  : êµ°ì§‘ ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •í•  í•„ìš” ì—†ìœ¼ë©°, ë´ë“œë¡œê·¸ë¨(dendrogram)ì„ í†µí•œ êµ°ì§‘ ë¶„ì„ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë³µì¡ë„ê°€ ë†’ìœ¼ë©°, ì´ˆê¸° ë³‘í•© ë˜ëŠ” ë¶„í•  ê²°ì •ì´ ìµœì¢… ê²°ê³¼ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ<br>
â–£ ì‘ìš©ë¶„ì•¼ : ê³„í†µìˆ˜ ë¶„ì„, í…ìŠ¤íŠ¸ ë° ë¬¸ì„œ ë¶„ë¥˜<br> 
â–£ ëª¨ë¸ì‹ : $ğ¶_ğ‘–$ì™€ $ğ¶_ğ‘—$ëŠ” ê°ê° ë‘ êµ°ì§‘ì´ê³ , ğ‘‘(ğ‘¥,ğ‘¦)ëŠ” ë‘ ë°ì´í„° í¬ì¸íŠ¸ ğ‘¥ì™€ ğ‘¦ ê°„ì˜ ê±°ë¦¬<br>

	from scipy.cluster.hierarchy import dendrogram, linkage
	import matplotlib.pyplot as plt
	from sklearn.datasets import load_iris

	iris = load_iris()
	X = iris.data

	Z = linkage(X, 'ward')  # ward: ìµœì†Œë¶„ì‚° ê¸°ì¤€ ë³‘í•©

	plt.figure(figsize=(10, 5))
	dendrogram(Z)
	plt.title("Hierarchical Clustering Dendrogram")
	plt.xlabel("Sample Index")
	plt.ylabel("Distance")
	plt.show()

â–£ ë´ë“œë¡œê·¸ë¨(dendrogram) : 

<br>

# [4] DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

<br>

# [5] ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ (Gaussian Mixture Model, GMM)

<br>
