#  15 : CASE Study

---

  https://scikit-learn.org/stable/auto_examples/applications/index.html

<br>

  https://www.kaggle.com/

<br>

  ì°¸ê³  : https://github.com/dair-ai/ML-Course-Notes

<br>

# Machine Learning ìµœì í™” ë°©ì•ˆ

<br>

	[1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
		[1-1] ë°ì´í„° ì¦ê°•(Data Augmentation)
		[1-2] êµì°¨ ê²€ì¦(Cross-Validation)
		[1-3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§(Data Scaling)
		[1-4] ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬(Handling Imbalanced Data)
		[1-5] ê²°ì¸¡ê°’ ì²˜ë¦¬(Handling Missing Data)
		[1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection)
		[1-7] ë°ì´í„° ì¤‘ë³µ ì œê±°(Data Deduplication)
		[1-8] ë°ì´í„° ë³€í™˜(Data Transformation)
		[1-9] íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)
		[1-10] ì •ë³´ ë³‘í•©(Data Fusion)
	
	[2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™” : ê³¼ì í•© ë°©ì§€(Overfitting Prevention)
		[2-1] ì •ê·œí™”(L1, L2 Regularization)
		[2-2] ì¡°ê¸° ì¢…ë£Œ(Early Stopping)
		[2-3] ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)
		[2-4] ëª¨ë¸ í•´ì„ì„±(Model Interpretability)
	
	[3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
		[3-1] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Hyperparameter Tuning)
		[3-2] ê·¸ë¦¬ë“œ ì„œì¹˜(Grid Search)
		[3-3] ëœë¤ ì„œì¹˜(Random Search)
		[3-4] ë² ì´ì¦ˆ ìµœì í™”(Bayesian Optimization)
		[3-5] í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìë™í™”(Automated Hyperparameter Tuning)
		[3-6] AutoML í™œìš©(AutoML)
	
	[4] í•™ìŠµ ê³¼ì • ìµœì í™”
		[4-1] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Learning Rate Scheduling)
		[4-2] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Initialization)
		[4-3] í™œì„±í™” í•¨ìˆ˜ ì„ íƒ(Activation Function Selection)
		[4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
		[4-5] ì „ì´ í•™ìŠµ(Transfer Learning)
		[4-6] ëª¨ë¸ êµ¬ì¡° ìµœì í™”(Model Architecture Optimization)
		[4-7] ì˜¨ë¼ì¸ í•™ìŠµ(Online Learning)
	
	[5] ì„±ëŠ¥ í–¥ìƒ
		[5-1] íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ(Feature Importance & Selection)
		[5-2] ì†ì‹¤í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•(Custom Loss Function)
  
	[6] í•˜ë“œì›¨ì–´ ë° ì‹œìŠ¤í…œ ìµœì í™”
		[6-1] í•˜ë“œì›¨ì–´ ìµœì í™”(Hardware Optimization)

	[7] ëª¨ë¸ ê²€ì¦ ë° ë¹„êµ
		[7-1] ëª¨ë¸ ê²€ì¦(Model Validation)
		[7-2] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ(Model Performance Comparison) 

	[8] ê¸°ìˆ  ë¶€ì±„ ê´€ë¦¬
		[8-1] ê¸°ìˆ  ë¶€ì±„(Technical Debt) ê´€ë¦¬

<br>

--- 


# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
## [1-1] ë°ì´í„° ì¦ê°•(Data Augmentation)
â–£ ì •ì˜ : ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ë³€í˜•í•˜ê±°ë‚˜ ê°€ê³µí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë°ì´í„° ì–‘ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë°ì´í„° ë‹¤ì–‘ì„±ì´ ë‚®ì€ ê²½ìš°, ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ<br>
â–£ ì¥ì  : ë°ì´í„°ì…‹ì˜ ë‹¤ì–‘ì„±ì„ ì¸ìœ„ì ìœ¼ë¡œ ì¦ê°€, ì¶”ê°€ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ì—†ì´ ì„±ëŠ¥ í–¥ìƒ, ê³¼ì í•© ë°©ì§€ íš¨ê³¼<br>
â–£ ë‹¨ì  : ì¦ê°•ëœ ë°ì´í„°ê°€ ì‹¤ì œ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ì²˜ë¦¬ ì‹œê°„ì´ ì¦ê°€í•˜ë©°, ë¹„íš¨ìœ¨ì ì¸ ì¦ê°•ì€ ì„±ëŠ¥ì— ë¶€ì •ì  ì˜í–¥<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ (CNN, RNN ë“±), ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ìŒì„± ì²˜ë¦¬ ëª¨ë¸<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-1] ë°ì´í„° ì¦ê°• (Data Augmentation)
	# ìˆ«ì ë°ì´í„° : ë…¸ì´ì¦ˆì™€ ë¹„ì„ í˜•ì„± ì¶”ê°€
	#############################################################
	import numpy as np
	import pandas as pd
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import mean_squared_error, r2_score
	import matplotlib.pyplot as plt

	# ì›ë³¸ ë°ì´í„° ìƒì„±
	np.random.seed(42)
	X = np.random.rand(100, 1) * 10  # Feature
	y = 2.5 * X.flatten() + np.random.randn(100) * 2  # Target with noise

	# ì›ë³¸ ë°ì´í„°í”„ë ˆì„
	original_data = pd.DataFrame({"X": X.flatten(), "y": y})

	# ë°ì´í„° ì¦ê°•: ë…¸ì´ì¦ˆì™€ ë¹„ì„ í˜•ì„± ì¶”ê°€
	augmented_X = X + np.random.randn(100, 1) * 0.2  # ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€
	augmented_y = 2.5 * augmented_X.flatten() + np.random.randn(100) * 0.5 + 0.2 * np.sin(augmented_X.flatten())  # ë¹„ì„ í˜•ì„± ì¶”ê°€
	augmented_data = pd.DataFrame({"X": augmented_X.flatten(), "y": augmented_y})

	# ë°ì´í„° ë³‘í•©
	combined_data = pd.concat([original_data, augmented_data], ignore_index=True)

	# ë°ì´í„° ë¶„ë¦¬: ì›ë³¸ ë°ì´í„°
	X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
	    original_data["X"].values.reshape(-1, 1), original_data["y"], test_size=0.2, random_state=42)

	# ë°ì´í„° ë¶„ë¦¬: ì¦ê°• ë°ì´í„°
	X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(
	    combined_data["X"].values.reshape(-1, 1), combined_data["y"], test_size=0.2, random_state=42)

	# ëª¨ë¸ í•™ìŠµ: ì›ë³¸ ë°ì´í„°
	model_orig = LinearRegression()
	model_orig.fit(X_train_orig, y_train_orig)
	y_pred_orig = model_orig.predict(X_test_orig)

	# ëª¨ë¸ í•™ìŠµ: ì¦ê°• ë°ì´í„°
	model_aug = LinearRegression()
	model_aug.fit(X_train_aug, y_train_aug)
	y_pred_aug = model_aug.predict(X_test_aug)

	# ì„±ëŠ¥ í‰ê°€
	mse_orig = mean_squared_error(y_test_orig, y_pred_orig)
	r2_orig = r2_score(y_test_orig, y_pred_orig)

	mse_aug = mean_squared_error(y_test_aug, y_pred_aug)
	r2_aug = r2_score(y_test_aug, y_pred_aug)

	# ê²°ê³¼ ì¶œë ¥
	print("=== ì›ë³¸ ë°ì´í„° ì„±ëŠ¥ ===")
	print(f"Mean Squared Error: {mse_orig:.4f}")
	print(f"R2 Score: {r2_orig:.4f}")

	print("\n=== ì¦ê°• ë°ì´í„° ì„±ëŠ¥ ===")
	print(f"Mean Squared Error: {mse_aug:.4f}")
	print(f"R2 Score: {r2_aug:.4f}")

	# ì‹œê°í™”
	plt.scatter(original_data["X"], original_data["y"], label="Original Data", color="blue", alpha=0.6)
	plt.scatter(augmented_data["X"], augmented_data["y"], label="Augmented Data", color="orange", alpha=0.4)
	plt.plot(X_test_orig, y_pred_orig, label="Model (Original)", color="green")
	plt.plot(X_test_aug, y_pred_aug, label="Model (Augmented)", color="red")
	plt.xlabel("X")
	plt.ylabel("y")
	plt.title("Original vs Augmented Data and Models")
	plt.legend()
	plt.show()

![](./images/1-1.png) 
<br>

## [1-2] êµì°¨ ê²€ì¦(Cross-Validation)
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµê³¼ ê²€ì¦ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€<br>
â–£ í•„ìš”ì„± : ê³¼ì í•© ë° ê³¼ì†Œì í•© ì—¬ë¶€ë¥¼ íŒë³„í•˜ê±°ë‚˜, ë°ì´í„°ê°€ ì œí•œì ì¼ ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹ ë¢°ì„± ìˆê²Œ í‰ê°€í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë°ì´í„° ë¶„í¬ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ì¼ë°˜í™” ì„±ëŠ¥ì— ëŒ€í•œ ì‹ ë¢°ë„ ì¦ê°€<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš©ì´ ì¦ê°€í•˜ì—¬ í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë©°, í° ë°ì´í„°ì…‹ì—ì„œëŠ” ë¹„íš¨ìœ¨ì <br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ (ë¶„ë¥˜, íšŒê·€ ë“±), íŠ¹íˆ ì†Œê·œëª¨ ë°ì´í„°ì…‹ì— ì í•©<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-2] êµì°¨ ê²€ì¦ (Cross-Validation) + ë°ì´í„°ì¦ê°•
	#############################################################
	from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.datasets import load_wine
	from sklearn.preprocessing import StandardScaler
	from sklearn.pipeline import Pipeline
	from sklearn.metrics import accuracy_score
	import numpy as np
	import pandas as pd

	# ë°ì´í„° ë¡œë“œ
	data = load_wine()
	X, y = data.data, data.target

	# ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì™€ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
	X_train, X_new, y_train, y_new = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì¦ê°•: ë…¸ì´ì¦ˆ ì¶”ê°€ ë° íŠ¹ì„± ë³€í˜•
	np.random.seed(42)
	noise = np.random.normal(0, 0.2, X_train.shape)  # í‰ê·  0, í‘œì¤€í¸ì°¨ 0.2ì¸ ë…¸ì´ì¦ˆ
	X_augmented = X_train + noise  # ë…¸ì´ì¦ˆ ì¶”ê°€
	y_augmented = y_train  # ë ˆì´ë¸”ì€ ë™ì¼

	# ì¦ê°• ë°ì´í„° í•©ì¹˜ê¸°
	X_combined = np.vstack((X_train, X_augmented))
	y_combined = np.hstack((y_train, y_augmented))

	# ê¸°ë³¸ ëª¨ë¸ ìƒì„±
	model = RandomForestClassifier(random_state=42)

	# êµì°¨ ê²€ì¦ ì—†ì´ í•™ìŠµ ë° ìƒˆë¡œìš´ ë°ì´í„° í‰ê°€ (ê¸°ë³¸ ë°ì´í„°)
	model.fit(X_train, y_train)
	new_predictions = model.predict(X_new)
	accuracy_new_data = accuracy_score(y_new, new_predictions)

	# êµì°¨ ê²€ì¦ (ê¸°ë³¸ ë°ì´í„°)
	kf = KFold(n_splits=5, shuffle=True, random_state=42)
	scores_without_augmentation = cross_val_score(model, X_train, y_train, cv=kf)

	# êµì°¨ ê²€ì¦ (ì¦ê°• ë°ì´í„° í¬í•¨)
	scores_with_augmentation = cross_val_score(model, X_combined, y_combined, cv=kf)

	# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ í¬í•¨í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
	pipeline = Pipeline([
	    ('scaler', StandardScaler()),
	    ('classifier', RandomForestClassifier(random_state=42))])

	# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì¦ê°• ë°ì´í„° í¬í•¨)
	param_grid = {
	    'classifier__n_estimators': [50, 100, 150],
	    'classifier__max_depth': [None, 10, 20],
	    'classifier__min_samples_split': [2, 5, 10]}

	grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
	grid_search.fit(X_combined, y_combined)

	# ìµœì  ëª¨ë¸ë¡œ êµì°¨ ê²€ì¦
	best_model = grid_search.best_estimator_
	scores_with_tuning = cross_val_score(best_model, X_combined, y_combined, cv=kf)

	# ê²°ê³¼ ì¶œë ¥
	print("=== ìƒˆë¡œìš´ ë°ì´í„°ë¡œ í‰ê°€ (ê¸°ë³¸ ë°ì´í„°) ===")
	print(f"ìƒˆë¡œìš´ ë°ì´í„° ì •í™•ë„: {accuracy_new_data:.4f}")

	print("\n=== ê¸°ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê²°ê³¼ ===")
	print(f"êµì°¨ ê²€ì¦ ì ìˆ˜ (ê¸°ë³¸ ë°ì´í„°): {scores_without_augmentation}")
	print(f"í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_without_augmentation.mean():.4f}")

	print("\n=== ì¦ê°• ë°ì´í„° í¬í•¨ êµì°¨ ê²€ì¦ ê²°ê³¼ ===")
	print(f"êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨): {scores_with_augmentation}")
	print(f"í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_with_augmentation.mean():.4f}")

	print("\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ (ì¦ê°• ë°ì´í„° í¬í•¨) ===")
	print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
	print(f"ìµœì  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.4f}")

	print("\n=== ìµœì  ëª¨ë¸ êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨) ===")
	print(f"êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_with_tuning}")
	print(f"í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_with_tuning.mean():.4f}")

<br>

	=== ìƒˆë¡œìš´ ë°ì´í„°ë¡œ í‰ê°€ (ê¸°ë³¸ ë°ì´í„°) ===
	ìƒˆë¡œìš´ ë°ì´í„° ì •í™•ë„: 1.0000

	=== ê¸°ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê²°ê³¼ ===
	êµì°¨ ê²€ì¦ ì ìˆ˜ (ê¸°ë³¸ ë°ì´í„°): [0.93103448 0.96551724 1.         1.         1.        ]
	í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9793

	=== ì¦ê°• ë°ì´í„° í¬í•¨ êµì°¨ ê²€ì¦ ê²°ê³¼ ===
	êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨): [0.96491228 1.         0.96491228 0.98245614 0.98214286]
	í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9789

	=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ (ì¦ê°• ë°ì´í„° í¬í•¨) ===
	ìµœì  íŒŒë¼ë¯¸í„°: {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}
	ìµœì  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9895

	=== ìµœì  ëª¨ë¸ êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨) ===
	êµì°¨ ê²€ì¦ ì ìˆ˜: [0.96491228 1.         0.96491228 0.98245614 0.98214286]
	í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9789

<br>

## [1-3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§(Data Scaling)
â–£ ì •ì˜ : ë°ì´í„°ì˜ íŠ¹ì„± ê°’ì„ ì¼ì •í•œ ë²”ìœ„ë‚˜ ë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœ(í‘œì¤€í™”(Standardization): í‰ê·  0, í‘œì¤€í¸ì°¨ 1, 
ì •ê·œí™”(Normalization): ìµœì†Œ-ìµœëŒ€ ìŠ¤ì¼€ì¼ë§ 0-1)ë¡œ ë§Œë“œëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : íŠ¹ì„± ê°„ì˜ í¬ê¸° ì°¨ì´ê°€ í´ ê²½ìš°, ëª¨ë¸ í•™ìŠµì´ ì™œê³¡ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ì¡°ì •í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„ í–¥ìƒ<br>
â–£ ì¥ì  : ëª¨ë¸ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ, í•™ìŠµ ì†ë„ ì¦ê°€, ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ë°ì´í„° ë¶„í¬ë¥¼ ì˜ëª» ì¡°ì •í•˜ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜, ìŠ¤ì¼€ì¼ë§ ë‹¨ê³„ì—ì„œ ì¶”ê°€ì ì¸ ê³„ì‚°ì´ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ (KNN, SVM ë“±), ì„ í˜• ëª¨ë¸ (ë¡œì§€ìŠ¤í‹± íšŒê·€, ì„ í˜• íšŒê·€), ë”¥ëŸ¬ë‹ ëª¨ë¸<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (Data Scaling)
	# MinMaxScaler : ë°ì´í„°ë¥¼ íŠ¹ì • ë²”ìœ„(ê¸°ë³¸ê°’: [0, 1])ë¡œ ì •ê·œí™”
	# ğ‘‹ : ì›ë³¸ ë°ì´í„° ê°’
	# ğ‘‹_{min}  : ê° ì—´ì˜ ìµœì†Œê°’
	# ğ‘‹_{max}  : ê° ì—´ì˜ ìµœëŒ€ê°’
	# ğ‘‹â€² : ë³€í™˜ëœ ë°ì´í„° ê°’
	# ğ‘‹â€² = (ğ‘‹ - ğ‘‹_{min}) / (ğ‘‹_{max} - ğ‘‹_{min})
	#############################################################
	import numpy as np
	from sklearn.datasets import make_classification
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import MinMaxScaler
	from sklearn.neighbors import KNeighborsClassifier
	from sklearn.metrics import accuracy_score

	# ë°ì´í„° ì¶œë ¥ í˜•ì‹ ì„¤ì • (ì†Œìˆ˜ì  ì´í•˜ 4ìë¦¬ê¹Œì§€)
	np.set_printoptions(precision=4, suppress=True)

	# ë°ì´í„° ìƒì„±
	X, y = make_classification(
	    n_samples=500,
	    n_features=5,
	    n_informative=3,
	    n_redundant=0,
	    random_state=42)

	# ì¸ìœ„ì ìœ¼ë¡œ íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ë¥¼ í¬ê²Œ ë§Œë“¦
	X[:, 0] *= 1    # ì²« ë²ˆì§¸ íŠ¹ì„±: 0~1
	X[:, 1] *= 100  # ë‘ ë²ˆì§¸ íŠ¹ì„±: 0~100
	X[:, 2] *= 1000 # ì„¸ ë²ˆì§¸ íŠ¹ì„±: 0~1000

	print("ì›ë³¸ ë°ì´í„° (ì¼ë¶€):\n", X[:5])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

	# KNN ëª¨ë¸ ìƒì„±
	knn = KNeighborsClassifier()

	# 1. ì›ë³¸ ë°ì´í„°ë¡œ í‰ê°€
	knn.fit(X_train, y_train)
	y_pred_original = knn.predict(X_test)
	accuracy_original = accuracy_score(y_test, y_pred_original)

	# 2. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
	scaler = MinMaxScaler()
	X_train_scaled = scaler.fit_transform(X_train)
	X_test_scaled = scaler.transform(X_test)

	print("\nìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° (í›ˆë ¨ ì„¸íŠ¸ ì¼ë¶€):\n", X_train_scaled[:5])

	# ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í•™ìŠµ ë° í‰ê°€
	knn.fit(X_train_scaled, y_train)
	y_pred_scaled = knn.predict(X_test_scaled)
	accuracy_scaled = accuracy_score(y_test, y_pred_scaled)

	# ê²°ê³¼ ì¶œë ¥
	print("\n=== í‰ê°€ ê²°ê³¼ ===")
	print(f"ì›ë³¸ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy_original:.4f}")
	print(f"ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy_scaled:.4f}")

<br>

	ì›ë³¸ ë°ì´í„° (ì¼ë¶€):
	 [[  -1.8306   -9.534  -654.0757    0.7241   -0.1813]
	 [   0.2603    8.0151 -413.4652   -1.2733    1.4826]
 	 [  -1.3796    9.8744 -971.6567   -0.0728   -1.5796]
	 [  -0.9981  -16.1506 1051.9476    2.3985    2.1207]
	 [  -0.3696  122.3565  621.5719    0.0128   -1.4224]]

	ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° (í›ˆë ¨ ì„¸íŠ¸ ì¼ë¶€):
	 [[0.5246 0.7534 0.5159 0.7898 0.714 ]
	 [0.6738 0.2881 0.6199 0.4736 0.4592]
	 [0.3458 0.3688 0.2804 0.1617 0.5876]
	 [0.3992 0.5641 0.541  0.6432 0.4749]
	 [0.5227 0.4134 0.4271 0.1323 0.6014]]

	=== í‰ê°€ ê²°ê³¼ ===
	ì›ë³¸ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.8480
	ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9360
 
<br>

## [1-4] ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬(Handling Imbalanced Data)
![](./images/DA_vs.png)<br>
â–£ ì •ì˜ : í´ë˜ìŠ¤ ê°„ ë°ì´í„° ë¹„ìœ¨ì´ ì‹¬ê°í•˜ê²Œ ë¶ˆê· í˜•í•  ë•Œ, ëª¨ë¸ì˜ í•™ìŠµ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì¡°ì •<br>
â–£ í•„ìš”ì„± : ë¶ˆê· í˜• ë°ì´í„°ëŠ” ëª¨ë¸ì´ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì„ í˜¸í•˜ë„ë¡ í•™ìŠµí•˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì—, ì´ë¥¼ í•´ê²°í•˜ì§€ ì•Šìœ¼ë©´ íŠ¹ì • í´ë˜ìŠ¤ì˜ ì„±ëŠ¥ì´ ì €í•˜<br>
â–£ ì¥ì  : í´ë˜ìŠ¤ ê°„ ê· í˜•ì„ ë§ì¶° ëª¨ë¸ì˜ ê³µì •ì„±ê³¼ ì„±ëŠ¥ í–¥ìƒ, ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê°œì„ <br>
â–£ ë‹¨ì  : ì–¸ë”ìƒ˜í”Œë§ì€ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±, ì˜¤ë²„ìƒ˜í”Œë§ì€ ê³¼ì í•© ìœ„í—˜, ê°€ì¤‘ì¹˜ ì¡°ì •ì€ ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ ë¶„ë¥˜ ë¬¸ì œ (ì´ì§„ ë¶„ë¥˜, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜)<br>

**SMOTE (Synthetic Minority Over-sampling Technique)** <br>
ê¸°ì¡´ì˜ ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ì„ í˜• ë³´ê°„í•˜ì—¬ ìƒì„±í•˜ëŠ” ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²•<br>
ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ ê· ì¼í•˜ê²Œ ì¦ê°•í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì‹œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì™„í™”<br>
ì†Œìˆ˜ í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ê· ì¼í•˜ê²Œ ì¦ê°•í•˜ì—¬ í´ë˜ìŠ¤ ë¶„í¬ ê· í˜•ì— íš¨ê³¼ì <br>
ìµœê·¼ì ‘ ì´ì›ƒê³„ì‚° â†’ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„± â†’ ì¦ê°•ê³¼ì • ë°˜ë³µ<br>

**ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning)** <br>
SMOTEì˜ í™•ì¥ìœ¼ë¡œ, ì†Œìˆ˜ í´ë˜ìŠ¤ ì£¼ë³€ì˜ ë°€ë„ì— ë”°ë¼ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±<br>
ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë¶ˆê· í˜• ë°ì´í„°ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²•<br>
ë°€ë„ê°€ ë‚®ì€ ì˜ì—­ì— ë” ë§ì€ ìƒ˜í”Œì„ ìƒì„±í•˜ì—¬ ë¶„ë¥˜ ê²½ê³„ì— ê°€ê¹Œìš´ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ìƒ˜í”Œì— ì´ˆì <br>
ë°€ë„ ê³„ì‚° â†’ ê°€ì¤‘ì¹˜ ê³„ì‚° â†’ ìƒ˜í”Œìƒì„± ë¹„ìœ¨ ê²°ì • â†’ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„±<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-4] ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ (Handling Imbalanced Data)
	# ADASYN + SMOTE(Synthetic Minority Over-sampling Technique)
	#############################################################
	from imblearn.over_sampling import SMOTE, ADASYN
	from sklearn.datasets import make_classification
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
	from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
	import pandas as pd

	# ê·¹ë‹¨ì ì¸ ë¶ˆê· í˜• ë°ì´í„° ìƒì„±
	X, y = make_classification(
	    n_classes=2,          # ì´ì§„ ë¶„ë¥˜
	    class_sep=2,          # í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ ì •ë„
	    weights=[0.005, 0.995], # í´ë˜ìŠ¤ ë¹„ìœ¨: 0.5% vs 99.5%
	    n_informative=3,      # ì •ë³´ê°€ ìˆëŠ” ë…ë¦½ ë³€ìˆ˜ 3ê°œ
	    n_redundant=1,        # ì¤‘ë³µëœ ë…ë¦½ ë³€ìˆ˜ 1ê°œ
	    flip_y=0,             # ë¼ë²¨ ë’¤ì§‘ê¸° ë¹„ìœ¨ ì—†ìŒ
	    n_features=5,         # ì´ íŠ¹ì„± ìˆ˜: 5ê°œ
	    n_clusters_per_class=1, # ê° í´ë˜ìŠ¤ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°
	    n_samples=2000,       # ì´ ìƒ˜í”Œ ìˆ˜: 2000ê°œ
	    random_state=10       # ë‚œìˆ˜ ê³ ì •
	)
	print("ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬:\n", pd.Series(y).value_counts())

	# êµì°¨ ê²€ì¦ ì„¤ì •
	kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

	# 1. ì›ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ í‰ê°€
	model = RandomForestClassifier(random_state=42)
	scores_original = cross_val_score(model, X, y, cv=kf, scoring='roc_auc')
	print("\n[êµì°¨ ê²€ì¦] ì›ë³¸ ë°ì´í„° ROC-AUC:", scores_original.mean())

	# ë°ì´í„° ë¶„ë¦¬ (í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

	# 2. ì›ë³¸ ë°ì´í„° ë¶„ë¦¬ í‰ê°€
	model.fit(X_train, y_train)
	y_pred_original = model.predict(X_test)
	accuracy_original = accuracy_score(y_test, y_pred_original)

	print("\n[ë¶„ë¦¬ í‰ê°€] === ì›ë³¸ ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
	print(f"ì •í™•ë„: {accuracy_original:.4f}")
	print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred_original, zero_division=0))

	# ADASYN ì ìš©
	adasyn = ADASYN(sampling_strategy=0.5, random_state=42)
	X_adasyn, y_adasyn = adasyn.fit_resample(X, y)
	X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)
	print("\nADASYN ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ (ì „ì²´ ë°ì´í„°):\n", pd.Series(y_adasyn).value_counts())

	# 3. ADASYN êµì°¨ ê²€ì¦ í‰ê°€
	scores_adasyn = cross_val_score(model, X_adasyn, y_adasyn, cv=kf, scoring='roc_auc')
	print("\n[êµì°¨ ê²€ì¦] ADASYN ë°ì´í„° ROC-AUC:", scores_adasyn.mean())

	# 4. ADASYN ë¶„ë¦¬ í‰ê°€
	model.fit(X_train_adasyn, y_train_adasyn)
	y_pred_adasyn = model.predict(X_test)
	accuracy_adasyn = accuracy_score(y_test, y_pred_adasyn)

	print("\n[ë¶„ë¦¬ í‰ê°€] === ADASYN ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
	print(f"ì •í™•ë„: {accuracy_adasyn:.4f}")
	print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred_adasyn, zero_division=0))

	# SMOTE ì ìš©
	smote = SMOTE(sampling_strategy=0.5, random_state=42)
	X_smote, y_smote = smote.fit_resample(X, y)
	X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
	print("\nSMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ (ì „ì²´ ë°ì´í„°):\n", pd.Series(y_smote).value_counts())

	# 5. SMOTE êµì°¨ ê²€ì¦ í‰ê°€
	scores_smote = cross_val_score(model, X_smote, y_smote, cv=kf, scoring='roc_auc')
	print("\n[êµì°¨ ê²€ì¦] SMOTE ë°ì´í„° ROC-AUC:", scores_smote.mean())

	# 6. SMOTE ë¶„ë¦¬ í‰ê°€
	model.fit(X_train_smote, y_train_smote)
	y_pred_smote = model.predict(X_test)
	accuracy_smote = accuracy_score(y_test, y_pred_smote)

	print("\n[ë¶„ë¦¬ í‰ê°€] === SMOTE ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
	print(f"ì •í™•ë„: {accuracy_smote:.4f}")
	print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred_smote, zero_division=0))

<br>

![](./images/1-4.png) 

<br>

## [1-5] ê²°ì¸¡ê°’ ì²˜ë¦¬(Handling Missing Data)
â–£ ì •ì˜ : ë°ì´í„°ì…‹ì—ì„œ ëˆ„ë½ëœ ê°’(null, NaN)ì„ ì²˜ë¦¬<br>
â–£ í•„ìš”ì„± : ê²°ì¸¡ê°’ì€ ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ì„ ë°©í•´í•˜ê±°ë‚˜ ì™œê³¡ëœ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„ìˆ˜ì ìœ¼ë¡œ í•„ìš”<br>
â–£ ì£¼ìš” ê¸°ë²• : ê²°ì¸¡ê°’ì´ í¬í•¨ëœ í–‰ì´ë‚˜ ì—´ì„ ì œê±°(Deletion), í‰ê· /ì¤‘ì•™ê°’/ìµœë¹ˆê°’ ë“±ìœ¼ë¡œ ëŒ€ì²´(Imputation), ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê²°ì¸¡ê°’ ì˜ˆì¸¡(Predictive Modeling), ìœ ì‚¬í•œ ê´€ì¸¡ì¹˜ë¡œ ê²°ì¸¡ê°’ ëŒ€ì²´(KNN Imputation)<br>
â–£ ì¥ì  : ë°ì´í„° í’ˆì§ˆ í–¥ìƒìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ , ì•ˆì •ì ì´ê³  ì‹ ë¢°ì„± ìˆëŠ” í•™ìŠµ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³¼ë„í•œ ì‚­ì œëŠ” ë°ì´í„° ì†ì‹¤ ìœ„í—˜, ë¶€ì •í™•í•œ ëŒ€ì²´ëŠ” ëª¨ë¸ í¸í–¥ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì„ í˜• íšŒê·€, ì˜ì‚¬ê²°ì • ë‚˜ë¬´, ì‹ ê²½ë§ ë“± ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-5] ê²°ì¸¡ê°’ ì²˜ë¦¬(Handling Missing Data)
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_df.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_df['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 3. ê²°ì¸¡ê°’ ìƒì„± (ì˜ˆì œìš©)
	# ëœë¤ìœ¼ë¡œ 10ê°œì˜ ê°’ì— ê²°ì¸¡ê°’(NaN)ì„ ì‚½ì…
	np.random.seed(42)
	missing_indices = np.random.choice(X.size, 10, replace=False)
	X_flat = X.values.flatten()
	X_flat[missing_indices] = np.nan
	X_with_missing = pd.DataFrame(X_flat.reshape(X.shape), columns=X.columns)

	# ê²°ì¸¡ê°’ í™•ì¸
	print("Data with missing values:")
	print(X_with_missing.isnull().sum())

	# 4. ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²•
	# (1) ê²°ì¸¡ê°’ì„ í¬í•¨í•œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´)
	X_with_zeros = X_with_missing.fillna(0)
	X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
	    X_with_zeros, y, test_size=0.3, random_state=42)

	# (2) ê²°ì¸¡ê°’ì´ ìˆëŠ” ìƒ˜í”Œ ì œê±°
	X_dropped = X_with_missing.dropna()
	y_dropped = y[X_with_missing.dropna().index]
	X_train_dropped, X_test_dropped, y_train_dropped, y_test_dropped = train_test_split(
	    X_dropped, y_dropped, test_size=0.3, random_state=42)

	# (3) ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
	X_imputed = X_with_missing.fillna(X_with_missing.mean())
	X_train_imputed, X_test_imputed, y_train_imputed, y_test_imputed = train_test_split(
	    X_imputed, y, test_size=0.3, random_state=42)

	# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ (0 ëŒ€ì²´)
	model_raw = LogisticRegression(max_iter=200)
	model_raw.fit(X_train_raw, y_train_raw)
	y_pred_raw = model_raw.predict(X_test_raw)
	accuracy_raw = accuracy_score(y_test_raw, y_pred_raw)

	# (2) ê²°ì¸¡ê°’ ì œê±° ë°ì´í„° ì‚¬ìš©
	model_dropped = LogisticRegression(max_iter=200)
	model_dropped.fit(X_train_dropped, y_train_dropped)
	y_pred_dropped = model_dropped.predict(X_test_dropped)
	accuracy_dropped = accuracy_score(y_test_dropped, y_pred_dropped)

	# (3) ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•œ ë°ì´í„° ì‚¬ìš©
	model_imputed = LogisticRegression(max_iter=200)
	model_imputed.fit(X_train_imputed, y_train_imputed)
	y_pred_imputed = model_imputed.predict(X_test_imputed)
	accuracy_imputed = accuracy_score(y_test_imputed, y_pred_imputed)

	# 6. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy before handling missing values (0 imputation): {accuracy_raw:.2f}")
	print(f"Accuracy after dropping missing samples: {accuracy_dropped:.2f}")
	print(f"Accuracy after handling missing values (mean imputation): {accuracy_imputed:.2f}")

<br>

	Data with missing values:
	sepal length (cm)    1
	sepal width (cm)     5
	petal length (cm)    3
	petal width (cm)     1
	dtype: int64

	Accuracy before handling missing values (0 imputation): 0.96
	Accuracy after dropping missing samples: 0.98
	Accuracy after handling missing values (mean imputation): 0.96

<br>

## [1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection)
â–£ ì •ì˜ : ë°ì´í„° ë¶„í¬ì—ì„œ ë¹„ì •ìƒì ìœ¼ë¡œ ë²—ì–´ë‚œ ë°ì´í„°ë¥¼ íƒì§€í•˜ê³  ì²˜ë¦¬<br>
â–£ í•„ìš”ì„± : ì´ìƒì¹˜ëŠ” ë°ì´í„° ë¶„í¬ë¥¼ ì™œê³¡í•˜ê³  ëª¨ë¸ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹ë³„ê³¼ ì²˜ë¦¬ê°€ í•„ìš”<br>
â–£ ì£¼ìš” ê¸°ë²• : í†µê³„ ê¸°ë°˜ ê¸°ë²•(ì‚¬ë¶„ìœ„ ë²”ìœ„(IQR), ì¤‘ì•™ê°’ ì ˆëŒ€ í¸ì°¨(MAD), Z-Score), ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•(Isolation Forest, DBSCAN, One-Class SVM), ì‹œê°í™” ê¸°ë°˜ íƒì§€(Box Plot, Scatter Plot)<br>
â–£ ì¥ì  : ë°ì´í„° ì‹ ë¢°ì„±ê³¼ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”, ì ì¬ì  ì˜¤ë¥˜ë¥¼ ì‹ë³„í•˜ì—¬ ë¬¸ì œ ì˜ˆë°©<br>
â–£ ë‹¨ì  : ê³¼ë„í•œ íƒì§€ ê¸°ì¤€ì€ ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ì œê±°í•  ìœ„í—˜, ê³ ì°¨ì› ë°ì´í„°ì—ì„œëŠ” íƒì§€ê°€ ì–´ë ¤ì›€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : íšŒê·€ë¶„ì„, PCA, í´ëŸ¬ìŠ¤í„°ë§, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë”¥ëŸ¬ë‹ ë“±<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection)
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_df.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_df['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 3. ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
	def detect_outliers_iqr(data):
	    """IQR(ì‚¬ë¶„ìœ„ ë²”ìœ„)ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ íƒì§€"""
	    Q1 = data.quantile(0.25)
	    Q3 = data.quantile(0.75)
	    IQR = Q3 - Q1
	    lower_bound = Q1 - 1.5 * IQR
	    upper_bound = Q3 + 1.5 * IQR
	    outliers = (data < lower_bound) | (data > upper_bound)
	    return ~outliers.any(axis=1), outliers

	# ì´ìƒì¹˜ íƒì§€
	outlier_mask, outliers_boolean = detect_outliers_iqr(X)
	X_no_outliers = X[outlier_mask]
	y_no_outliers = y[outlier_mask]

	# ì´ìƒì¹˜ ë°ì´í„° ì¶”ì¶œ
	outliers_detected = X[~outlier_mask].copy()

	# ì´ìƒì¹˜ ì‚¬ìœ  ì¶”ê°€
	reasons = []
	for index, row in outliers_detected.iterrows():
	    reason = []
	    for column in X.columns:
	        if outliers_boolean.at[index, column]:
	            reason.append(f"{column} out of range")
	    reasons.append(", ".join(reason))
	outliers_detected['Reason'] = reasons

	print("Outliers detected:")
	print(outliers_detected)

	# ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
	print(f"\nOriginal data size: {X.shape[0]}")
	print(f"Data size after removing outliers: {X_no_outliers.shape[0]}")
	print(f"Number of outliers detected: {X.shape[0] - X_no_outliers.shape[0]}")

	# 4. ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì…‹)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
	X_train_no_outliers, X_test_no_outliers, y_train_no_outliers, y_test_no_outliers = 	train_test_split(
	    X_no_outliers, y_no_outliers, test_size=0.3, random_state=42)

	# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ì´ìƒì¹˜ ì œê±° ì „
	model = LogisticRegression(max_iter=200)
	model.fit(X_train, y_train)
	y_pred = model.predict(X_test)
	accuracy_before = accuracy_score(y_test, y_pred)

	# (2) ì´ìƒì¹˜ ì œê±° í›„
	model_no_outliers = LogisticRegression(max_iter=200)
	model_no_outliers.fit(X_train_no_outliers, y_train_no_outliers)
	y_pred_no_outliers = model_no_outliers.predict(X_test_no_outliers)
	accuracy_after = accuracy_score(y_test_no_outliers, y_pred_no_outliers)

	# 6. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy before removing outliers: {accuracy_before:.2f}")
	print(f"Accuracy after removing outliers: {accuracy_after:.2f}")

<br>

	Outliers detected:
 		   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm) 
	15                5.7               4.4                1.5               0.4   
	32                5.2               4.1                1.5               0.1   
	33                5.5               4.2                1.4               0.2   
	60                5.0               2.0                3.5               1.0   

	                           Reason  
	15  sepal width (cm) out of range  
	32  sepal width (cm) out of range  
	33  sepal width (cm) out of range  
	60  sepal width (cm) out of range  

	Original data size: 150
	Data size after removing outliers: 146
	Number of outliers detected: 4

	Accuracy before removing outliers: 1.00
	Accuracy after removing outliers: 0.95

<br>

## [1-7] ë°ì´í„° ì¤‘ë³µ ì œê±°(Data Deduplication)
â–£ ì •ì˜ : ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ íƒì§€í•˜ê³  ì œê±°í•˜ì—¬ ë°ì´í„°ì…‹ì˜ ì¼ê´€ì„±ê³¼ ì •í™•ì„±ì„ ë†’ì´ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : ì¤‘ë³µ ë°ì´í„°ëŠ” ë¶„ì„ ë° ëª¨ë¸ í•™ìŠµì— í¸í–¥ì„ ì´ˆë˜, ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì—¬ ì²˜ë¦¬ ì†ë„ì™€ ì €ì¥ì†Œ ë¹„ìš©ì„ ì ˆê°, ì¼ê´€ëœ ë°ì´í„°ì…‹ì„ í™•ë³´í•˜ì—¬ ë¶„ì„ ì‹ ë¢°ì„±ì„ ë†’ì„<br>
â–£ ì£¼ìš” ê¸°ë²• : ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì¤‘ë³µ ì œê±°, ê³ ìœ  ì‹ë³„ìë¥¼ í™œìš©í•œ ì¤‘ë³µ íƒì§€(í‚¤ ê¸°ë°˜ í•„í„°ë§), í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°(Jaccard, Cosine Similarity), MinHashë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ì„± íƒì§€, SQL ì¿¼ë¦¬ë¥¼ í™œìš©í•œ ì¤‘ë³µ ì œê±°<br>
â–£ ì¥ì  : ë°ì´í„° í¬ê¸° ê°ì†Œë¡œ ì²˜ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ, ì¤‘ë³µ ë°ì´í„°ë¡œ ì¸í•œ ì™œê³¡ ê°ì†Œ, ë°ì´í„° ì¼ê´€ì„±ê³¼ ì‹ ë¢°ì„± í™•ë³´<br>
â–£ ë‹¨ì  : ìœ ì‚¬ì„± ê¸°ì¤€ì„ ì„¤ì •í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ íƒì§€ ë° ì œê±° ê³¼ì •ì´ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìŒ, ì˜ëª»ëœ ì œê±°ëŠ” ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ì†ì‹¤ì‹œí‚¬ ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ë° ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ í™œìš©, ë°ì´í„°ì…‹ í¬ê¸°ì— ë¯¼ê°í•œ ì•Œê³ ë¦¬ì¦˜(KNN, êµ°ì§‘í™”)<br>

<br>

## [1-8] ë°ì´í„° ë³€í™˜(Data Transformation)
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜, ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ë°ì´í„°ë¥¼ ë³€í˜•<br>
â–£ í•„ìš”ì„± : ë°ì´í„° ë¶„í¬ë¥¼ ì¡°ì •í•˜ì—¬ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ ìµœì í™”, ì…ë ¥ ë°ì´í„°ê°€ ì•Œê³ ë¦¬ì¦˜ì˜ ìš”êµ¬ì‚¬í•­ì— ë§ë„ë¡ ì¤€ë¹„, ì´ìƒì¹˜, ë¶ˆê· í˜• ë°ì´í„° ë“±ì˜ ì˜í–¥ì„ ìµœì†Œí™”<br>
â–£ ì£¼ìš” ê¸°ë²• : ë¡œê·¸ ë³€í™˜(ë¹„ëŒ€ì¹­ ë¶„í¬ë¥¼ ì •ê·œ ë¶„í¬ë¡œ ì¡°ì •), ìŠ¤ì¼€ì¼ë§(Min-Max, Standardization), ë²”ì£¼í˜• ë³€í™˜(ì›-í•« ì¸ì½”ë”©, ë¼ë²¨ ì¸ì½”ë”©), ì°¨ì› ì¶•ì†Œ(PCA, t-SNE)<br>
â–£ ì¥ì  : ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ íš¨ê³¼ ì¦ê°€, ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ë³´, í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì—¬ ë°ì´í„° ì´í•´ë„ í–¥ìƒ<br>
â–£ ë‹¨ì  : ì ì ˆí•œ ë³€í™˜ ê¸°ë²• ì„ íƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ, ì›ë³¸ ë°ì´í„°ì˜ ì˜ë¯¸ê°€ ì™œê³¡ë  ê°€ëŠ¥ì„±, ê³ ì°¨ì› ë°ì´í„°ì—ì„œëŠ” ë³€í™˜ ë¹„ìš© ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : íšŒê·€ ëª¨ë¸, ë”¥ëŸ¬ë‹, PCA, SVM ë“±<br>

<br>

## [1-9] íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)
â–£ ì •ì˜ : ëª¨ë¸ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ê±°ë‚˜ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ìƒì„±í•˜ê±°ë‚˜ ë³€í™˜, ì„ íƒ ë“±ì˜ ì‘ì—…<br>
â–£ í•„ìš”ì„± : ê³ í’ˆì§ˆ íŠ¹ì„±ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ, ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ ë°˜ì˜í•˜ì—¬ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€, íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±° ê°€ëŠ¥<br>
â–£ ì£¼ìš” ê¸°ë²• : ìˆ˜í•™ì  ì¡°í•©ê³¼ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ íŒŒìƒ íŠ¹ì„± ìƒì„±, ë¡œê·¸ ë³€í™˜ê³¼ ë‹¤í•­ì‹ ë³€í™˜ìœ¼ë¡œ íŠ¹ì„± ë³€í™˜, ëª¨ë¸ ê¸°ë°˜ ì„ íƒ(Lasso, XGBoost), ì¤‘ìš”ë„ í‰ê°€(LIME, SHAP)* ë° ì°¨ì› ì¶•ì†Œ(PCA, t-SNE)ë¡œ íŠ¹ì„± ì„ íƒ<br>
â–£ ì¥ì  : ëª¨ë¸ ì„±ëŠ¥ì„ í° í­ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥, ë„ë©”ì¸ ì§€ì‹ì„ ë°˜ì˜í•˜ì—¬ ë” ë‚˜ì€ í•´ì„ ê°€ëŠ¥, ì¤‘ìš”í•˜ì§€ ì•Šì€ íŠ¹ì„±ì„ ì œê±°í•´ í•™ìŠµ ì†ë„ í–¥ìƒ<br>
â–£ ë‹¨ì  : ë†’ì€ ë„ë©”ì¸ ì§€ì‹ ìš”êµ¬, ì‹œê°„ê³¼ ìì› ì†Œëª¨, ì˜ëª»ëœ íŠ¹ì„± ìƒì„±ì€ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì„ í˜• ëª¨ë¸, ì˜ì‚¬ê²°ì • ë‚˜ë¬´, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë”¥ëŸ¬ë‹ ë“± ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜<br>
**LIM(Local Interpretable Model-agnostic Explanations)** : ëª¨ë¸ì— ê´€ê³„ì—†ì´ ë¡œì»¬(Local) ë‹¨ìœ„ì—ì„œ íŠ¹ì • ì˜ˆì¸¡ì— ëŒ€í•´ ëª¨ë¸ì´ ì™œ ê·¸ì™€ ê°™ì€ ê²°ì •ì„ ë‚´ë ¸ëŠ”ì§€ë¥¼ ì„¤ëª…(ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ê° íŠ¹ì„±(feature)ì´ ì˜ˆì¸¡ê°’ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ ë‚˜íƒ€ëƒ„)<br>
**SHAP(SHapley Additive exPlanations)** : ê° íŠ¹ì„±ì´ ì˜ˆì¸¡ê°’ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ê³µì •í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ, ëª¨ë“  íŠ¹ì„± ì¡°í•©ì—ì„œì˜ í‰ê·  ê¸°ì—¬ë„ë¥¼ ê³„ì‚°<br>
<br>

## [1-10] ì •ë³´ ë³‘í•©(Data Fusion)
â–£ ì •ì˜ : ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ê²°í•©í•˜ì—¬ ë‹¨ì¼í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ë°ì´í„°ì…‹ì„ ìƒì„±<br>
â–£ í•„ìš”ì„± : ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ë” í’ë¶€í•œ ì •ë³´ë¥¼ ì œê³µ, ë°ì´í„° ì¤‘ë³µ ë° ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•˜ì—¬ ë¶„ì„ ê°€ëŠ¥ì„±ì„ ë†’ì„<br>
â–£ ì£¼ìš” ê¸°ë²• : ì„œë¡œ ë‹¤ë¥¸ ìŠ¤í‚¤ë§ˆ ê°„ ë§¤í•‘ ì •ì˜(Data Mapping), ë™ì¼í•œ ë°ì´í„° í¬ì¸íŠ¸ ì¤‘ë³µ ì œê±°(Deduplication), ë‹¤ì–‘í•œ í˜•ì‹ì„ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì •ê·œí™”(Normalization)<br>
â–£ ì¥ì  : ë°ì´í„° í™œìš© ê°€ëŠ¥ì„± ì¦ëŒ€, ë³µí•©ì ì¸ ë¬¸ì œì— ëŒ€í•œ ë‹¤ê°ì  ë¶„ì„ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ë°ì´í„° ì†ŒìŠ¤ ê°„ ì¼ì¹˜ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ, í†µí•© ê·œì¹™ ì„¤ì •ê³¼ ë³€í™˜ ê³¼ì •ì´ ë³µì¡<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë°ì´í„° í†µí•© í›„ ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì— ì ìš© ê°€ëŠ¥<br>

<br>

---
  
# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™” : ê³¼ì í•© ë°©ì§€(Overfitting Prevention)
â–£ ì •ì˜ : ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ì ì‘í•˜ì§€ ì•Šë„ë¡ ì œì–´í•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œë„ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ë‹¤ì–‘í•œ ê¸°ë²•ì˜ ì¡°í•©<br>
â–£ í•„ìš”ì„± : ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë‚˜ ë¶ˆí•„ìš”í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë‚˜ ì‹¤ì „ ë°ì´í„°ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ë³´ì¥<br>
â–£ ì£¼ìš” ê¸°ë²• : ë°ì´í„° ê´€ë ¨(Data Augmentation, Cross Validation), ëª¨ë¸ ê´€ë ¨(Model Simplification, Regularization, Dropout), í›ˆë ¨ ê´€ë ¨(Early Stopping)<br>
â–£ ì¥ì  : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œì˜ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ë³´, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ, ì˜ˆì¸¡ ëª¨ë¸ì˜ ì‹ ë¢°ë„ ì¦ê°€<br>
â–£ ë‹¨ì  : ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì´ ê³¼ë„í•˜ê²Œ ì ìš©ë˜ë©´ ê³¼ì†Œì í•©(Underfitting), ìµœì ì˜ ì„¤ì •ì„ ì°¾ê¸° ìœ„í•œ ì¶”ê°€ì ì¸ ì‹¤í—˜ê³¼ ì¡°ì •ì´ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

<br>

## [2-1] ì •ê·œí™”(L1, L2 Regularization)
â–£ ì •ì˜ : ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜ì— íŒ¨ë„í‹°ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ë¥¼ ì œì–´(L1 ì •ê·œí™”: ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©, L2 ì •ê·œí™”: ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©)<br>
â–£ í•„ìš”ì„± : ëª¨ë¸ì´ ë¶ˆí•„ìš”í•˜ê²Œ í° ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ì—¬ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€<br>
â–£ ì£¼ìš” ê¸°ë²• : L1 ì •ê·œí™”(Lasso Regression), L2 ì •ê·œí™”(Ridge Regression), L1ê³¼ L2 í˜¼í•©(Elastic Net)<br>
â–£ ì¥ì  : L1ì€ í¬ì†Œ ëª¨ë¸(sparse model)ì„ ìƒì„±í•˜ì—¬ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì„ íƒí•˜ëŠ” ë° ìœ ìš©, L2ëŠ” ê³¼ë„í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì—¬ ëª¨ë¸ ì•ˆì •ì„± í–¥ìƒ<br>
â–£ ë‹¨ì  : ê³¼ì†Œì í•© ê°€ëŠ¥ì„±, ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì •í•˜ê¸° ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°(Î») ì„ íƒ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì„ í˜• íšŒê·€(Linear Regression), ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression), ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM), ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬<br>

<br>

## [2-2] ì¡°ê¸° ì¢…ë£Œ(Early Stopping)
â–£ ì •ì˜ : í•™ìŠµ ì¤‘ ê²€ì¦ ì„¸íŠ¸ì˜ ì„±ëŠ¥ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•ŠëŠ” ì‹œì ì—ì„œ í•™ìŠµì„ ì¤‘ë‹¨<br>
â–£ í•„ìš”ì„± : í•™ìŠµì„ ë„ˆë¬´ ì˜¤ë˜ ì§„í–‰í•  ê²½ìš° ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©ë  ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•¨<br>
â–£ ì£¼ìš” ê¸°ë²• : ê²€ì¦ ì†ì‹¤ ëª¨ë‹ˆí„°ë§(ê²€ì¦ ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° ì¤‘ë‹¨), Patience ì„¤ì •(íŠ¹ì • ì—í¬í¬ ë™ì•ˆ í–¥ìƒì´ ì—†ì„ ë•Œ ì¢…ë£Œ)<br>
â–£ ì¥ì  : ê³¼ì í•© ë°©ì§€, ë¶ˆí•„ìš”í•œ í•™ìŠµ ì‹œê°„ ì ˆì•½<br>
â–£ ë‹¨ì  : ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ì„ ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ê±°ë‚˜, ìµœì ì˜ ì¢…ë£Œ ì‹œì  ê²°ì • ê³¤ë€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë”¥ëŸ¬ë‹ ëª¨ë¸, ì¼ë¶€ ë¨¸ì‹ ëŸ¬ë‹(Gradient Boosting) ë“± ì—í­(Epoch)*ì´ ê¸´ ê²½ìš°<br>
   * ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ì „ì²´ ë°ì´í„°ì…‹ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ëŠ” íšŸìˆ˜(ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìƒ˜í”Œì´ ëª¨ë¸ì— ì…ë ¥ë˜ì–´ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ë˜ëŠ” ê³¼ì •ì„ í•œë²ˆ ì™„ë£Œí•˜ëŠ” ê²ƒì´ 1 ì—í­)<br>

<br>

## [2-3] ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)
â–£ ì •ì˜ : ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ê²°í•©(ë°°ê¹…: ê° ëª¨ë¸ì˜ ë…ë¦½ì ì¸ í•™ìŠµ, ë¶€ìŠ¤íŒ…: ê° ëª¨ë¸ì´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ, ìŠ¤íƒœí‚¹: ì„œë¡œë‹¤ë¥¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ê²°ê³¼ ê²°í•©)<br>
â–£ í•„ìš”ì„± : ë‹¨ì¼ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ë°ì´í„°ì˜ ë‹¤ì–‘í•œ íŒ¨í„´ì„ ë” ì˜ í•™ìŠµ<br>
â–£ ì£¼ìš” ê¸°ë²• : ìŠ¤íƒœí‚¹, ë°°ê¹…(Random Forest), ë¶€ìŠ¤íŒ…(AdaBoost, Gradient Boosting, XGBoost, LightGBM)<br>
â–£ ì¥ì  : ë†’ì€ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥, ë‹¤ì–‘í•œ ë°ì´í„° ë° ëª¨ë¸ ìœ í˜•ì— ì ìš© ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš© ì¦ê°€, êµ¬í˜„ ë³µì¡ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜(ë¶„ë¥˜, íšŒê·€ ë“±)<br>

<br>

## [2-4] ëª¨ë¸ í•´ì„ì„± (Model Interpretability)
â–£ ì •ì˜ : ëª¨ë¸ì´ ë‚´ë¦° ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•´ ì„¤ëª… ê°€ëŠ¥í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë¸”ë™ë°•ìŠ¤ ëª¨ë¸(ë”¥ëŸ¬ë‹, ì•™ìƒë¸”)ì˜ íˆ¬ëª…ì„± í™•ë³´, ë¹„ì¦ˆë‹ˆìŠ¤ë‚˜ ì˜ë£Œ ë“± ê³ ìœ„í—˜ ë¶„ì•¼ì—ì„œ ì‹ ë¢° í™•ë³´<br>
â–£ ì£¼ìš” ê¸°ë²• : LIME(íŠ¹ì • ì˜ˆì¸¡ ë¡œì»¬ ë‹¨ìœ„ì—ì„œ ë‹¨ìˆœ ëª¨ë¸ë¡œ ê·¼ì‚¬), SHAP(íŠ¹ì§•ë³„ ê¸°ì—¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ˆì¸¡ì— ëŒ€í•œ ê¸€ë¡œë²Œ ë° ë¡œì»¬ í•´ì„ ì œê³µ)<br>
â–£ ì¥ì  : ì‚¬ìš©ì ì‹ ë¢° í™•ë³´, ëª¨ë¸ ë””ë²„ê¹… ë° ê°œì„  ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ, ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ì—ì„œ ë³µì¡ì„± ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ (e.g., ì‹ ê²½ë§, ì•™ìƒë¸” í•™ìŠµ ëª¨ë¸)<br>

<br>

---

# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
## [3-1] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Hyperparameter Tuning)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©ìê°€ ì‚¬ì „ì— ì„¤ì •í•˜ëŠ” ë³€ìˆ˜ë¡œ, í•™ìŠµë¥ , ì •ê·œí™” ê°•ë„, ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ ìµœëŒ€ ê¹Šì´ ë“± ë“±ì„ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ê³¼ì í•©, ê³¼ì†Œì í•© ë˜ëŠ” í•™ìŠµ ì†ë„ ì €í•˜ê°€ ë°œìƒ<br>
â–£ ì¥ì  : ëª¨ë¸ ì„±ëŠ¥ ê·¹ëŒ€í™” ê°€ëŠ¥, ë‹¤ì–‘í•œ ë°ì´í„°ì™€ ë¬¸ì œ ìœ í˜•ì— ì ìš© ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ê³ , ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë©°, íƒìƒ‰ ê³µê°„ì´ ì»¤ì§ˆìˆ˜ë¡ ë³µì¡ë„ ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

<br>

## [3-2] ê·¸ë¦¬ë“œ ì„œì¹˜(Grid Search)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ëª¨ë“  ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ë°©ë²•<br>
â–£ í•„ìš”ì„± : ì²´ê³„ì ìœ¼ë¡œ ëª¨ë“  ì¡°í•©ì„ íƒìƒ‰í•˜ë¯€ë¡œ ìµœì ì˜ ì„¤ì •ì„ ì°¾ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ<br>
â–£ ì¥ì  : ê°„ë‹¨í•˜ê³  ì§ê´€ì ì´ë©° êµ¬í˜„ì´ ìš©ì´í•˜ë©°, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ì „ ë²”ìœ„ë¥¼ íƒìƒ‰ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : íƒìƒ‰ ê³µê°„ì´ ì»¤ì§ˆìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ê³¼ ì‹œê°„ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ë©°, ë¶ˆí•„ìš”í•œ ì¡°í•©ê¹Œì§€ ê³„ì‚°í•  ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì´ ë¹„êµì  ì‘ì€ ë¬¸ì œì— ì í•©, Scikit-learnì—ì„œ GridSearchCV í•¨ìˆ˜ ì œê³µ<br>

<br>

## [3-3] ëœë¤ ì„œì¹˜(Random Search)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ë“¤ì„ ì„ì˜ë¡œ ì„ íƒí•˜ì—¬ ìµœì ì˜ ì¡°í•©ì„ íƒìƒ‰<br>
â–£ í•„ìš”ì„± : ê·¸ë¦¬ë“œ ì„œì¹˜ë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë†’ì´ë©°, ë” í° íƒìƒ‰ ê³µê°„ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ íƒìƒ‰<br>
â–£ ì¥ì  : ê³„ì‚° íš¨ìœ¨ì„± ì¦ëŒ€, ì ì€ ê³„ì‚°ìœ¼ë¡œë„ ë†’ì€ ì„±ëŠ¥ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ê°€ëŠ¥ì„±<br>
â–£ ë‹¨ì  : ìµœì ì˜ ì¡°í•©ì„ ë°˜ë“œì‹œ ì°¾ì§€ ëª»í•  ê°€ëŠ¥ì„±, íƒìƒ‰ ê²°ê³¼ê°€ ì‹¤í–‰ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì— ì í•©, Scikit-learnì—ì„œ RandomizedSearchCV í•¨ìˆ˜ ì œê³µ<br>

<br>

## [3-4] ë² ì´ì¦ˆ ìµœì í™”(Bayesian Optimization)
â–£ ì •ì˜ : ì´ì „ íƒìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìµœì ê°’ì„ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ë¥  ëª¨ë¸(ì˜ˆ: ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤)ì„ í™œìš©í•˜ì—¬ íƒìƒ‰<br>
â–£ í•„ìš”ì„± : ê³„ì‚° ë¹„ìš©ì´ ë†’ì€ ë¬¸ì œì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : íš¨ìœ¨ì ì¸ íƒìƒ‰ìœ¼ë¡œ ê³„ì‚° ìì›ì„ ì ˆì•½, ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´<br>
â–£ ë‹¨ì  : êµ¬í˜„ ë° ì´í•´ê°€ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë©°, íƒìƒ‰ ì´ˆê¸°ì—ëŠ” ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ê³„ì‚° ë¹„ìš©ì´ ë†’ì€ ë¨¸ì‹ ëŸ¬ë‹(ëœë¤ í¬ë ˆìŠ¤íŠ¸) ë° ë”¥ëŸ¬ë‹ ëª¨ë¸<br>

<br>

## [3-5] í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìë™í™”(Automated Hyperparameter Tuning)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì •ì„ ìë™í™”í•˜ì—¬ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ìˆ˜ë™ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ë° ë“œëŠ” ì‹œê°„ê³¼ ë…¸ë ¥ì„ ì¤„ì´ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : íš¨ìœ¨ì ì´ê³  í¸ë¦¬í•˜ë©° ë°˜ë³µ ê°€ëŠ¥, ì´ˆë³´ìë„ ê³ ì„±ëŠ¥ ëª¨ë¸ êµ¬í˜„ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ë„êµ¬ ë° ì•Œê³ ë¦¬ì¦˜ì˜ ì œí•œ ì‚¬í•­ì— ë”°ë¼ ìµœì  ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìœ¼ë©°, ë„êµ¬ ì‚¬ìš©ì— ë”°ë¥¸ ë¹„ìš© ë°œìƒ ê°€ëŠ¥<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ AutoMLì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸<br>

<br>

## [3-6] AutoML(Automated Machine Learning) í™œìš©
â–£ ì •ì˜ : AutoML (Automated Machine Learning)ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œì˜ ì „ ê³¼ì •ì„ ìë™í™”í•˜ëŠ” ê¸°ìˆ  ë˜ëŠ” ë„êµ¬<br>
â–£ í•„ìš”ì„± : ì „ë¬¸ê°€ ë¶€ì¡± ë¬¸ì œ í•´ê²°, ì‹œê°„ê³¼ ìì› ì ˆì•½, ìµœì ì˜ ëª¨ë¸ íƒìƒ‰, ë°ì´í„° ì¦ê°€ì™€ ë³µì¡ì„± ëŒ€ì‘, ì´ˆê¸° í”„ë¡œí† íƒ€ì´í•‘<br>
â–£ ì£¼ìš”ê¸°ë²•  : ìë™ ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ ì„ íƒ (Algorithm Selection), í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, íŒŒì´í”„ë¼ì¸ ìë™í™” (Pipeline Optimization), ë©”íƒ€ëŸ¬ë‹ (Meta-Learning), ì‹ ê²½ ì•„í‚¤í…ì²˜ ê²€ìƒ‰ (Neural Architecture Search, NAS), ì•™ìƒë¸” ê¸°ë²•<br>
â–£ ì¥ì  : ì‚¬ìš© ìš©ì´ì„±, ì‹œê°„ ì ˆì•½, íš¨ìœ¨ì„±, ë²”ìš©ì„±, ì„±ëŠ¥ ìµœì í™”<br>
â–£ ë‹¨ì  : ì„¤ëª… ê°€ëŠ¥ì„± ë¶€ì¡±, ì œí•œëœ ì»¤ìŠ¤í„°ë§ˆì´ì§•, ê³„ì‚° ë¹„ìš©, ê¸°ìˆ ì  ì œì•½, ë°ì´í„° ì „ì²˜ë¦¬ í•œê³„<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, ì•™ìƒë¸”, ë”¥ëŸ¬ë‹, ì‹ ê²½ ì•„í‚¤í…ì²˜ ê²€ìƒ‰ (NAS), ì‹œê³„ì—´ ì˜ˆì¸¡, ê°•í™”í•™ìŠµ<br>

<br>

---

# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
## [4-1] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Learning Rate Scheduling)
â–£ ì •ì˜ : í•™ìŠµë¥ (Learning Rate)ì„ í•™ìŠµ ê³¼ì • ì¤‘ì— ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ìµœì í™” ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•(Step Decay: ì¼ì • ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥  ê°ì†Œ, Exponential Decay: í•™ìŠµë¥ ì„ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ, Cosine Annealing: í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ë‚®ì¶”ëŠ” ë°©ì‹)<br>
â–£ í•„ìš”ì„± : ê³ ì •ëœ í•™ìŠµë¥ ì€ ì´ˆê¸°ì™€ í›„ë°˜ë¶€ í•™ìŠµì— ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ˆê¸°ì—ëŠ” ë¹ ë¥¸ í•™ìŠµ, í›„ë°˜ë¶€ì—ëŠ” ì•ˆì •ì ì¸ ìˆ˜ë ´ì´ í•„ìš”<br>
â–£ ì¥ì  : í•™ìŠµ ì†ë„ì™€ ìµœì í™” ì•ˆì •ì„±ì„ ëª¨ë‘ í™•ë³´ ê°€ëŠ¥í•˜ë©°, ê³¼ì í•© ë°©ì§€ì— ë„ì›€<br>
â–£ ë‹¨ì  : ì ì ˆí•œ ìŠ¤ì¼€ì¤„ì„ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±, ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì£¼ë¡œ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ SGD, Adamê³¼ ê°™ì€ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸<br>

<br>

## [4-2] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Initialization)
â–£ ì •ì˜ : ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ê°’ì„ í•™ìŠµ ì´ˆê¸°ì— ì ì ˆíˆ ì„¤ì •í•˜ì—¬ í•™ìŠµ ê³¼ì •ì„ ì•ˆì •í™”í•˜ëŠ” ê¸°ë²•(Xavier Initialization: ì…ë ¥ ë° ì¶œë ¥ ë…¸ë“œ ìˆ˜ì— ê¸°ë°˜, He Initialization: ReLU ê³„ì—´ í™œì„±í™” í•¨ìˆ˜ì— ì í•©)<br>
â–£ í•„ìš”ì„± : ì˜ëª»ëœ ì´ˆê¸°í™”ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì´ë‚˜ í­ë°œ(Exploding Gradient)ì„ ìœ ë°œí•  ìˆ˜ ìˆìŒ.<br>
â–£ ì¥ì  : í•™ìŠµ ì´ˆê¸° ì•ˆì •ì„± í–¥ìƒ, ë¹ ë¥¸ ìˆ˜ë ´ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ì¼ë¶€ ì•Œê³ ë¦¬ì¦˜ì—ì„œ íŠ¹ì • ì´ˆê¸°í™” ì „ëµì´ ë” ì í•©í•˜ë¯€ë¡œ ì ì ˆí•œ ì„ íƒ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ëª¨ë¸ (íŠ¹íˆ ì‹¬ì¸µ ì‹ ê²½ë§)<br>

<br>

## [4-3] í™œì„±í™” í•¨ìˆ˜ ì„ íƒ(Activation Function Selection)
â–£ ì •ì˜ : ë‰´ëŸ°ì˜ ì¶œë ¥ ê°’ì„ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ íŒ¨í„´ì„ ëŠ˜ë¦¬ëŠ” ì—­í• ì„ í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •<br>
(Sigmoid: [0, 1] ì¶œë ¥, ì´ì§„ ë¶„ë¥˜ì—ì„œ ì‚¬ìš©, ReLU: ë¹„ì„ í˜•ì„± ì œê³µ, ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì™„í™”, Leaky ReLU: ReLUì˜ ë³€í˜•, ìŒìˆ˜ êµ¬ê°„ ê¸°ìš¸ê¸° ë³´ì •, Softmax: ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ ì¶œë ¥)<br>
â–£ í•„ìš”ì„± : ì ì ˆí•œ í™œì„±í™” í•¨ìˆ˜ ì„ íƒì€ í•™ìŠµ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹¨<br>
â–£ ì¥ì  : ë¹„ì„ í˜•ì„±ì„ ë„ì…í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²° ê°€ëŠ¥í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ê³¼ ë¬¸ì œì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ì˜ëª»ëœ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ì‹œ í•™ìŠµ ì†ë„ ì €í•˜ë‚˜ ì„±ëŠ¥ ì•…í™”, íŠ¹ì • í•¨ìˆ˜ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥(Sigmoid, Tanh)<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

<br>

## [4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
### [4-4-1] Adam (Adaptive Moment Estimation)
â–£ ì •ì˜: Stochastic Gradient Descent(SGD)ì˜ í™•ì¥ìœ¼ë¡œ ëª¨ë©˜í…€ê³¼ ì ì‘ í•™ìŠµë¥ (Adaptive Learning Rate)ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜. ë”¥ëŸ¬ë‹ì—ì„œ ë„ë¦¬ ì‚¬ìš©(ê³¼ê±°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ê°€ì†í™”í•˜ê³  ì•ˆì •ì„±ì„ ë†’ì„)<br>
â–£ í•„ìš”ì„± : ë³µì¡í•œ ë¹„ì„ í˜• í•¨ìˆ˜ì—ì„œ ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì´ ìˆ˜ë ´í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì£¼ìš” ê¸°ë²• : ê·¸ë˜ë””ì–¸íŠ¸ì˜ ê³¼ê±° ë°©í–¥(ëˆ„ì )ì„ ì°¸ê³ í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ ê°€ì†í™”(Momentum), ê° ë§¤ê°œë³€ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ì— ë”°ë¼ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ëŠ” ì ì‘ í•™ìŠµë¥  (Adaptive Learning Rate), ì´ë™ í‰ê·  (Exponential Moving Averages)<br>
â–£ ì¥ì  : í•™ìŠµë¥  ì¡°ì •ì´ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§, ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„, ìŠ¤íŒŒìŠ¤ ë°ì´í„° ì²˜ë¦¬ì— íš¨ê³¼ì , ê³¼ê±° ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ í™œìš©í•´ ì§„ë™(oscillation) ê°ì†Œ<br>
â–£ ë‹¨ì  : í•™ìŠµë¥ ì´ ì ì  ì‘ì•„ì ¸, ìˆ˜ë ´ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ, ì¶”ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • í•„ìš”, ê³¼ì í•© ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ì—ì„œ ì£¼ë¡œ ì‚¬ìš©(CNN, RNN, GAN, Transformer ë“±)<br>

### [4-4-2] SGD(Stochastic Gradient Descent)
â–£ ì •ì˜ : ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì˜ ë³€í˜•ìœ¼ë¡œ, ê° ë°°ì¹˜(batch) ë˜ëŠ” ìƒ˜í”Œì— ëŒ€í•´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„± : ë°ì´í„°ê°€ í´ìˆ˜ë¡ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì†ë„ë¥¼ ê°œì„ <br>
â–£ ì£¼ìš” ê¸°ë²• : ë°°ì¹˜ì— ëŒ€í•´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³  ì¦‰ê°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸, ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ì¼ë¶€ ë°ì´í„°(ë°°ì¹˜)ë¥¼ í™œìš©í•œ í™•ë¥ ì  ì ‘ê·¼<br>
â–£ ì¥ì  : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° íš¨ìœ¨ì„±, ê°„ë‹¨í•œ êµ¬í˜„ìœ¼ë¡œ ëª¨ë¸ ì¼ë°˜í™”(generalization)ì— ìœ ë¦¬<br>
â–£ ë‹¨ì  : ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì €ì  ì£¼ë³€ì—ì„œ ì§„ë™(oscillation : ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ì—ì„œ ìµœì €ì (ìµœì ê°’, Optimum) ì£¼ë³€ì—ì„œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë°©í–¥ì´ ê³„ì† ë°”ë€ŒëŠ” í˜„ìƒ)ì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°. í•™ìŠµë¥  ì„¤ì •ì´ ë¯¼ê°í•˜ê³ , ëŠë¦° ìˆ˜ë ´ ì†ë„<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹: Logistic Regression, Linear Regression, CNN, RNN<br>

### [4-4-3] RMSprop
â–£ ì •ì˜ : ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ í•™ìŠµë¥  ê°ì†Œ(Adaptive Learning Rate)ì™€ ë£¨íŠ¸ í‰ê·  ì œê³±(Root Mean Square Propagation) ê°œë…ì„ í™œìš©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„± : í•™ìŠµ ê³¼ì •ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í¬ê¸°ê°€ ì§€ë‚˜ì¹˜ê²Œ í¬ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì£¼ìš” ê¸°ë²• : ê° ë§¤ê°œë³€ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •, ê·¸ë˜ë””ì–¸íŠ¸ì˜ ì œê³± í‰ê· ì„ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•´ í•™ìŠµë¥ ì„ ì—…ë°ì´íŠ¸<br>
â–£ ì¥ì  : ì§„ë™(oscillation) ê°ì†Œ, í•™ìŠµë¥ ì´ ìë™ìœ¼ë¡œ ì¡°ì •ë˜ì–´ ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢ì€ ê³¨ì§œê¸°ë¥¼ ë¹ ë¥´ê²Œ íƒìƒ‰, SGDë³´ë‹¤ ì•ˆì •ì <br>
â–£ ë‹¨ì  : ì¥ê¸°ì ìœ¼ë¡œëŠ” ì ì‘ í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì•„ì ¸ í•™ìŠµì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŒ, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : RNN ë° LSTM ê°™ì€ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ëª¨ë¸, CNN ê¸°ë°˜ ëª¨ë¸<br>

<br>

## [4-5] ì „ì´ í•™ìŠµ(Transfer Learning)
â–£ ì •ì˜ : ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ìƒˆë¡œìš´ ë¬¸ì œì— ì¬ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•(Pre-trained Model Utilization)<br>
â–£ í•„ìš”ì„± : ë°ì´í„° ë¶€ì¡± ìƒí™©ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì¥, í•™ìŠµ ì‹œê°„ì„ í¬ê²Œ ë‹¨ì¶•<br>
â–£ ì¥ì  : ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ ê°€ëŠ¥, ë¹ ë¥¸ í•™ìŠµê³¼ ë†’ì€ ì´ˆê¸° ì„±ëŠ¥<br>
â–£ ë‹¨ì  : ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë¬¸ì œì— ìµœì í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ì‚¬ì „ í•™ìŠµëœ ë°ì´í„°ì…‹ê³¼ ë„ë©”ì¸ ì°¨ì´ê°€ í´ ê²½ìš° ì„±ëŠ¥ ì €í•˜<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì´ë¯¸ì§€ ì²˜ë¦¬(CNN), ìì—°ì–´ ì²˜ë¦¬(Transformer, GPT)<br>

<br>

## [4-6] ëª¨ë¸êµ¬ì¡° ìµœì í™”(Model Architecture Optimization)
â–£ ì •ì˜ : ëª¨ë¸ì˜ êµ¬ì¡°(ë ˆì´ì–´ ìˆ˜, ë‰´ëŸ° ìˆ˜, ì—°ê²° ë°©ì‹ ë“±)ë¥¼ ìµœì í™”í•˜ì—¬ í•™ìŠµ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : ë³µì¡í•œ ëª¨ë¸ êµ¬ì¡°ëŠ” ê³¼ì í•© ìœ„í—˜ ì¦ê°€, ë‹¨ìˆœí•œ êµ¬ì¡°ëŠ” í‘œí˜„ë ¥ì´ ë¶€ì¡±í•˜ë¯€ë¡œ ì ì ˆí•œ ê· í˜• í•„ìš”<br>
â–£ ì¥ì  : ë°ì´í„°ì™€ ë¬¸ì œì— ì í•©í•œ ëª¨ë¸ ì„¤ê³„ ê°€ëŠ¥, ê³¼ì í•© ìœ„í—˜ ê°ì†Œ<br>
â–£ ë‹¨ì  : ì„¤ê³„ì— ë§ì€ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ ì†Œëª¨, ìë™í™” ë„êµ¬ ì‚¬ìš© ì‹œ ë†’ì€ ê³„ì‚° ë¹„ìš© ë°œìƒ ê°€ëŠ¥<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ëª¨ë¸ (íŠ¹íˆ ì‹ ê²½ë§)<br>

<br>

## [4-7] ì˜¨ë¼ì¸ í•™ìŠµ(Online Learning)
â–£ ì •ì˜ : ì ì§„ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ë©° ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë°ì´í„°ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê±°ë‚˜, ì €ì¥ ê³µê°„ì´ ì œí•œì ì¸ ê²½ìš°<br>
â–£ ì¥ì  : ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ<br>
â–£ ë‹¨ì  : ì˜ëª»ëœ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ëª¨ë¸ì— ì¦‰ì‹œ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµ ê³¼ì • ì¶”ì  ë° ë””ë²„ê¹…ì´ ì–´ë ¤ì›€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ëª¨ë¸ (ì˜ˆ: ì˜¨ë¼ì¸ ì¶”ì²œ ì‹œìŠ¤í…œ, ì‹¤ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸), SGD ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜<br>
Scikit-learnì—ì„œ ì£¼ë¡œ SGD ê¸°ë°˜ ëª¨ë¸(SGDClassifier, SGDRegressor, PassiveAggressiveClassifier, PassiveAggressiveRegressor), ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸, MiniBatchKMeansì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë“± ì œê³µ(partial_fit() ë©”ì„œë“œë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ì€ ì¼ë¶€ì— í•œì •)<br>

<br>

---

# [5] ì„±ëŠ¥ í–¥ìƒ
## [5-1] íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ(Feature Selection)
â–£ ì •ì˜ : ëª¨ë¸ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì‹ë³„í•˜ê³ , ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ìƒê´€ì„±ì´ ë‚®ì€ íŠ¹ì„±ì„ ì œê±°í•˜ëŠ” ê³¼ì •<br>
(Filter Methods: ìƒê´€ê³„ìˆ˜, ì¹´ì´ì œê³± ê²€ì • ë“±, Wrapper Methods: ìˆœì°¨ì „ì§„ì„ íƒ(SFS), ìˆœì°¨í›„ì§„ì œê±°(SBS), Embedded Methods: L1 ì •ê·œí™”, ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê¸°ë°˜ ì¤‘ìš”ë„)<br>
â–£ í•„ìš”ì„± : ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì€ í•™ìŠµ ì‹œê°„ì„ ì¦ê°€ì‹œí‚¤ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, íŠ¹ì„± ì„ íƒì€ ëª¨ë¸ ë‹¨ìˆœí™”ì™€ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬<br>
â–£ ì¥ì  : ê³¼ì í•© ìœ„í—˜ ê°ì†Œ, í•™ìŠµ ì‹œê°„ ë‹¨ì¶•, ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± ì¦ê°€<br>
â–£ ë‹¨ì  : íŠ¹ì„± ì„ íƒ ê³¼ì •ì´ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìœ¼ë©°, ì¤‘ìš”í•œ íŠ¹ì„±ì„ ë†“ì¹  ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ ê³ ì°¨ì› ë°ì´í„°ì…‹ì´ í¬í•¨ëœ ë¬¸ì œ<br>
Scikit-learnì—ì„œ Variance Threshold(íŠ¹ì„±ì˜ ë¶„ì‚°ì´ ë‚®ì€ íŠ¹ì„± ì œê±°), SelectKBest(ê°€ì¥ ì¤‘ìš”í•œ Kê°œì˜ íŠ¹ì„±ì„ ì„ íƒ), SelectPercentile(ìƒìœ„ n%ì˜ íŠ¹ì„±ì„ ì„ íƒ) í•¨ìˆ˜ ì œê³µ<br>

<br>

## [5-2] ì†ì‹¤í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•(Custom Loss Function)
â–£ ì •ì˜ : ë¬¸ì œì˜ íŠ¹ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ì„¤ê³„í•˜ê±°ë‚˜ ê¸°ì¡´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë³€í˜•í•˜ì—¬ ì‚¬ìš©<br>
â–£ í•„ìš”ì„± : ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ê°€ ë¬¸ì œì˜ ëª©í‘œë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í•  ê²½ìš°, ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : ë¬¸ì œì˜ ìš”êµ¬ì‚¬í•­ì— íŠ¹í™”ëœ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥, ì†ì‹¤ í•¨ìˆ˜ ìì²´ê°€ ëª¨ë¸ í•™ìŠµ ë°©í–¥ì„ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— ì„¸ë°€í•œ ì¡°ì • ê°€ëŠ¥<br>
â–£ ë‹¨ì  : êµ¬í˜„ì´ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë©°, ì†ì‹¤ í•¨ìˆ˜ ì„¤ê³„ ì˜¤ë¥˜ëŠ” í•™ìŠµ ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§ˆ ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ ë¹„ì •í˜• ë°ì´í„°, ë¶ˆê· í˜• ë°ì´í„° ë¬¸ì œì— ì í•©<br>
Scikit-learnì—ì„œëŠ” ì£¼ë¡œ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í†µí•©í•˜ë ¤ë©´ make_scorerë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì§€í‘œë¡œ ì •ì˜<br>

<br>

---

# [6] í•˜ë“œì›¨ì–´ ë° ì‹œìŠ¤í…œ ìµœì í™”
## [6-1] í•˜ë“œì›¨ì–´ ìµœì í™”(Hardware Optimization)
â–£ ì •ì˜ : ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ ê³¼ì •ì—ì„œ GPU, TPU ë“± í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë¥¼ í™œìš©í•˜ê±°ë‚˜, ë³‘ë ¬ ì²˜ë¦¬ì™€ ë¶„ì‚° í•™ìŠµì„ í†µí•´ ê³„ì‚°ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë”¥ëŸ¬ë‹ ë° ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ëª¨ë¸ì—ì„œ ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : í•™ìŠµ ì†ë„ ë° ì¶”ë¡  ì†ë„ í–¥ìƒ, ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ëª¨ë¸ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™•ì¥ì„± ì œê³µ<br>
â–£ ë‹¨ì  : í•˜ë“œì›¨ì–´ ì¥ë¹„ì˜ ì´ˆê¸° ë¹„ìš©ì´ ë†’ìœ¼ë©°, í•˜ë“œì›¨ì–´ ìµœì í™”ë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ ì„¤ì •ê³¼ ê¸°ìˆ  ì§€ì‹ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ëª¨ë¸ (CNN, RNN, Transformer ë“±), ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ë° ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜<br>

<br>

---

# [7] ëª¨ë¸ ê²€ì¦ ë° ë¹„êµ
## [7-1] ëª¨ë¸ ê²€ì¦(Model Validation)
â–£ ì •ì˜ : ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ì¼ë°˜í™”ë˜ëŠ”ì§€ í‰ê°€í•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : í•™ìŠµ ë°ì´í„°ì—ë§Œ ì˜ ì‘ë™í•˜ëŠ” ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê²ƒì„ë°©ì§€í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì£¼ìš”ê¸°ë²• : <br>
Hold-Out Method(ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°:ê²€ì¦ ë°ì´í„° = 80:20 ë˜ëŠ” 70:30 ë¹„ìœ¨ë¡œ êµ¬ë¶„)<br>
K-Fold Cross Validation(ë°ì´í„°ë¥¼ Kê°œì˜ í´ë“œë¡œ ë‚˜ëˆˆ ë’¤, ê° í´ë“œë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë²ˆê°ˆì•„ê°€ë©° ì‚¬ìš©)<br>
Stratified K-Fold Cross Validation(K-Fold Cross Validationì˜ ë³€í˜•ìœ¼ë¡œ, í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ê· ë“±í•˜ë„ë¡ ë°ì´í„°ë¥¼ ë‚˜ëˆ”)<br>
Leave-One-Out Cross Validation (LOOCV): ë°ì´í„°ì˜ ê° ìƒ˜í”Œì„ í•œ ë²ˆì”© ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©<br>
Time Series Validation: ì‹œê³„ì—´ ë°ì´í„°ì— ì í•©í•œ ë°©ë²•ìœ¼ë¡œ, ê³¼ê±° ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê³  ë¯¸ë˜ ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©<br>
Bootstrap Method: ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë³µì› ìƒ˜í”Œë§í•˜ì—¬ ì—¬ëŸ¬ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ê²€ì¦<br>
â–£ ì¥ì  : ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ, ê°ê´€ì  í‰ê°€, ê³¼ì í•©/ê³¼ì†Œì í•© í™•ì¸<br>
â–£ ë‹¨ì  : ì¶”ê°€ ë°ì´í„° í•„ìš”, ì‹œê°„ ë¹„ìš© ì¦ê°€, ê³¼ë„í•œ ìµœì í™” ìœ„í—˜<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì ìš©<br>

## [7-2] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ(Model Performance Comparison) 
â–£ ì •ì˜ : ì—¬ëŸ¬ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„±: ìµœì ì˜ ëª¨ë¸ ì„ íƒ, íš¨ìœ¨ì ì¸ ìì› í™œìš©<br>
â–£ ì£¼ìš” ê¸°ë²•: í‰ê°€ ì§€í‘œ ì‚¬ìš©, Cross Validation, í†µê³„ì  í…ŒìŠ¤íŠ¸(t-í…ŒìŠ¤íŠ¸ ë˜ëŠ” ANOVA), ì•™ìƒë¸” ë¹„êµ, ì‹œê°„ ë³µì¡ë„ ë° ìì› ì‚¬ìš© í‰ê°€, AutoML í™œìš©<br>
â–£ ì¥ì  : ê°€ì¥ ì í•©í•œ ëª¨ë¸ ì„ íƒ ê°€ëŠ¥, ê°ê´€ì ì¸ ë¹„êµ<br>
â–£ ë‹¨ì  : ì‹œê°„ê³¼ ìì› ì†Œëª¨, ëª¨ë¸ ë³µì¡ë„ ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜: ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, ë”¥ëŸ¬ë‹<br> 

<br>

---

# [8] ê¸°ìˆ  ë¶€ì±„ ê´€ë¦¬
## [8-1] ê¸°ìˆ  ë¶€ì±„(Technical Debt) ê´€ë¦¬
â–£ ì •ì˜ :  ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ê³¼ì •ì—ì„œ ë‹¨ê¸°ì ì¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë¹ ë¥´ê³  ë¹„íš¨ìœ¨ì ì¸ í•´ê²°ì±…ì„ ì„ íƒí•¨ìœ¼ë¡œì¨ ë°œìƒí•˜ëŠ” ë¯¸ë˜ì˜ ì¶”ê°€ ì‘ì—…<br>
â–£ í•„ìš”ì„± : ì¥ê¸°ì  ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ê°ì†Œ, ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥, ê¸°ìˆ  ìŠ¤íƒ ê°œì„ , íŒ€ ìƒì‚°ì„± í–¥ìƒ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¯¼ì²©ì„± í–¥ìƒ<br>
â–£ ì£¼ìš”ê¸°ë²• : ì½”ë“œ ë¦¬ë·°(Code Review), ë¦¬íŒ©í† ë§(Refactoring), ìë™í™” í…ŒìŠ¤íŠ¸(Automated Testing), CI/CD (Continuous Integration/Continuous Deployment), ê¸°ìˆ  ë¶€ì±„ ì¸¡ì • ë„êµ¬ ì‚¬ìš©, ë°ë¸Œì˜µìŠ¤(DevOps)ì™€ í˜‘ì—… ê°•í™”, ê¸°ìˆ  ë¶€ì±„ ëª©ë¡í™” (Debt Backlog), ì •ê¸°ì ì¸ ê¸°ìˆ  ìŠ¤íƒ ì—…ë°ì´íŠ¸<br>
â–£ ì¥ì  : ì¥ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ê°ì†Œ, ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ , ê°œë°œ ìƒì‚°ì„± í–¥ìƒ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¯¼ì²©ì„± ì¦ê°€, íŒ€ í˜‘ì—… ê°•í™”<br>
â–£ ë‹¨ì  : ì´ˆê¸° ì‹œê°„ê³¼ ë¹„ìš© ì¦ê°€, ìš°ì„ ìˆœìœ„ ì„¤ì •ì˜ ì–´ë ¤ì›€, ì™„ë²½í•œ ì œê±°ëŠ” ë¶ˆê°€ëŠ¥, ë‹¨ê¸°ì ì¸ ì†ë„ ì €í•˜, ì¸¡ì •ì˜ ì–´ë ¤ì›€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë¨¸ì‹ ëŸ¬ë‹ ë° ë°ì´í„° íŒŒì´í”„ë¼ì¸, ì›¹ ê°œë°œ í”„ë ˆì„ì›Œí¬, ì•Œê³ ë¦¬ì¦˜ ìµœì í™”<br>

<br>

---
