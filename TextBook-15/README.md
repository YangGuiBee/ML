#  15 : CASE Study

---

  https://scikit-learn.org/stable/auto_examples/applications/index.html

<br>

  https://www.kaggle.com/

<br>

  참고 : https://github.com/dair-ai/ML-Course-Notes

<br>

# 최적화 방안

<br>

	[1] 데이터 처리 및 변환
		[1-1] 데이터 증강 (Data Augmentation)
		[1-2] 교차 검증 (Cross-Validation)
		[1-3] 데이터 스케일링 (Data Scaling)
		[1-4] 데이터 불균형 처리 (Handling Imbalanced Data)
		[1-5] 정보 병합 (Data Fusion Techniques)
	
	[2] 모델 복잡도 및 일반화
		[2-1] 과적합 방지 (Overfitting Prevention)
		[2-2] 정규화 (L1, L2 Regularization)
		[2-3] 드롭아웃 (Dropout)
		[2-4] 조기 종료 (Early Stopping)
		[2-5] 앙상블 학습 (Ensemble Learning)
	
	[3] 하이퍼파라미터 최적화
		[3-1] 하이퍼파라미터 튜닝 (Hyperparameter Tuning)
		[3-2] 그리드 서치 (Grid Search)
		[3-3] 랜덤 서치 (Random Search)
		[3-4] 베이즈 최적화 (Bayesian Optimization)
		[3-5] 하이퍼파라미터 탐색 자동화 (Automated Hyperparameter Tuning)
	
	[4] 학습 과정 최적화
		[4-1] 학습률 스케줄링 (Learning Rate Scheduling)
		[4-2] 가중치 초기화 (Weight Initialization)
		[4-3] 활성화 함수 선택 (Activation Function Selection)
		[4-4] 전이 학습 (Transfer Learning)
		[4-5] 모델 구조 최적화 (Model Architecture Optimization)
		[4-6] 온라인 학습 (Online Learning)
	
	[5] 성능 향상
		[5-1] 특성 중요도 분석 및 선택 (Feature Selection)
		[5-2] 손실 함수 커스터마이징 (Custom Loss Function)
		
	[6] 하드웨어 및 시스템 최적화
		[6-1] 하드웨어 최적화 (Hardware Optimization)


<br>

--- 


# [1] 데이터 처리 및 변환
## [1-1] 데이터 증강 (Data Augmentation)
▣ 정의 : 기존 데이터셋을 변형하거나 가공하여 새로운 데이터를 생성하는 기법<br>
▣ 필요성 : 데이터 양이 부족하거나 데이터 다양성이 낮은 경우, 모델의 일반화 성능을 향상<br>
▣ 장점 : 데이터셋의 다양성을 인위적으로 증가, 추가적인 데이터 수집 없이 성능 향상, 과적합 방지 효과<br>
▣ 단점 : 증강된 데이터가 실제 데이터를 충분히 반영하지 않을 수 있으며, 처리 시간이 증가하며, 비효율적인 증강은 성능에 부정적 영향을 줄 수도 있음<br>
▣ 적용대상 알고리즘 : 딥러닝 알고리즘 (CNN, RNN 등), 이미지, 텍스트, 음성 처리 모델<br>

## [1-2] 교차 검증 (Cross-Validation)
▣ 정의 : 데이터를 여러 부분으로 나누어 학습과 검증을 반복적으로 수행하여 모델의 일반화 성능을 평가<br>
▣ 필요성 : 과적합 및 과소적합 여부를 판별하거나, 데이터가 제한적일 때 모델의 성능을 신뢰성 있게 평가하기 위해 필요<br>
▣ 장점 : 데이터를 최대한 활용할 수 있으며, 다양한 데이터 분포에서 모델의 성능을 평가하고, 일반화 성능에 대한 신뢰도 증가<br>
▣ 단점 : 계산 비용이 증가하여 학습 시간이 오래 걸리며, 큰 데이터셋에서는 비효율적<br>
▣ 적용대상 알고리즘 : 모든 지도학습 알고리즘 (분류, 회귀 등), 특히 소규모 데이터셋에 적합<br>

## [1-3] 데이터 스케일링 (Data Scaling)
▣ 정의 : 데이터의 특성 값을 일정한 범위나 분포로 변환하여 모델 학습에 적합한 형태(표준화(Standardization): 평균 0, 표준편차 1, 
정규화(Normalization): 최소~최대 스케일링 0~1)로 만드는 과정<br>
▣ 필요성 : 특성 간의 크기 차이가 클 경우, 모델 학습이 왜곡될 수 있기 때문에 이를 조정하여 학습 효율을 향상<br>
▣ 장점 : 모델 학습 안정성 향상, 학습 속도 증가, 성능 개선 가능<br>
▣ 단점 : 데이터 분포를 잘못 조정하면 오히려 성능 저하, 스케일링 단계에서 추가적인 계산이 필요<br>
▣ 적용대상 알고리즘 : 거리 기반 알고리즘 (KNN, SVM 등), 선형 모델 (로지스틱 회귀, 선형 회귀), 딥러닝 모델<br>

## [1-4] 데이터 불균형 처리 (Handling Imbalanced Data)
▣ 정의 : 클래스 간 데이터 비율이 심각하게 불균형할 때, 모델의 학습 성능을 개선하기 위해 데이터를 조정<br>
▣ 필요성 : 불균형 데이터는 모델이 다수 클래스를 선호하도록 학습하게 만들기 때문에, 이를 해결하지 않으면 특정 클래스의 성능이 저하<br>
▣ 장점 : 클래스 간 균형을 맞춰 모델의 공정성과 성능 향상, 소수 클래스의 예측 성능을 개선<br>
▣ 단점 : 언더샘플링은 데이터 손실 가능성, 오버샘플링은 과적합 위험, 가중치 조정은 추가적인 하이퍼파라미터 조정 필요<br>
▣ 적용대상 알고리즘 : 모든 지도학습 알고리즘, 특히 분류 문제 (이진 분류, 다중 클래스 분류)<br>

## [1-5] 정보 병합 (Data Fusion Techniques)
▣ 정의 : 여러 데이터 소스나 형식을 통합하여 모델 학습에 활용(데이터 레벨, 특징 레벨, 의사결정 레벨)<br>
▣ 필요성 : 서로 다른 소스의 데이터 간 상호작용을 모델링하고, 데이터의 다양성을 확보하여 모델이 더 풍부한 정보를 학습<br>
▣ 장점 : 데이터의 다양성을 활용하여 성능 향상, 복합 문제 해결 가능<br>
▣ 단점 : 데이터 통합 과정에서 불일치 문제가 발생할 수 있으며, 데이터 전처리 및 통합 과정이 복잡하고 비용이 높음<br>
▣ 적용대상 알고리즘 : 다중 입력 데이터를 사용하는 알고리즘, 추천 시스템, 다중 센서 데이터 분석, 이미지와 텍스트 결합 모델<br>


# [2] 모델 복잡도 및 일반화
## [2-1] 과적합 방지 (Overfitting Prevention)
▣ 정의 : 모델이 학습 데이터에만 지나치게 적응하지 않도록 제어하여, 새로운 데이터에서도 일반화된 성능을 유지할 수 있도록 하는 다양한 기법의 조합<br>
▣ 필요성 : 모델이 학습 데이터의 노이즈나 불필요한 패턴을 학습하지 않도록 하여, 테스트 데이터나 실전 데이터에서도 높은 성능을 유지하도록 보장<br>
▣ 장점 : 일반화 성능 향상, 예측 모델의 신뢰도 증가<br>
▣ 단점 : 과적합 방지 기법이 과도하게 적용되면 과소적합(Underfitting)을 유발할 수 있으며, 최적의 설정을 찾기 위한 추가적인 실험과 조정이 필요<br>
▣ 적용대상 알고리즘 : 모든 머신러닝 및 딥러닝 알고리즘<br>

## [2-2] 정규화 (L1, L2 Regularization)
▣ 정의 : 모델의 복잡성을 줄이기 위해 손실 함수에 패널티(term)를 추가하여 모델 파라미터의 크기를 제어(L1 정규화: 가중치의 절댓값 합, L2 정규화: 가중치의 제곱합)<br>
▣ 필요성 : 모델이 불필요하게 큰 가중치를 학습하여 과적합되는 것을 방지<br>
▣ 장점 : L1은 희소 모델(sparse model)을 생성하여 중요한 특성을 선택하는 데 유용, L2는 과도한 가중치를 줄여 모델 안정성 향상<br>
▣ 단점 : 과소적합 가능성, 정규화 강도를 조정하기 위한 하이퍼파라미터(λ) 선택 필요<br>
▣ 적용대상 알고리즘 : 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression), 서포트 벡터 머신(SVM), 뉴럴 네트워크<br>

## [2-3] 드롭아웃 (Dropout)
▣ 정의 : 학습 과정에서 뉴런의 일부를 랜덤하게 비활성화하여(=드롭아웃) 네트워크가 특정 뉴런에 의존하지 않도록 하는 기법<br>
▣ 필요성 : 신경망에서의 과적합 문제를 완화하여 더 일반화된 성능을 도모<br>
▣ 장점 : 과적합 방지, 네트워크 구조에 레이어별 다양성을 부여<br>
▣ 단점 : 학습 시간이 늘어날 수 있으며, 테스트 시 드롭아웃을 적용하지 않기 때문에 추가적인 처리 단계가 필요<br>
▣ 적용대상 알고리즘 : 딥러닝 모델 (특히 CNN, RNN 등)<br>

## [2-4] 조기 종료 (Early Stopping)
▣ 정의 : 학습 중 검증 세트의 성능이 더 이상 개선되지 않는 시점에서 학습을 중단<br>
▣ 필요성 : 학습을 너무 오래 진행할 경우 모델이 학습 데이터에 과적합될 위험을 줄이기 위함<br>
▣ 장점 : 과적합 방지, 불필요한 학습 시간 절약<br>
▣ 단점 : 검증 데이터의 성능을 과도하게 의존하거나, 최적의 종료 시점 결정 곤란<br>
▣ 적용대상 알고리즘 : 모든 딥러닝 모델, 일부 머신러닝 알고리즘에서도 사용 가능(Gradient Boosting)<br>

## [2-5] 앙상블 학습 (Ensemble Learning)
▣ 정의 : 여러 개의 모델을 결합(배깅: 각 모델의 독립적인 학습, 부스팅: 각 모델이 순차적으로 학습, 스태킹: 서로다른 모델의 예측결과 결합)<br>
▣ 필요성 : 단일 모델의 한계를 극복하고, 데이터의 다양한 패턴을 더 잘 학습<br>
▣ 장점 : 높은 성능과 일반화 능력, 다양한 데이터 및 모델 유형에 적용 가능<br>
▣ 단점 : 계산 비용 증가, 구현 복잡성<br>
▣ 적용대상 알고리즘 : 모든 지도 학습 알고리즘 (분류, 회귀 등)<br>


# [3] 하이퍼파라미터 최적화
## [3-1] 하이퍼파라미터 튜닝 (Hyperparameter Tuning)
▣ 정의 : 하이퍼파라미터는 학습 과정에서 사용자가 사전에 설정하는 변수로, 학습률, 정규화 강도, 의사결정나무의 최대 깊이 등 등을 조정하여 모델의 성능을 최적화하는 과정<br>
▣ 필요성 : 최적의 하이퍼파라미터를 찾지 못하면 과적합, 과소적합 또는 학습 속도 저하가 발생<br>
▣ 장점 : 모델 성능 극대화 가능, 다양한 데이터와 문제 유형에 적용 가능<br>
▣ 단점 : 계산 비용이 많이 들고, 시간이 오래 걸릴 수 있으며, 탐색 공간이 커질수록 복잡도 증가<br>
▣ 적용대상 알고리즘 : 모든 머신러닝 및 딥러닝 알고리즘<br>

## [3-2] 그리드 서치 (Grid Search)
▣ 정의 : 하이퍼파라미터의 모든 조합을 체계적으로 탐색하여 최적의 하이퍼파라미터를 찾는 방법<br>
▣ 필요성 : 체계적으로 모든 조합을 탐색하므로 최적의 설정을 찾을 가능성이 높음<br>
▣ 장점 : 간단하고 직관적이며 구현이 용이하며, 하이퍼파라미터 조합의 전 범위를 탐색 가능<br>
▣ 단점 : 탐색 공간이 커질수록 계산 비용과 시간이 기하급수적으로 증가하며, 불필요한 조합까지 계산할 수 있음<br>
▣ 적용대상 알고리즘 : 모든 머신러닝 및 딥러닝 알고리즘, 하이퍼파라미터 공간이 비교적 작은 문제에 적합<br>

## [3-3] 랜덤 서치 (Random Search)
▣ 정의 : 하이퍼파라미터의 값들을 임의로 선택하여 최적의 조합을 탐색<br>
▣ 필요성 : 그리드 서치보다 계산 효율성을 높이며, 더 큰 탐색 공간에서 효과적으로 탐색<br>
▣ 장점 : 계산 효율성 증대, 적은 계산으로도 높은 성능의 하이퍼파라미터를 찾을 가능성<br>
▣ 단점 : 최적의 조합을 반드시 찾지 못할 가능성, 탐색 결과가 실행마다 달라질 수 있음<br>
▣ 적용대상 알고리즘 : 모든 머신러닝 및 딥러닝 알고리즘, 대규모 하이퍼파라미터 공간에 적합<br>

## [3-4] 베이즈 최적화 (Bayesian Optimization)
▣ 정의 : 이전 탐색 결과를 바탕으로 하이퍼파라미터 공간을 효율적으로 탐색하여 최적값을 찾는 방법으로 확률 모델(예: 가우시안 프로세스)을 활용하여 탐색<br>
▣ 필요성 : 계산 비용이 높은 문제에서 효율적으로 최적의 하이퍼파라미터를 찾기 위해 필요<br>
▣ 장점 : 효율적인 탐색으로 계산 자원을 절약, 이전 결과를 바탕으로 탐색하여 빠른 수렴<br>
▣ 단점 : 구현 및 이해가 복잡할 수 있으며, 탐색 초기에는 성능이 낮을 수 있음<br>
▣ 적용대상 알고리즘 : 계산 비용이 높은 머신러닝(랜덤 포레스트) 및 딥러닝 모델<br>

## [3-5] 하이퍼파라미터 탐색 자동화 (Automated Hyperparameter Tuning)
▣ 정의 : 하이퍼파라미터 튜닝 과정을 자동화하여 최적화된 하이퍼파라미터를 찾는 기법<br>
▣ 필요성 : 수동으로 하이퍼파라미터를 조정하는 데 드는 시간과 노력을 줄이기 위해 필요<br>
▣ 장점 : 효율적이고 편리하며 반복 가능, 초보자도 고성능 모델 구현 가능<br>
▣ 단점 : 도구 및 알고리즘의 제한 사항에 따라 최적 성능을 보장하지 못할 수도 있으며, 도구 사용에 따른 비용 발생 가능<br>
▣ 적용대상 알고리즘 : 모든 머신러닝 및 딥러닝 알고리즘, 특히 AutoML을 사용하는 대규모 프로젝트<br>


# [4] 학습 과정 최적화
## [4-1] 학습률 스케줄링 (Learning Rate Scheduling)
▣ 정의 : 학습률(Learning Rate)을 학습 과정 중에 동적으로 조정하여 최적화 성능과 속도를 향상시키는 방법(Step Decay: 일정 에포크마다 학습률 감소, Exponential Decay: 학습률을 지수적으로 감소, Cosine Annealing: 학습률을 점진적으로 낮추는 방식)<br>
▣ 필요성 : 고정된 학습률은 초기와 후반부 학습에 비효율적일 수 있으며, 초기에는 빠른 학습, 후반부에는 안정적인 수렴이 필요<br>
▣ 장점 : 학습 속도와 최적화 안정성을 모두 확보 가능하며, 과적합 방지에 도움<br>
▣ 단점 : 적절한 스케줄을 설정하지 않으면 성능 저하 가능성, 추가적인 하이퍼파라미터 조정 필요<br>
▣ 적용대상 알고리즘 : 주로 딥러닝 알고리즘, 특히 SGD, Adam과 같은 옵티마이저를 사용하는 모델<br>

## [4-2] 가중치 초기화 (Weight Initialization)
▣ 정의 : 모델의 가중치 값을 학습 초기에 적절히 설정하여 학습 과정을 안정화하는 기법(Xavier Initialization: 입력 및 출력 노드 수에 기반, 
He Initialization: ReLU 계열 활성화 함수에 적합)<br>
▣ 필요성 : 잘못된 초기화는 기울기 소실(Vanishing Gradient)이나 폭발(Exploding Gradient)을 유발할 수 있음.<br>
▣ 장점 : 학습 초기 안정성 향상, 빠른 수렴 가능<br>
▣ 단점 : 일부 알고리즘에서 특정 초기화 전략이 더 적합하므로 적절한 선택 필요<br>
▣ 적용대상 알고리즘 : 딥러닝 모델 (특히 심층 신경망)<br>

## [4-3] 활성화 함수 선택 (Activation Function Selection)
▣ 정의 : 뉴런의 출력 값을 비선형적으로 변환하여 학습 가능한 패턴을 늘리는 역할을 하는 활성화 함수를 선택하는 과정(Sigmoid: [0, 1] 출력, 이진 분류에서 사용, ReLU: 비선형성 제공, 기울기 소실 문제 완화, Leaky ReLU: ReLU의 변형, 음수 구간 기울기 보정, Softmax: 다중 클래스 확률 분포 출력)<br>
▣ 필요성 : 적절한 활성화 함수 선택은 학습 효율성과 성능에 큰 영향을 미침<br>
▣ 장점 : 비선형성을 도입하여 복잡한 문제를 해결 가능하고, 다양한 데이터 유형과 문제에 맞게 조정 가능<br>
▣ 단점 : 잘못된 활성화 함수 선택 시 학습 속도 저하나 성능 악화, 특정 함수는 기울기 소실 문제 발생 가능(Sigmoid, Tanh)<br>
▣ 적용대상 알고리즘 : 모든 딥러닝 알고리즘<br>

## [4-4] 전이 학습 (Transfer Learning)
▣ 정의 : 사전 학습된 모델의 가중치를 새로운 문제에 재사용하여 학습 시간을 단축하고 성능을 향상시키는 기법<br>
▣ 필요성 : 데이터 부족 상황에서 강력한 성능을 보장, 학습 시간을 크게 단축<br>
▣ 장점 : 적은 데이터로도 높은 성능 가능, 빠른 학습과 높은 초기 성능<br>
▣ 단점 : 사전 학습 모델이 새로운 문제에 최적화되지 않을 수 있으며, 사전 학습된 데이터셋과 도메인 차이가 클 경우 성능 저하<br>
▣ 적용대상 알고리즘 : 이미지 처리(CNN), 자연어 처리(Transformer, GPT)<br>

## [4-5] 모델 구조 최적화 (Model Architecture Optimization)
▣ 정의 : 모델의 구조(레이어 수, 뉴런 수, 연결 방식 등)를 최적화하여 학습 성능을 극대화하는 과정<br>
▣ 필요성 : 복잡한 모델 구조는 과적합 위험 증가, 단순한 구조는 표현력이 부족하므로 적절한 균형 필요<br>
▣ 장점 : 데이터와 문제에 적합한 모델 설계 가능, 과적합 위험 감소<br>
▣ 단점 : 설계에 많은 시간과 리소스 소모, 자동화 도구 사용 시 높은 계산 비용 발생 가능<br>
▣ 적용대상 알고리즘 : 딥러닝 모델 (특히 신경망)<br>

## [4-6] 온라인 학습 (Online Learning)
▣ 정의 : 점진적으로 데이터를 학습하며 새로운 데이터가 들어올 때마다 모델을 업데이트하는 기법<br>
▣ 필요성 : 데이터가 실시간으로 수집되거나, 저장 공간이 제한적인 경우<br>
▣ 장점 : 실시간 데이터 처리 가능, 메모리 사용량 감소<br>
▣ 단점 : 잘못된 데이터가 들어오면 모델에 즉시 영향을 미칠 수 있으며, 학습 과정 추적 및 디버깅이 어려움<br>
▣ 적용대상 알고리즘 : 실시간 데이터 처리 모델 (예: 온라인 추천 시스템, 실시간 예측 모델), SGD 기반 알고리즘<br>


# [5] 성능 향상
## [5-1] 특성 중요도 분석 및 선택 (Feature Selection)
▣ 정의 : 모델 성능에 가장 큰 영향을 미치는 중요한 특성을 식별하고, 불필요하거나 상관성이 낮은 특성을 제거하는 과정(Filter Methods: 상관계수, 카이제곱 검정 등, Wrapper Methods: 순차전진선택(SFS), 순차후진제거(SBS), Embedded Methods: L1 정규화, 랜덤 포레스트 기반 중요도)<br>
▣ 필요성 : 고차원 데이터에서 불필요한 특성은 학습 시간을 증가시키고 모델의 일반화 성능을 저하시킬 수 있으며, 특성 선택은 모델 단순화와 성능 향상에 기여<br>
▣ 장점 : 과적합 위험 감소, 학습 시간 단축, 모델 해석 가능성 증가<br>
▣ 단점 : 특성 선택 과정이 계산 비용이 많이 들 수 있으며, 중요한 특성을 놓칠 가능성<br>
▣ 적용대상 알고리즘 : 모든 지도 학습 알고리즘, 특히 고차원 데이터셋이 포함된 문제<br>

## [5-2] 손실 함수 커스터마이징 (Custom Loss Function)
▣ 정의 : 문제의 특성과 요구사항에 맞게 손실 함수를 새로 설계하거나 기존 손실 함수를 변형하여 사용<br>
▣ 필요성 : 기본 손실 함수가 문제의 목표를 충분히 반영하지 못할 경우, 성능을 향상시키기 위해 필요<br>
▣ 장점 : 문제의 요구사항에 특화된 성능 향상 가능, 손실 함수 자체가 모델 학습 방향을 결정하기 때문에 세밀한 조정 가능<br>
▣ 단점 : 구현이 복잡할 수 있으며, 손실 함수 설계 오류는 학습 성능 저하로 이어질 가능성<br>
▣ 적용대상 알고리즘 : 모든 머신러닝 및 딥러닝 알고리즘, 특히 비정형 데이터, 불균형 데이터 문제에 적합<br>


# [6] 하드웨어 및 시스템 최적화
## [6-1] 하드웨어 최적화 (Hardware Optimization)
▣ 정의 : 모델 학습 및 추론과정에서 GPU, TPU 등 하드웨어 가속기를 활용하거나, 병렬 처리와 분산 학습을 통해 계산성능을 극대화하는 기법<br>
▣ 필요성 : 딥러닝 및 대규모 데이터 처리 모델에서 계산량이 많아지는 문제를 해결하기 위해 필요<br>
▣ 장점 : 학습 속도 및 추론 속도 향상, 대규모 데이터와 모델을 처리할 수 있는 확장성 제공<br>
▣ 단점 : 하드웨어 장비의 초기 비용이 높으며, 하드웨어 최적화를 위한 추가적인 설정과 기술 지식 필요<br>
▣ 적용대상 알고리즘 : 딥러닝 모델 (CNN, RNN, Transformer 등), 대규모 데이터 처리 및 병렬화가 가능한 모든 알고리즘<br>
