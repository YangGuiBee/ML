#  15 : CASE Study

---

  https://scikit-learn.org/stable/auto_examples/applications/index.html

<br>

  https://www.kaggle.com/

<br>

  ì°¸ê³  : https://github.com/dair-ai/ML-Course-Notes

<br>

# Machine Learning ìµœì í™” ë°©ì•ˆ

<br>

	[1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
		[1-1] ë°ì´í„° ì¦ê°•(Data Augmentation)
		[1-2] êµì°¨ ê²€ì¦(Cross-Validation)
		[1-3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§(Data Scaling)
		[1-4] ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬(Handling Imbalanced Data)
		[1-5] ê²°ì¸¡ê°’ ì²˜ë¦¬(Handling Missing Data)
		[1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection)
		[1-7] ë°ì´í„° ì¤‘ë³µ ì œê±°(Data Deduplication)
		[1-8] ë°ì´í„° ë³€í™˜(Data Transformation)
		[1-9] íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)
		[1-10] ì •ë³´ ë³‘í•©(Data Fusion)
	
	[2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™” : ê³¼ì í•© ë°©ì§€(Overfitting Prevention)
		[2-1] ì •ê·œí™”(L1, L2 Regularization)
		[2-2] ì¡°ê¸° ì¢…ë£Œ(Early Stopping)
		[2-3] ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)
		[2-4] ëª¨ë¸ í•´ì„ì„±(Model Interpretability)
	
	[3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
		[3-1] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Hyperparameter Tuning)
		[3-2] ê·¸ë¦¬ë“œ ì„œì¹˜(Grid Search)
		[3-3] ëœë¤ ì„œì¹˜(Random Search)
		[3-4] ë² ì´ì¦ˆ ìµœì í™”(Bayesian Optimization)
		[3-5] í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìë™í™”(Automated Hyperparameter Tuning)
		[3-6] AutoML í™œìš©(AutoML)
	
	[4] í•™ìŠµ ê³¼ì • ìµœì í™”
		[4-1] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Learning Rate Scheduling)
		[4-2] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Initialization)
		[4-3] í™œì„±í™” í•¨ìˆ˜ ì„ íƒ(Activation Function Selection)
		[4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
		[4-5] ì „ì´ í•™ìŠµ(Transfer Learning)
		[4-6] ëª¨ë¸ êµ¬ì¡° ìµœì í™”(Model Architecture Optimization)
		[4-7] ì˜¨ë¼ì¸ í•™ìŠµ(Online Learning)
	
	[5] ì„±ëŠ¥ í–¥ìƒ
		[5-1] íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ(Feature Importance & Selection)
		[5-2] ì†ì‹¤í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•(Custom Loss Function)
  
	[6] í•˜ë“œì›¨ì–´ ë° ì‹œìŠ¤í…œ ìµœì í™”
		[6-1] í•˜ë“œì›¨ì–´ ìµœì í™”(Hardware Optimization)

	[7] ëª¨ë¸ ê²€ì¦ ë° ë¹„êµ
		[7-1] ëª¨ë¸ ê²€ì¦(Model Validation)
		[7-2] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ(Model Performance Comparison) 

	[8] ê¸°ìˆ  ë¶€ì±„ ê´€ë¦¬
		[8-1] ê¸°ìˆ  ë¶€ì±„(Technical Debt) ê´€ë¦¬

<br>

--- 


# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
## [1-1] ë°ì´í„° ì¦ê°•(Data Augmentation)
â–£ ì •ì˜ : ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ë³€í˜•í•˜ê±°ë‚˜ ê°€ê³µí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë°ì´í„° ì–‘ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë°ì´í„° ë‹¤ì–‘ì„±ì´ ë‚®ì€ ê²½ìš°, ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ<br>
â–£ ì¥ì  : ë°ì´í„°ì…‹ì˜ ë‹¤ì–‘ì„±ì„ ì¸ìœ„ì ìœ¼ë¡œ ì¦ê°€, ì¶”ê°€ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ì—†ì´ ì„±ëŠ¥ í–¥ìƒ, ê³¼ì í•© ë°©ì§€ íš¨ê³¼<br>
â–£ ë‹¨ì  : ì¦ê°•ëœ ë°ì´í„°ê°€ ì‹¤ì œ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ì²˜ë¦¬ ì‹œê°„ì´ ì¦ê°€í•˜ë©°, ë¹„íš¨ìœ¨ì ì¸ ì¦ê°•ì€ ì„±ëŠ¥ì— ë¶€ì •ì  ì˜í–¥<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ (CNN, RNN ë“±), ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ìŒì„± ì²˜ë¦¬ ëª¨ë¸<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-1] ë°ì´í„° ì¦ê°• (Data Augmentation)
	# ìˆ«ì ë°ì´í„° : ë…¸ì´ì¦ˆì™€ ë¹„ì„ í˜•ì„± ì¶”ê°€
	#############################################################
	import numpy as np
	import pandas as pd
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import mean_squared_error, r2_score
	import matplotlib.pyplot as plt

	# ì›ë³¸ ë°ì´í„° ìƒì„±
	np.random.seed(42)
	X = np.random.rand(100, 1) * 10  # Feature
	y = 2.5 * X.flatten() + np.random.randn(100) * 2  # Target with noise

	# ì›ë³¸ ë°ì´í„°í”„ë ˆì„
	original_data = pd.DataFrame({"X": X.flatten(), "y": y})

	# ë°ì´í„° ì¦ê°•: ë…¸ì´ì¦ˆì™€ ë¹„ì„ í˜•ì„± ì¶”ê°€
	augmented_X = X + np.random.randn(100, 1) * 0.2  # ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€
	augmented_y = 2.5 * augmented_X.flatten() + np.random.randn(100) * 0.5 + 0.2 * np.sin(augmented_X.flatten())  # ë¹„ì„ í˜•ì„± ì¶”ê°€
	augmented_data = pd.DataFrame({"X": augmented_X.flatten(), "y": augmented_y})

	# ë°ì´í„° ë³‘í•©
	combined_data = pd.concat([original_data, augmented_data], ignore_index=True)

	# ë°ì´í„° ë¶„ë¦¬: ì›ë³¸ ë°ì´í„°
	X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
	    original_data["X"].values.reshape(-1, 1), original_data["y"], test_size=0.2, random_state=42)

	# ë°ì´í„° ë¶„ë¦¬: ì¦ê°• ë°ì´í„°
	X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(
	    combined_data["X"].values.reshape(-1, 1), combined_data["y"], test_size=0.2, random_state=42)

	# ëª¨ë¸ í•™ìŠµ: ì›ë³¸ ë°ì´í„°
	model_orig = LinearRegression()
	model_orig.fit(X_train_orig, y_train_orig)
	y_pred_orig = model_orig.predict(X_test_orig)

	# ëª¨ë¸ í•™ìŠµ: ì¦ê°• ë°ì´í„°
	model_aug = LinearRegression()
	model_aug.fit(X_train_aug, y_train_aug)
	y_pred_aug = model_aug.predict(X_test_aug)

	# ì„±ëŠ¥ í‰ê°€
	mse_orig = mean_squared_error(y_test_orig, y_pred_orig)
	r2_orig = r2_score(y_test_orig, y_pred_orig)

	mse_aug = mean_squared_error(y_test_aug, y_pred_aug)
	r2_aug = r2_score(y_test_aug, y_pred_aug)

	# ê²°ê³¼ ì¶œë ¥
	print("=== ì›ë³¸ ë°ì´í„° ì„±ëŠ¥ ===")
	print(f"Mean Squared Error: {mse_orig:.4f}")
	print(f"R2 Score: {r2_orig:.4f}")

	print("\n=== ì¦ê°• ë°ì´í„° ì„±ëŠ¥ ===")
	print(f"Mean Squared Error: {mse_aug:.4f}")
	print(f"R2 Score: {r2_aug:.4f}")

	# ì‹œê°í™”
	plt.scatter(original_data["X"], original_data["y"], label="Original Data", color="blue", alpha=0.6)
	plt.scatter(augmented_data["X"], augmented_data["y"], label="Augmented Data", color="orange", alpha=0.4)
	plt.plot(X_test_orig, y_pred_orig, label="Model (Original)", color="green")
	plt.plot(X_test_aug, y_pred_aug, label="Model (Augmented)", color="red")
	plt.xlabel("X")
	plt.ylabel("y")
	plt.title("Original vs Augmented Data and Models")
	plt.legend()
	plt.show()

![](./images/1-1.png) 
<br>

## [1-2] êµì°¨ ê²€ì¦(Cross-Validation)
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµê³¼ ê²€ì¦ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€<br>
â–£ í•„ìš”ì„± : ê³¼ì í•© ë° ê³¼ì†Œì í•© ì—¬ë¶€ë¥¼ íŒë³„í•˜ê±°ë‚˜, ë°ì´í„°ê°€ ì œí•œì ì¼ ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹ ë¢°ì„± ìˆê²Œ í‰ê°€í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë°ì´í„° ë¶„í¬ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ì¼ë°˜í™” ì„±ëŠ¥ì— ëŒ€í•œ ì‹ ë¢°ë„ ì¦ê°€<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš©ì´ ì¦ê°€í•˜ì—¬ í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë©°, í° ë°ì´í„°ì…‹ì—ì„œëŠ” ë¹„íš¨ìœ¨ì <br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ (ë¶„ë¥˜, íšŒê·€ ë“±), íŠ¹íˆ ì†Œê·œëª¨ ë°ì´í„°ì…‹ì— ì í•©<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-2] êµì°¨ ê²€ì¦ (Cross-Validation) + ë°ì´í„°ì¦ê°•
	#############################################################
	from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.datasets import load_wine
	from sklearn.preprocessing import StandardScaler
	from sklearn.pipeline import Pipeline
	from sklearn.metrics import accuracy_score
	import numpy as np
	import pandas as pd

	# ë°ì´í„° ë¡œë“œ
	data = load_wine()
	X, y = data.data, data.target

	# ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì™€ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
	X_train, X_new, y_train, y_new = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì¦ê°•: ë…¸ì´ì¦ˆ ì¶”ê°€ ë° íŠ¹ì„± ë³€í˜•
	np.random.seed(42)
	noise = np.random.normal(0, 0.2, X_train.shape)  # í‰ê·  0, í‘œì¤€í¸ì°¨ 0.2ì¸ ë…¸ì´ì¦ˆ
	X_augmented = X_train + noise  # ë…¸ì´ì¦ˆ ì¶”ê°€
	y_augmented = y_train  # ë ˆì´ë¸”ì€ ë™ì¼

	# ì¦ê°• ë°ì´í„° í•©ì¹˜ê¸°
	X_combined = np.vstack((X_train, X_augmented))
	y_combined = np.hstack((y_train, y_augmented))

	# ê¸°ë³¸ ëª¨ë¸ ìƒì„±
	model = RandomForestClassifier(random_state=42)

	# êµì°¨ ê²€ì¦ ì—†ì´ í•™ìŠµ ë° ìƒˆë¡œìš´ ë°ì´í„° í‰ê°€ (ê¸°ë³¸ ë°ì´í„°)
	model.fit(X_train, y_train)
	new_predictions = model.predict(X_new)
	accuracy_new_data = accuracy_score(y_new, new_predictions)

	# êµì°¨ ê²€ì¦ (ê¸°ë³¸ ë°ì´í„°)
	kf = KFold(n_splits=5, shuffle=True, random_state=42)
	scores_without_augmentation = cross_val_score(model, X_train, y_train, cv=kf)

	# êµì°¨ ê²€ì¦ (ì¦ê°• ë°ì´í„° í¬í•¨)
	scores_with_augmentation = cross_val_score(model, X_combined, y_combined, cv=kf)

	# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ í¬í•¨í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
	pipeline = Pipeline([
	    ('scaler', StandardScaler()),
	    ('classifier', RandomForestClassifier(random_state=42))])

	# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì¦ê°• ë°ì´í„° í¬í•¨)
	param_grid = {
	    'classifier__n_estimators': [50, 100, 150],
	    'classifier__max_depth': [None, 10, 20],
	    'classifier__min_samples_split': [2, 5, 10]}

	grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
	grid_search.fit(X_combined, y_combined)

	# ìµœì  ëª¨ë¸ë¡œ êµì°¨ ê²€ì¦
	best_model = grid_search.best_estimator_
	scores_with_tuning = cross_val_score(best_model, X_combined, y_combined, cv=kf)

	# ê²°ê³¼ ì¶œë ¥
	print("=== ìƒˆë¡œìš´ ë°ì´í„°ë¡œ í‰ê°€ (ê¸°ë³¸ ë°ì´í„°) ===")
	print(f"ìƒˆë¡œìš´ ë°ì´í„° ì •í™•ë„: {accuracy_new_data:.4f}")

	print("\n=== ê¸°ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê²°ê³¼ ===")
	print(f"êµì°¨ ê²€ì¦ ì ìˆ˜ (ê¸°ë³¸ ë°ì´í„°): {scores_without_augmentation}")
	print(f"í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_without_augmentation.mean():.4f}")

	print("\n=== ì¦ê°• ë°ì´í„° í¬í•¨ êµì°¨ ê²€ì¦ ê²°ê³¼ ===")
	print(f"êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨): {scores_with_augmentation}")
	print(f"í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_with_augmentation.mean():.4f}")

	print("\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ (ì¦ê°• ë°ì´í„° í¬í•¨) ===")
	print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
	print(f"ìµœì  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.4f}")

	print("\n=== ìµœì  ëª¨ë¸ êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨) ===")
	print(f"êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_with_tuning}")
	print(f"í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: {scores_with_tuning.mean():.4f}")

<br>

	=== ìƒˆë¡œìš´ ë°ì´í„°ë¡œ í‰ê°€ (ê¸°ë³¸ ë°ì´í„°) ===
	ìƒˆë¡œìš´ ë°ì´í„° ì •í™•ë„: 1.0000

	=== ê¸°ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ ê²°ê³¼ ===
	êµì°¨ ê²€ì¦ ì ìˆ˜ (ê¸°ë³¸ ë°ì´í„°): [0.93103448 0.96551724 1.         1.         1.        ]
	í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9793

	=== ì¦ê°• ë°ì´í„° í¬í•¨ êµì°¨ ê²€ì¦ ê²°ê³¼ ===
	êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨): [0.96491228 1.         0.96491228 0.98245614 0.98214286]
	í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9789

	=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ (ì¦ê°• ë°ì´í„° í¬í•¨) ===
	ìµœì  íŒŒë¼ë¯¸í„°: {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}
	ìµœì  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9895

	=== ìµœì  ëª¨ë¸ êµì°¨ ê²€ì¦ ì ìˆ˜ (ì¦ê°• ë°ì´í„° í¬í•¨) ===
	êµì°¨ ê²€ì¦ ì ìˆ˜: [0.96491228 1.         0.96491228 0.98245614 0.98214286]
	í‰ê·  êµì°¨ ê²€ì¦ ì ìˆ˜: 0.9789

<br>

## [1-3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§(Data Scaling)
â–£ ì •ì˜ : ë°ì´í„°ì˜ íŠ¹ì„± ê°’ì„ ì¼ì •í•œ ë²”ìœ„ë‚˜ ë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœ(í‘œì¤€í™”(Standardization): í‰ê·  0, í‘œì¤€í¸ì°¨ 1, 
ì •ê·œí™”(Normalization): ìµœì†Œ-ìµœëŒ€ ìŠ¤ì¼€ì¼ë§ 0-1)ë¡œ ë§Œë“œëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : íŠ¹ì„± ê°„ì˜ í¬ê¸° ì°¨ì´ê°€ í´ ê²½ìš°, ëª¨ë¸ í•™ìŠµì´ ì™œê³¡ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ì¡°ì •í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„ í–¥ìƒ<br>
â–£ ì¥ì  : ëª¨ë¸ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ, í•™ìŠµ ì†ë„ ì¦ê°€, ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ë°ì´í„° ë¶„í¬ë¥¼ ì˜ëª» ì¡°ì •í•˜ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜, ìŠ¤ì¼€ì¼ë§ ë‹¨ê³„ì—ì„œ ì¶”ê°€ì ì¸ ê³„ì‚°ì´ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ (KNN, SVM ë“±), ì„ í˜• ëª¨ë¸ (ë¡œì§€ìŠ¤í‹± íšŒê·€, ì„ í˜• íšŒê·€), ë”¥ëŸ¬ë‹ ëª¨ë¸<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (Data Scaling)
	# MinMaxScaler : ë°ì´í„°ë¥¼ íŠ¹ì • ë²”ìœ„(ê¸°ë³¸ê°’: [0, 1])ë¡œ ì •ê·œí™”
	# ğ‘‹ : ì›ë³¸ ë°ì´í„° ê°’
	# ğ‘‹_{min}  : ê° ì—´ì˜ ìµœì†Œê°’
	# ğ‘‹_{max}  : ê° ì—´ì˜ ìµœëŒ€ê°’
	# ğ‘‹â€² : ë³€í™˜ëœ ë°ì´í„° ê°’
	# ğ‘‹â€² = (ğ‘‹ - ğ‘‹_{min}) / (ğ‘‹_{max} - ğ‘‹_{min})
	#############################################################
	import numpy as np
	from sklearn.datasets import make_classification
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import MinMaxScaler
	from sklearn.neighbors import KNeighborsClassifier
	from sklearn.metrics import accuracy_score

	# ë°ì´í„° ì¶œë ¥ í˜•ì‹ ì„¤ì • (ì†Œìˆ˜ì  ì´í•˜ 4ìë¦¬ê¹Œì§€)
	np.set_printoptions(precision=4, suppress=True)

	# ë°ì´í„° ìƒì„±
	X, y = make_classification(
	    n_samples=500,
	    n_features=5,
	    n_informative=3,
	    n_redundant=0,
	    random_state=42)

	# ì¸ìœ„ì ìœ¼ë¡œ íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ë¥¼ í¬ê²Œ ë§Œë“¦
	X[:, 0] *= 1    # ì²« ë²ˆì§¸ íŠ¹ì„±: 0~1
	X[:, 1] *= 100  # ë‘ ë²ˆì§¸ íŠ¹ì„±: 0~100
	X[:, 2] *= 1000 # ì„¸ ë²ˆì§¸ íŠ¹ì„±: 0~1000

	print("ì›ë³¸ ë°ì´í„° (ì¼ë¶€):\n", X[:5])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

	# KNN ëª¨ë¸ ìƒì„±
	knn = KNeighborsClassifier()

	# 1. ì›ë³¸ ë°ì´í„°ë¡œ í‰ê°€
	knn.fit(X_train, y_train)
	y_pred_original = knn.predict(X_test)
	accuracy_original = accuracy_score(y_test, y_pred_original)

	# 2. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
	scaler = MinMaxScaler()
	X_train_scaled = scaler.fit_transform(X_train)
	X_test_scaled = scaler.transform(X_test)

	print("\nìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° (í›ˆë ¨ ì„¸íŠ¸ ì¼ë¶€):\n", X_train_scaled[:5])

	# ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í•™ìŠµ ë° í‰ê°€
	knn.fit(X_train_scaled, y_train)
	y_pred_scaled = knn.predict(X_test_scaled)
	accuracy_scaled = accuracy_score(y_test, y_pred_scaled)

	# ê²°ê³¼ ì¶œë ¥
	print("\n=== í‰ê°€ ê²°ê³¼ ===")
	print(f"ì›ë³¸ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy_original:.4f}")
	print(f"ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy_scaled:.4f}")

<br>

	ì›ë³¸ ë°ì´í„° (ì¼ë¶€):
	 [[  -1.8306   -9.534  -654.0757    0.7241   -0.1813]
	 [   0.2603    8.0151 -413.4652   -1.2733    1.4826]
 	 [  -1.3796    9.8744 -971.6567   -0.0728   -1.5796]
	 [  -0.9981  -16.1506 1051.9476    2.3985    2.1207]
	 [  -0.3696  122.3565  621.5719    0.0128   -1.4224]]

	ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° (í›ˆë ¨ ì„¸íŠ¸ ì¼ë¶€):
	 [[0.5246 0.7534 0.5159 0.7898 0.714 ]
	 [0.6738 0.2881 0.6199 0.4736 0.4592]
	 [0.3458 0.3688 0.2804 0.1617 0.5876]
	 [0.3992 0.5641 0.541  0.6432 0.4749]
	 [0.5227 0.4134 0.4271 0.1323 0.6014]]

	=== í‰ê°€ ê²°ê³¼ ===
	ì›ë³¸ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.8480
	ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9360
 
<br>


**ì™œë„(Skewness):** ë°ì´í„° ë¶„í¬ì˜ ë¹„ëŒ€ì¹­ì„±ì„ ì¸¡ì •. ë°ì´í„°ê°€ í‰ê· ì„ ê¸°ì¤€ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ëŒ€ì¹­ì ì¸ì§€, ë˜ëŠ” í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³¤ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„ë¡œ ì •ê·œë¶„í¬ëŠ” ëŒ€ì¹­ì ì´ë¯€ë¡œ ì™œë„ ê°’ì€ 0ì— ê°€ê¹Œì›Œì•¼ í•¨<br>
**ì²¨ë„(Kurtosis):** ì²¨ë„ëŠ” ë°ì´í„° ë¶„í¬ì˜ ì¤‘ì‹¬ë¶€ ë¾°ì¡±í•¨ê³¼ ê¼¬ë¦¬ ë‘ê»˜ë¥¼ ì¸¡ì •. ë°ì´í„° ë¶„í¬ì˜ ì¤‘ì‹¬ë¶€ì™€ ê¼¬ë¦¬ ë¶€ë¶„ì´ ì •ê·œë¶„í¬ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ë‚˜íƒ€ëƒ„<br>


	import numpy as np
	import pandas as pd
	from scipy.stats import skew, kurtosis
	from scipy.stats import boxcox
	from sklearn.preprocessing import PowerTransformer

	# ë°ì´í„° ìƒì„±
	data = np.random.exponential(scale=2, size=1000)  # ë¹„ì •ê·œ ë°ì´í„°

	# ì™œë„ì™€ ì²¨ë„ ê³„ì‚°
	data_skewness = skew(data)
	data_kurtosis = kurtosis(data, fisher=True)  # Excess Kurtosis

	print(f"Before Transformation - Skewness: {data_skewness}, Kurtosis: {data_kurtosis}")

	# Box-Cox ë³€í™˜ (ë°ì´í„°ê°€ ì–‘ìˆ˜ì¼ ê²½ìš°)
	data_boxcox, _ = boxcox(data + 1e-9)  # 0 ë°©ì§€ìš© ì‘ì€ ê°’ ì¶”ê°€
	print(f"After Box-Cox - Skewness: {skew(data_boxcox)}, Kurtosis: {kurtosis(data_boxcox, fisher=True)}")

	# Yeo-Johnson ë³€í™˜ (ì–‘ìˆ˜/ìŒìˆ˜ ëª¨ë‘ ê°€ëŠ¥)
	transformer = PowerTransformer(method='yeo-johnson')
	data_yeojohnson = transformer.fit_transform(data.reshape(-1, 1))
	print(f"After Yeo-Johnson - Skewness: {skew(data_yeojohnson)}, Kurtosis: {kurtosis(data_yeojohnson, fisher=True)}")



## [1-4] ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬(Handling Imbalanced Data)
![](./images/DA_vs.png)<br>
â–£ ì •ì˜ : í´ë˜ìŠ¤ ê°„ ë°ì´í„° ë¹„ìœ¨ì´ ì‹¬ê°í•˜ê²Œ ë¶ˆê· í˜•í•  ë•Œ, ëª¨ë¸ì˜ í•™ìŠµ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì¡°ì •<br>
â–£ í•„ìš”ì„± : ë¶ˆê· í˜• ë°ì´í„°ëŠ” ëª¨ë¸ì´ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì„ í˜¸í•˜ë„ë¡ í•™ìŠµí•˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì—, ì´ë¥¼ í•´ê²°í•˜ì§€ ì•Šìœ¼ë©´ íŠ¹ì • í´ë˜ìŠ¤ì˜ ì„±ëŠ¥ì´ ì €í•˜<br>
â–£ ì¥ì  : í´ë˜ìŠ¤ ê°„ ê· í˜•ì„ ë§ì¶° ëª¨ë¸ì˜ ê³µì •ì„±ê³¼ ì„±ëŠ¥ í–¥ìƒ, ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê°œì„ <br>
â–£ ë‹¨ì  : ì–¸ë”ìƒ˜í”Œë§ì€ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±, ì˜¤ë²„ìƒ˜í”Œë§ì€ ê³¼ì í•© ìœ„í—˜, ê°€ì¤‘ì¹˜ ì¡°ì •ì€ ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ ë¶„ë¥˜ ë¬¸ì œ (ì´ì§„ ë¶„ë¥˜, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜)<br>

**SMOTE (Synthetic Minority Over-sampling Technique)** <br>
ê¸°ì¡´ì˜ ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ì„ í˜• ë³´ê°„í•˜ì—¬ ìƒì„±í•˜ëŠ” ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²•<br>
ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ ê· ì¼í•˜ê²Œ ì¦ê°•í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì‹œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì™„í™”<br>
ì†Œìˆ˜ í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ê· ì¼í•˜ê²Œ ì¦ê°•í•˜ì—¬ í´ë˜ìŠ¤ ë¶„í¬ ê· í˜•ì— íš¨ê³¼ì <br>
ìµœê·¼ì ‘ ì´ì›ƒê³„ì‚° â†’ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„± â†’ ì¦ê°•ê³¼ì • ë°˜ë³µ<br>

**ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning)** <br>
SMOTEì˜ í™•ì¥ìœ¼ë¡œ, ì†Œìˆ˜ í´ë˜ìŠ¤ ì£¼ë³€ì˜ ë°€ë„ì— ë”°ë¼ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±<br>
ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë¶ˆê· í˜• ë°ì´í„°ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²•<br>
ë°€ë„ê°€ ë‚®ì€ ì˜ì—­ì— ë” ë§ì€ ìƒ˜í”Œì„ ìƒì„±í•˜ì—¬ ë¶„ë¥˜ ê²½ê³„ì— ê°€ê¹Œìš´ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ìƒ˜í”Œì— ì´ˆì <br>
ë°€ë„ ê³„ì‚° â†’ ê°€ì¤‘ì¹˜ ê³„ì‚° â†’ ìƒ˜í”Œìƒì„± ë¹„ìœ¨ ê²°ì • â†’ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„±<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-4] ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ (Handling Imbalanced Data)
	# ADASYN + SMOTE(Synthetic Minority Over-sampling Technique)
	#############################################################
	from imblearn.over_sampling import SMOTE, ADASYN
	from sklearn.datasets import make_classification
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
	from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
	import pandas as pd

	# ê·¹ë‹¨ì ì¸ ë¶ˆê· í˜• ë°ì´í„° ìƒì„±
	X, y = make_classification(
	    n_classes=2,          # ì´ì§„ ë¶„ë¥˜
	    class_sep=2,          # í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ ì •ë„
	    weights=[0.005, 0.995], # í´ë˜ìŠ¤ ë¹„ìœ¨: 0.5% vs 99.5%
	    n_informative=3,      # ì •ë³´ê°€ ìˆëŠ” ë…ë¦½ ë³€ìˆ˜ 3ê°œ
	    n_redundant=1,        # ì¤‘ë³µëœ ë…ë¦½ ë³€ìˆ˜ 1ê°œ
	    flip_y=0,             # ë¼ë²¨ ë’¤ì§‘ê¸° ë¹„ìœ¨ ì—†ìŒ
	    n_features=5,         # ì´ íŠ¹ì„± ìˆ˜: 5ê°œ
	    n_clusters_per_class=1, # ê° í´ë˜ìŠ¤ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°
	    n_samples=2000,       # ì´ ìƒ˜í”Œ ìˆ˜: 2000ê°œ
	    random_state=10       # ë‚œìˆ˜ ê³ ì •
	)
	print("ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬:\n", pd.Series(y).value_counts())

	# êµì°¨ ê²€ì¦ ì„¤ì •
	kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

	# 1. ì›ë³¸ ë°ì´í„° êµì°¨ ê²€ì¦ í‰ê°€
	model = RandomForestClassifier(random_state=42)
	scores_original = cross_val_score(model, X, y, cv=kf, scoring='roc_auc')
	print("\n[êµì°¨ ê²€ì¦] ì›ë³¸ ë°ì´í„° ROC-AUC:", scores_original.mean())

	# ë°ì´í„° ë¶„ë¦¬ (í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

	# 2. ì›ë³¸ ë°ì´í„° ë¶„ë¦¬ í‰ê°€
	model.fit(X_train, y_train)
	y_pred_original = model.predict(X_test)
	accuracy_original = accuracy_score(y_test, y_pred_original)

	print("\n[ë¶„ë¦¬ í‰ê°€] === ì›ë³¸ ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
	print(f"ì •í™•ë„: {accuracy_original:.4f}")
	print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred_original, zero_division=0))

	# ADASYN ì ìš©
	adasyn = ADASYN(sampling_strategy=0.5, random_state=42)
	X_adasyn, y_adasyn = adasyn.fit_resample(X, y)
	X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)
	print("\nADASYN ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ (ì „ì²´ ë°ì´í„°):\n", pd.Series(y_adasyn).value_counts())

	# 3. ADASYN êµì°¨ ê²€ì¦ í‰ê°€
	scores_adasyn = cross_val_score(model, X_adasyn, y_adasyn, cv=kf, scoring='roc_auc')
	print("\n[êµì°¨ ê²€ì¦] ADASYN ë°ì´í„° ROC-AUC:", scores_adasyn.mean())

	# 4. ADASYN ë¶„ë¦¬ í‰ê°€
	model.fit(X_train_adasyn, y_train_adasyn)
	y_pred_adasyn = model.predict(X_test)
	accuracy_adasyn = accuracy_score(y_test, y_pred_adasyn)

	print("\n[ë¶„ë¦¬ í‰ê°€] === ADASYN ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
	print(f"ì •í™•ë„: {accuracy_adasyn:.4f}")
	print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred_adasyn, zero_division=0))

	# SMOTE ì ìš©
	smote = SMOTE(sampling_strategy=0.5, random_state=42)
	X_smote, y_smote = smote.fit_resample(X, y)
	X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
	print("\nSMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ (ì „ì²´ ë°ì´í„°):\n", pd.Series(y_smote).value_counts())

	# 5. SMOTE êµì°¨ ê²€ì¦ í‰ê°€
	scores_smote = cross_val_score(model, X_smote, y_smote, cv=kf, scoring='roc_auc')
	print("\n[êµì°¨ ê²€ì¦] SMOTE ë°ì´í„° ROC-AUC:", scores_smote.mean())

	# 6. SMOTE ë¶„ë¦¬ í‰ê°€
	model.fit(X_train_smote, y_train_smote)
	y_pred_smote = model.predict(X_test)
	accuracy_smote = accuracy_score(y_test, y_pred_smote)

	print("\n[ë¶„ë¦¬ í‰ê°€] === SMOTE ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
	print(f"ì •í™•ë„: {accuracy_smote:.4f}")
	print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred_smote, zero_division=0))

<br>

![](./images/1-4.png) 

<br>

## [1-5] ê²°ì¸¡ê°’ ì²˜ë¦¬(Handling Missing Data)
â–£ ì •ì˜ : ë°ì´í„°ì…‹ì—ì„œ ëˆ„ë½ëœ ê°’(null, NaN)ì„ ì²˜ë¦¬<br>
â–£ í•„ìš”ì„± : ê²°ì¸¡ê°’ì€ ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ì„ ë°©í•´í•˜ê±°ë‚˜ ì™œê³¡ëœ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„ìˆ˜ì ìœ¼ë¡œ í•„ìš”<br>
â–£ ì£¼ìš” ê¸°ë²• : ê²°ì¸¡ê°’ì´ í¬í•¨ëœ í–‰ì´ë‚˜ ì—´ì„ ì œê±°(Deletion), í‰ê· /ì¤‘ì•™ê°’/ìµœë¹ˆê°’ ë“±ìœ¼ë¡œ ëŒ€ì²´(Imputation), ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê²°ì¸¡ê°’ ì˜ˆì¸¡(Predictive Modeling), ìœ ì‚¬í•œ ê´€ì¸¡ì¹˜ë¡œ ê²°ì¸¡ê°’ ëŒ€ì²´(KNN Imputation)<br>
â–£ ì¥ì  : ë°ì´í„° í’ˆì§ˆ í–¥ìƒìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ , ì•ˆì •ì ì´ê³  ì‹ ë¢°ì„± ìˆëŠ” í•™ìŠµ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³¼ë„í•œ ì‚­ì œëŠ” ë°ì´í„° ì†ì‹¤ ìœ„í—˜, ë¶€ì •í™•í•œ ëŒ€ì²´ëŠ” ëª¨ë¸ í¸í–¥ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì„ í˜• íšŒê·€, ì˜ì‚¬ê²°ì • ë‚˜ë¬´, ì‹ ê²½ë§ ë“± ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-5] ê²°ì¸¡ê°’ ì²˜ë¦¬(Handling Missing Data)
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_df.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_df['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 3. ê²°ì¸¡ê°’ ìƒì„± (ì˜ˆì œìš©)
	# ëœë¤ìœ¼ë¡œ 10ê°œì˜ ê°’ì— ê²°ì¸¡ê°’(NaN)ì„ ì‚½ì…
	np.random.seed(42)
	missing_indices = np.random.choice(X.size, 10, replace=False)
	X_flat = X.values.flatten()
	X_flat[missing_indices] = np.nan
	X_with_missing = pd.DataFrame(X_flat.reshape(X.shape), columns=X.columns)

	# ê²°ì¸¡ê°’ í™•ì¸
	print("Data with missing values:")
	print(X_with_missing.isnull().sum())

	# 4. ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²•
	# (1) ê²°ì¸¡ê°’ì„ í¬í•¨í•œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´)
	X_with_zeros = X_with_missing.fillna(0)
	X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
	    X_with_zeros, y, test_size=0.3, random_state=42)

	# (2) ê²°ì¸¡ê°’ì´ ìˆëŠ” ìƒ˜í”Œ ì œê±°
	X_dropped = X_with_missing.dropna()
	y_dropped = y[X_with_missing.dropna().index]
	X_train_dropped, X_test_dropped, y_train_dropped, y_test_dropped = train_test_split(
	    X_dropped, y_dropped, test_size=0.3, random_state=42)

	# (3) ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
	X_imputed = X_with_missing.fillna(X_with_missing.mean())
	X_train_imputed, X_test_imputed, y_train_imputed, y_test_imputed = train_test_split(
	    X_imputed, y, test_size=0.3, random_state=42)

	# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ (0 ëŒ€ì²´)
	model_raw = LogisticRegression(max_iter=200)
	model_raw.fit(X_train_raw, y_train_raw)
	y_pred_raw = model_raw.predict(X_test_raw)
	accuracy_raw = accuracy_score(y_test_raw, y_pred_raw)

	# (2) ê²°ì¸¡ê°’ ì œê±° ë°ì´í„° ì‚¬ìš©
	model_dropped = LogisticRegression(max_iter=200)
	model_dropped.fit(X_train_dropped, y_train_dropped)
	y_pred_dropped = model_dropped.predict(X_test_dropped)
	accuracy_dropped = accuracy_score(y_test_dropped, y_pred_dropped)

	# (3) ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•œ ë°ì´í„° ì‚¬ìš©
	model_imputed = LogisticRegression(max_iter=200)
	model_imputed.fit(X_train_imputed, y_train_imputed)
	y_pred_imputed = model_imputed.predict(X_test_imputed)
	accuracy_imputed = accuracy_score(y_test_imputed, y_pred_imputed)

	# 6. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy before handling missing values (0 imputation): {accuracy_raw:.2f}")
	print(f"Accuracy after dropping missing samples: {accuracy_dropped:.2f}")
	print(f"Accuracy after handling missing values (mean imputation): {accuracy_imputed:.2f}")

<br>

	Data with missing values:
	sepal length (cm)    1
	sepal width (cm)     5
	petal length (cm)    3
	petal width (cm)     1
	dtype: int64

	Accuracy before handling missing values (0 imputation): 0.96
	Accuracy after dropping missing samples: 0.98
	Accuracy after handling missing values (mean imputation): 0.96

<br>

## [1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection)
â–£ ì •ì˜ : ë°ì´í„° ë¶„í¬ì—ì„œ ë¹„ì •ìƒì ìœ¼ë¡œ ë²—ì–´ë‚œ ë°ì´í„°ë¥¼ íƒì§€í•˜ê³  ì²˜ë¦¬<br>
â–£ í•„ìš”ì„± : ì´ìƒì¹˜ëŠ” ë°ì´í„° ë¶„í¬ë¥¼ ì™œê³¡í•˜ê³  ëª¨ë¸ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹ë³„ê³¼ ì²˜ë¦¬ê°€ í•„ìš”<br>
â–£ ì£¼ìš” ê¸°ë²• : í†µê³„ ê¸°ë°˜ ê¸°ë²•(ì‚¬ë¶„ìœ„ ë²”ìœ„(IQR), ì¤‘ì•™ê°’ ì ˆëŒ€ í¸ì°¨(MAD), Z-Score), ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•(Isolation Forest, DBSCAN, One-Class SVM), ì‹œê°í™” ê¸°ë°˜ íƒì§€(Box Plot, Scatter Plot)<br>
â–£ ì¥ì  : ë°ì´í„° ì‹ ë¢°ì„±ê³¼ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”, ì ì¬ì  ì˜¤ë¥˜ë¥¼ ì‹ë³„í•˜ì—¬ ë¬¸ì œ ì˜ˆë°©<br>
â–£ ë‹¨ì  : ê³¼ë„í•œ íƒì§€ ê¸°ì¤€ì€ ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ì œê±°í•  ìœ„í—˜, ê³ ì°¨ì› ë°ì´í„°ì—ì„œëŠ” íƒì§€ê°€ ì–´ë ¤ì›€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : íšŒê·€ë¶„ì„, PCA, í´ëŸ¬ìŠ¤í„°ë§, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë”¥ëŸ¬ë‹ ë“±<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection)
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_df.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_df['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 3. ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
	def detect_outliers_iqr(data):
	    """IQR(ì‚¬ë¶„ìœ„ ë²”ìœ„)ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ íƒì§€"""
	    Q1 = data.quantile(0.25)
	    Q3 = data.quantile(0.75)
	    IQR = Q3 - Q1
	    lower_bound = Q1 - 1.5 * IQR
	    upper_bound = Q3 + 1.5 * IQR
	    outliers = (data < lower_bound) | (data > upper_bound)
	    return ~outliers.any(axis=1), outliers

	# ì´ìƒì¹˜ íƒì§€
	outlier_mask, outliers_boolean = detect_outliers_iqr(X)
	X_no_outliers = X[outlier_mask]
	y_no_outliers = y[outlier_mask]

	# ì´ìƒì¹˜ ë°ì´í„° ì¶”ì¶œ
	outliers_detected = X[~outlier_mask].copy()

	# ì´ìƒì¹˜ ì‚¬ìœ  ì¶”ê°€
	reasons = []
	for index, row in outliers_detected.iterrows():
	    reason = []
	    for column in X.columns:
	        if outliers_boolean.at[index, column]:
	            reason.append(f"{column} out of range")
	    reasons.append(", ".join(reason))
	outliers_detected['Reason'] = reasons

	print("Outliers detected:")
	print(outliers_detected)

	# ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
	print(f"\nOriginal data size: {X.shape[0]}")
	print(f"Data size after removing outliers: {X_no_outliers.shape[0]}")
	print(f"Number of outliers detected: {X.shape[0] - X_no_outliers.shape[0]}")

	# 4. ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì…‹)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
	X_train_no_outliers, X_test_no_outliers, y_train_no_outliers, y_test_no_outliers = 	train_test_split(
	    X_no_outliers, y_no_outliers, test_size=0.3, random_state=42)

	# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ì´ìƒì¹˜ ì œê±° ì „
	model = LogisticRegression(max_iter=200)
	model.fit(X_train, y_train)
	y_pred = model.predict(X_test)
	accuracy_before = accuracy_score(y_test, y_pred)

	# (2) ì´ìƒì¹˜ ì œê±° í›„
	model_no_outliers = LogisticRegression(max_iter=200)
	model_no_outliers.fit(X_train_no_outliers, y_train_no_outliers)
	y_pred_no_outliers = model_no_outliers.predict(X_test_no_outliers)
	accuracy_after = accuracy_score(y_test_no_outliers, y_pred_no_outliers)

	# 6. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy before removing outliers: {accuracy_before:.2f}")
	print(f"Accuracy after removing outliers: {accuracy_after:.2f}")

<br>

	Outliers detected:
 		   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm) 
	15                5.7               4.4                1.5               0.4   
	32                5.2               4.1                1.5               0.1   
	33                5.5               4.2                1.4               0.2   
	60                5.0               2.0                3.5               1.0   

	                           Reason  
	15  sepal width (cm) out of range  
	32  sepal width (cm) out of range  
	33  sepal width (cm) out of range  
	60  sepal width (cm) out of range  

	Original data size: 150
	Data size after removing outliers: 146
	Number of outliers detected: 4

	Accuracy before removing outliers: 1.00
	Accuracy after removing outliers: 0.95

<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-6] ì´ìƒì¹˜ íƒì§€(Outlier Detection) - í´ë˜ìŠ¤ë³„ ì´ìƒì¹˜íƒì§€
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_df.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_df['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 3. í´ë˜ìŠ¤ë³„ IQR ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
	def detect_outliers_iqr_by_class(data, target):
	    """í´ë˜ìŠ¤ë³„ IQRì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ íƒì§€"""
	    outlier_mask = pd.Series(True, index=data.index)
	    reasons = pd.Series("", index=data.index)

	    for cls in target.unique():
	        cls_data = data[target == cls]
	        Q1 = cls_data.quantile(0.25)
	        Q3 = cls_data.quantile(0.75)
	        IQR = Q3 - Q1
	        lower_bound = Q1 - 1.5 * IQR
	        upper_bound = Q3 + 1.5 * IQR
	        cls_outliers = ~((cls_data >= lower_bound) & (cls_data <= upper_bound)).all(axis=1)

 	       # ì—…ë°ì´íŠ¸: í´ë˜ìŠ¤ë³„ ë§ˆìŠ¤í¬ ë° ì´ìƒì¹˜ ì‚¬ìœ  ê¸°ë¡
 	       outlier_mask[cls_data.index] &= ~cls_outliers
	       for idx, row in cls_data.iterrows():
  	          if cls_outliers.at[idx]:
      	             reason = []
                     for column in data.columns:
                        if row[column] < lower_bound[column]:
                           reason.append(f"{column} below {lower_bound[column]:.2f}")
                        elif row[column] > upper_bound[column]:
                             reason.append(f"{column} above {upper_bound[column]:.2f}")
                     reasons.at[idx] = ", ".join(reason)
	       return outlier_mask, reasons

	# ì´ìƒì¹˜ íƒì§€ ìˆ˜í–‰
	class_outlier_mask, outlier_reasons = detect_outliers_iqr_by_class(X, y)
	X_no_outliers = X[class_outlier_mask]
	y_no_outliers = y[class_outlier_mask]

	# ì´ìƒì¹˜ ë°ì´í„° ì¶œë ¥
	outliers_detected = X[~class_outlier_mask].copy()
	outliers_detected["Reason"] = outlier_reasons[~class_outlier_mask]

	print("Outliers detected:")
	print(outliers_detected)

	# ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
	print(f"\nOriginal data size: {X.shape[0]}")
	print(f"Data size after removing outliers: {X_no_outliers.shape[0]}")
	print(f"Number of outliers detected: {X.shape[0] - X_no_outliers.shape[0]}")

	# 4. ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì…‹)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
	X_train_no_outliers, X_test_no_outliers, y_train_no_outliers, y_test_no_outliers = 	train_test_split(
	    X_no_outliers, y_no_outliers, test_size=0.3, random_state=42)

	# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ì´ìƒì¹˜ ì œê±° ì „
	model = LogisticRegression(max_iter=200)
	model.fit(X_train, y_train)
	y_pred = model.predict(X_test)
	accuracy_before = accuracy_score(y_test, y_pred)

	# (2) ì´ìƒì¹˜ ì œê±° í›„
	model_no_outliers = LogisticRegression(max_iter=200)
	model_no_outliers.fit(X_train_no_outliers, y_train_no_outliers)
	y_pred_no_outliers = model_no_outliers.predict(X_test_no_outliers)
	accuracy_after = accuracy_score(y_test_no_outliers, y_pred_no_outliers)

	# 6. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy before removing outliers: {accuracy_before:.2f}")
	print(f"Accuracy after removing outliers: {accuracy_after:.2f}")

	if accuracy_after > accuracy_before:
	    print("Removing outliers improved the model's accuracy.")
	elif accuracy_after == accuracy_before:
	    print("Removing outliers had no effect on the model's accuracy.")
	else:
	    print("Removing outliers decreased the model's accuracy.")

<br>    

	Outliers detected:
	     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \
	13                 4.3               3.0                1.1               0.1   
	15                 5.7               4.4                1.5               0.4   
	22                 4.6               3.6                1.0               0.2   
	23                 5.1               3.3                1.7               0.5   
	24                 4.8               3.4                1.9               0.2   
	41                 4.5               2.3                1.3               0.3   
	43                 5.0               3.5                1.6               0.6   
	44                 5.1               3.8                1.9               0.4   
	98                 5.1               2.5                3.0               1.1   
	106                4.9               2.5                4.5               1.7   
	117                7.7               3.8                6.7               2.2   
	119                6.0               2.2                5.0               1.5   
	131                7.9               3.8                6.4               2.0   

 	                          Reason  
	13   petal length (cm) below 1.14  
	15    sepal width (cm) above 4.39  
	22   petal length (cm) below 1.14  
	23    petal width (cm) above 0.45  
	24   petal length (cm) above 1.84  
	41    sepal width (cm) below 2.49  
	43    petal width (cm) above 0.45  
	44   petal length (cm) above 1.84  
	98   petal length (cm) below 3.10  
	106  sepal length (cm) below 5.21  
	117   sepal width (cm) above 3.74  
	119   sepal width (cm) below 2.24  
	131   sepal width (cm) above 3.74  

	Original data size: 150
	Data size after removing outliers: 137
	Number of outliers detected: 13

	Accuracy before removing outliers: 1.00
	Accuracy after removing outliers: 0.93
	Removing outliers decreased the model's accuracy.

<br>

## [1-7] ë°ì´í„° ì¤‘ë³µ ì œê±°(Data Deduplication)
â–£ ì •ì˜ : ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ íƒì§€í•˜ê³  ì œê±°í•˜ì—¬ ë°ì´í„°ì…‹ì˜ ì¼ê´€ì„±ê³¼ ì •í™•ì„±ì„ ë†’ì´ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : ì¤‘ë³µ ë°ì´í„°ëŠ” ë¶„ì„ ë° ëª¨ë¸ í•™ìŠµì— í¸í–¥ì„ ì´ˆë˜, ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì—¬ ì²˜ë¦¬ ì†ë„ì™€ ì €ì¥ì†Œ ë¹„ìš©ì„ ì ˆê°, ì¼ê´€ëœ ë°ì´í„°ì…‹ì„ í™•ë³´í•˜ì—¬ ë¶„ì„ ì‹ ë¢°ì„±ì„ ë†’ì„<br>
â–£ ì£¼ìš” ê¸°ë²• : ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì¤‘ë³µ ì œê±°, ê³ ìœ  ì‹ë³„ìë¥¼ í™œìš©í•œ ì¤‘ë³µ íƒì§€(í‚¤ ê¸°ë°˜ í•„í„°ë§), í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°(Jaccard, Cosine Similarity), MinHashë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ì„± íƒì§€, SQL ì¿¼ë¦¬ë¥¼ í™œìš©í•œ ì¤‘ë³µ ì œê±°<br>
â–£ ì¥ì  : ë°ì´í„° í¬ê¸° ê°ì†Œë¡œ ì²˜ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ, ì¤‘ë³µ ë°ì´í„°ë¡œ ì¸í•œ ì™œê³¡ ê°ì†Œ, ë°ì´í„° ì¼ê´€ì„±ê³¼ ì‹ ë¢°ì„± í™•ë³´<br>
â–£ ë‹¨ì  : ìœ ì‚¬ì„± ê¸°ì¤€ì„ ì„¤ì •í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ íƒì§€ ë° ì œê±° ê³¼ì •ì´ ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìŒ, ì˜ëª»ëœ ì œê±°ëŠ” ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ì†ì‹¤ì‹œí‚¬ ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ë° ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ í™œìš©, ë°ì´í„°ì…‹ í¬ê¸°ì— ë¯¼ê°í•œ ì•Œê³ ë¦¬ì¦˜(KNN, êµ°ì§‘í™”)<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-7] ë°ì´í„° ì¤‘ë³µ ì œê±°(Data Deduplication) LogisticRegression
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ì¤‘ë³µ ë°ì´í„° ìƒì„±
	# ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ í–‰ì„ ë³µì‚¬í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€ (ì¤‘ë³µ ë°ì´í„°)
	duplicated_rows = iris_df.iloc[[0, 1]]
	iris_with_duplicates = pd.concat([iris_df, duplicated_rows], ignore_index=True)

	# ì¤‘ë³µ ë°ì´í„° í™•ì¸
	print("Data with duplicates:")
	print(iris_with_duplicates.duplicated().sum(), "duplicate rows added.")

	# 3. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_with_duplicates.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_with_duplicates['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 4. ë°ì´í„° ì¤‘ë³µ ì œê±°
	# ì¤‘ë³µ ë°ì´í„° ì œê±°
	X_no_duplicates = X[~iris_with_duplicates.duplicated()]
	y_no_duplicates = y[~iris_with_duplicates.duplicated()]

	# 5. ë°ì´í„° ë¶„ë¦¬ (ì¤‘ë³µ ì œê±° ì „í›„)
	# - ì¤‘ë³µ ë°ì´í„° í¬í•¨
	X_train_with_duplicates, X_test_with_duplicates, y_train_with_duplicates, 		y_test_with_duplicates = train_test_split(X, y, test_size=0.3, random_state=42)
	# - ì¤‘ë³µ ë°ì´í„° ì œê±°
	X_train_no_duplicates, X_test_no_duplicates, y_train_no_duplicates, y_test_no_duplicates = train_test_split(X_no_duplicates, y_no_duplicates, test_size=0.3, random_state=42)

	# 6. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ì¤‘ë³µ ë°ì´í„° í¬í•¨
	model_with_duplicates = LogisticRegression(max_iter=200)
	model_with_duplicates.fit(X_train_with_duplicates, y_train_with_duplicates)
	y_pred_with_duplicates = model_with_duplicates.predict(X_test_with_duplicates)
	accuracy_with_duplicates = accuracy_score(y_test_with_duplicates, y_pred_with_duplicates)

	# (2) ì¤‘ë³µ ë°ì´í„° ì œê±°
	model_no_duplicates = LogisticRegression(max_iter=200)
	model_no_duplicates.fit(X_train_no_duplicates, y_train_no_duplicates)
	y_pred_no_duplicates = model_no_duplicates.predict(X_test_no_duplicates)
	accuracy_no_duplicates = accuracy_score(y_test_no_duplicates, y_pred_no_duplicates)

	# 7. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy with duplicates: {accuracy_with_duplicates:.2f}")
	print(f"Accuracy without duplicates: {accuracy_no_duplicates:.2f}")

	# 8. ê²°ê³¼ ë¹„êµ ë¶„ì„
	if accuracy_with_duplicates > accuracy_no_duplicates:
	    print("Including duplicates improved accuracy, but it may indicate overfitting.")
	elif accuracy_with_duplicates == accuracy_no_duplicates:
	    print("Duplicates had no effect on the model's accuracy.")
	else:
	    print("Removing duplicates improved the model's accuracy.")

<br>

	Data with duplicates: 3 duplicate rows added.
	Accuracy with duplicates: 1.00
	Accuracy without duplicates: 1.00
	Duplicates had no effect on the model's accuracy.

<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-7] ë°ì´í„° ì¤‘ë³µ ì œê±°(Data Deduplication) RandomForestClassifier
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.metrics import accuracy_score

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ì¤‘ë³µ ë°ì´í„° ìƒì„±
	# ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ í–‰ì„ ë³µì‚¬í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€ (ì¤‘ë³µ ë°ì´í„°)
	duplicated_rows = iris_df.iloc[[0, 1]]
	iris_with_duplicates = pd.concat([iris_df, duplicated_rows], ignore_index=True)

	# ì¤‘ë³µ ë°ì´í„° í™•ì¸
	print("Data with duplicates:")
	print(iris_with_duplicates.duplicated().sum(), "duplicate rows added.")

	# 3. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_with_duplicates.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_with_duplicates['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 4. ë°ì´í„° ì¤‘ë³µ ì œê±°
	# ì¤‘ë³µ ë°ì´í„° ì œê±°
	X_no_duplicates = X[~iris_with_duplicates.duplicated()]
	y_no_duplicates = y[~iris_with_duplicates.duplicated()]

	# 5. ë°ì´í„° ë¶„ë¦¬ (ì¤‘ë³µ ì œê±° ì „í›„)
	# - ì¤‘ë³µ ë°ì´í„° í¬í•¨
	X_train_with_duplicates, X_test_with_duplicates, y_train_with_duplicates, 		y_test_with_duplicates = train_test_split(X, y, test_size=0.3, random_state=42)

	# - ì¤‘ë³µ ë°ì´í„° ì œê±°
	X_train_no_duplicates, X_test_no_duplicates, y_train_no_duplicates, y_test_no_duplicates = train_test_split(X_no_duplicates, y_no_duplicates, test_size=0.3, random_state=42)

	# 6. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ì¤‘ë³µ ë°ì´í„° í¬í•¨
	model_with_duplicates = RandomForestClassifier(random_state=42)
	model_with_duplicates.fit(X_train_with_duplicates, y_train_with_duplicates)
	y_pred_with_duplicates = model_with_duplicates.predict(X_test_with_duplicates)
	accuracy_with_duplicates = accuracy_score(y_test_with_duplicates, y_pred_with_duplicates)

	# (2) ì¤‘ë³µ ë°ì´í„° ì œê±°
	model_no_duplicates = RandomForestClassifier(random_state=42)
	model_no_duplicates.fit(X_train_no_duplicates, y_train_no_duplicates)
	y_pred_no_duplicates = model_no_duplicates.predict(X_test_no_duplicates)
	accuracy_no_duplicates = accuracy_score(y_test_no_duplicates, y_pred_no_duplicates)

	# 7. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy with duplicates (Random Forest): {accuracy_with_duplicates:.2f}")
	print(f"Accuracy without duplicates (Random Forest): {accuracy_no_duplicates:.2f}")

	# 8. ê²°ê³¼ ë¹„êµ ë¶„ì„
	if accuracy_with_duplicates > accuracy_no_duplicates:
	    print("Including duplicates improved accuracy, but it may indicate overfitting.")
	elif accuracy_with_duplicates == accuracy_no_duplicates:
	    print("Duplicates had no effect on the model's accuracy.")
	else:
	    print("Removing duplicates improved the model's accuracy.")

<br>

	Data with duplicates:3 duplicate rows added.
	Accuracy with duplicates (Random Forest): 0.96
	Accuracy without duplicates (Random Forest): 1.00
	Removing duplicates improved the model's accuracy.

<br>    

## [1-8] ë°ì´í„° ë³€í™˜(Data Transformation)
â–£ ì •ì˜ : ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜, ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ë°ì´í„°ë¥¼ ë³€í˜•<br>
â–£ í•„ìš”ì„± : ë°ì´í„° ë¶„í¬ë¥¼ ì¡°ì •í•˜ì—¬ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ ìµœì í™”, ì…ë ¥ ë°ì´í„°ê°€ ì•Œê³ ë¦¬ì¦˜ì˜ ìš”êµ¬ì‚¬í•­ì— ë§ë„ë¡ ì¤€ë¹„, ì´ìƒì¹˜, ë¶ˆê· í˜• ë°ì´í„° ë“±ì˜ ì˜í–¥ì„ ìµœì†Œí™”<br>
â–£ ì£¼ìš” ê¸°ë²• : ë¡œê·¸ ë³€í™˜(ë¹„ëŒ€ì¹­ ë¶„í¬ë¥¼ ì •ê·œ ë¶„í¬ë¡œ ì¡°ì •), ìŠ¤ì¼€ì¼ë§(Min-Max, Standardization), ë²”ì£¼í˜• ë³€í™˜(ì›-í•« ì¸ì½”ë”©, ë¼ë²¨ ì¸ì½”ë”©), ì°¨ì› ì¶•ì†Œ(PCA, t-SNE)<br>
â–£ ì¥ì  : ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ íš¨ê³¼ ì¦ê°€, ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ë³´, í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì—¬ ë°ì´í„° ì´í•´ë„ í–¥ìƒ<br>
â–£ ë‹¨ì  : ì ì ˆí•œ ë³€í™˜ ê¸°ë²• ì„ íƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ, ì›ë³¸ ë°ì´í„°ì˜ ì˜ë¯¸ê°€ ì™œê³¡ë  ê°€ëŠ¥ì„±, ê³ ì°¨ì› ë°ì´í„°ì—ì„œëŠ” ë³€í™˜ ë¹„ìš© ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : íšŒê·€ ëª¨ë¸, ë”¥ëŸ¬ë‹, PCA, SVM ë“±<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-8] ë°ì´í„° ë³€í™˜(Data Transformation) - iris data
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score
	from sklearn.preprocessing import StandardScaler, MinMaxScaler

	# 1. Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris(as_frame=True)
	iris_df = iris.frame

	# 2. ë°ì´í„° ì¤€ë¹„ (ì…ë ¥ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬)
	X = iris_df.iloc[:, :-1]  # ì…ë ¥ íŠ¹ì„± (ê½ƒë°›ì¹¨, ê½ƒì)
	y = iris_df['target']     # íƒ€ê²Ÿ (í´ë˜ìŠ¤)

	# 3. ë°ì´í„° ë³€í™˜
	# (1) ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
	scaler = StandardScaler()
	X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

	# (2) ë¡œê·¸ ë³€í™˜
	X_log_transformed = np.log1p(X)

	# 4. ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì…‹)
	# ì›ë³¸ ë°ì´í„°
	X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.3, random_state=42)
	# ìŠ¤ì¼€ì¼ë§ ë°ì´í„°
	X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = 	train_test_split(X_scaled, y, test_size=0.3, random_state=42)
	# ë¡œê·¸ ë³€í™˜ ë°ì´í„°
	X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log_transformed, y, test_size=0.3, random_state=42)

	# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	# (1) ì›ë³¸ ë°ì´í„°
	model_raw = LogisticRegression(max_iter=200)
	model_raw.fit(X_train_raw, y_train_raw)
	y_pred_raw = model_raw.predict(X_test_raw)
	accuracy_raw = accuracy_score(y_test_raw, y_pred_raw)

	# (2) ìŠ¤ì¼€ì¼ë§ ë°ì´í„°
	model_scaled = LogisticRegression(max_iter=200)
	model_scaled.fit(X_train_scaled, y_train_scaled)
	y_pred_scaled = model_scaled.predict(X_test_scaled)
	accuracy_scaled = accuracy_score(y_test_scaled, y_pred_scaled)

	# (3) ë¡œê·¸ ë³€í™˜ ë°ì´í„°
	model_log = LogisticRegression(max_iter=200)
	model_log.fit(X_train_log, y_train_log)
	y_pred_log = model_log.predict(X_test_log)
	accuracy_log = accuracy_score(y_test_log, y_pred_log)

	# 6. ê²°ê³¼ ì¶œë ¥
	print(f"\nAccuracy with raw data: {accuracy_raw:.2f}")
	print(f"Accuracy with scaled data (StandardScaler): {accuracy_scaled:.2f}")
	print(f"Accuracy with log-transformed data: {accuracy_log:.2f}")

	# 7. ê²°ê³¼ ë¹„êµ ë¶„ì„
	if max(accuracy_raw, accuracy_scaled, accuracy_log) == accuracy_raw:
	    print("Raw data provided the highest accuracy.")
	elif max(accuracy_raw, accuracy_scaled, accuracy_log) == accuracy_scaled:
	    print("Scaled data provided the highest accuracy.")
	elif max(accuracy_raw, accuracy_scaled, accuracy_log) == accuracy_log:
	    print("Log-transformed data provided the highest accuracy.")
	else:
	    print("Multiple methods resulted in the same accuracy.")

<br>

	Accuracy with raw data: 1.00
	Accuracy with scaled data (StandardScaler): 1.00
	Accuracy with log-transformed data: 0.93
	Raw data provided the highest accuracy.

<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-8] ë°ì´í„° ë³€í™˜(Data Transformation) - housing.csv data
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
	from sklearn.impute import SimpleImputer
	import math

	# 1. ë°ì´í„° ë¡œë“œ
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	data = pd.read_csv(url)

	# ë°ì´í„° í™•ì¸
	print("California housing dataset loaded successfully!")
	print("\nFeature statistics before transformation:")
	print(data.describe(include="all"))

	# 2. ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬
	encoder = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
	encoded_columns = encoder.fit_transform(data[["ocean_proximity"]])
	encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(["ocean_proximity"]))

	# ì›ë³¸ ë°ì´í„°ì—ì„œ `ocean_proximity` ì œê±° í›„ ì¸ì½”ë”©ëœ ë°ì´í„° ì¶”ê°€
	data = pd.concat([data.drop(columns=["ocean_proximity"]), encoded_df], axis=1)

	# 3. ê²°ì¸¡ê°’ ì²˜ë¦¬
	imputer = SimpleImputer(strategy="mean")
	data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

	# ë°ì´í„° ì¤€ë¹„
	X = data_imputed.drop(columns=["median_house_value"])  # íŠ¹ì„± ë°ì´í„°
	y = data_imputed["median_house_value"]  # íƒ€ê¹ƒ ë³€ìˆ˜

	# 4. ë°ì´í„° ë³€í™˜
	# (1) ì›ë³¸ ë°ì´í„°
	X_raw = X.copy()

	# (2) ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
	scaler = StandardScaler()
	X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

	# (3) ë¡œê·¸ ë³€í™˜
	X_log_transformed = X.copy()
	for column in X_log_transformed.columns:
	    if (X_log_transformed[column] <= 0).any():
	        X_log_transformed[column] += abs(X_log_transformed[column].min()) + 1
	X_log_transformed = np.log1p(X_log_transformed)
	X_log_transformed = pd.DataFrame(
	    SimpleImputer(strategy="mean").fit_transform(X_log_transformed), columns=X.columns)

	# (4) Min-Max Scaling
	minmax_scaler = MinMaxScaler()
	X_minmax_scaled = pd.DataFrame(minmax_scaler.fit_transform(X), columns=X.columns)

	# 5. ë°ì´í„° ë¶„ë¦¬
	X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_raw, y, test_size=0.3, random_state=42)
	X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
	X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log_transformed, y, test_size=0.3, random_state=42)
	X_train_minmax, X_test_minmax, y_train_minmax, y_test_minmax = train_test_split(X_minmax_scaled, y, test_size=0.3, random_state=42)

	# 6. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	def calculate_rmse(mse):
	    return math.sqrt(mse)

	def evaluate_model(model, X_train, X_test, y_train, y_test):
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		mse = mean_squared_error(y_test, y_pred)
		mae = mean_absolute_error(y_test, y_pred)
		rmse = calculate_rmse(mse)
		r2 = r2_score(y_test, y_pred)
		evs = explained_variance_score(y_test, y_pred)
		return mse, mae, rmse, r2, evs

	# í‰ê°€ ê²°ê³¼ ì €ì¥
	results = {}

	# (1) ì›ë³¸ ë°ì´í„°
	model_raw = LinearRegression()
	results["Raw"] = evaluate_model(model_raw, X_train_raw, X_test_raw, y_train_raw, y_test_raw)

	# (2) ìŠ¤ì¼€ì¼ë§ ë°ì´í„°
	model_scaled = LinearRegression()
	results["Standard Scaled"] = evaluate_model(model_scaled, X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled)

	# (3) ë¡œê·¸ ë³€í™˜ ë°ì´í„°
	model_log = LinearRegression()
	results["Log-transformed"] = evaluate_model(model_log, X_train_log, X_test_log, y_train_log, y_test_log)

	# (4) Min-Max Scaling ë°ì´í„°
	model_minmax = LinearRegression()
	results["Min-Max Scaled"] = evaluate_model(model_minmax, X_train_minmax, 	X_test_minmax, y_train_minmax, y_test_minmax)

	# 7. ê²°ê³¼ ì¶œë ¥
	print("\nEvaluation Results (MSE, MAE, RMSE, R2 Score, Explained Variance Score):")
	for key, (mse, mae, rmse, r2, evs) in results.items():
	    print(f"{key}: MSE = {mse:.2f}, MAE = {mae:.2f}, RMSE = {rmse:.2f}, R2 = {r2:.4f}, EVS = {evs:.4f}")

	# 8. ê²°ê³¼ ë¹„êµ ë¶„ì„
	best_r2 = max(results[key][3] for key in results)
	if best_r2 == results["Raw"][3]:
	    print("\nRaw data provided the best RÂ² score.")
	elif best_r2 == results["Standard Scaled"][3]:
	    print("\nStandard scaling provided the best RÂ² score.")
	elif best_r2 == results["Log-transformed"][3]:
	    print("\nLog transformation provided the best RÂ² score.")
	elif best_r2 == results["Min-Max Scaled"][3]:
	    print("\nMin-Max scaling provided the best RÂ² score.")
	else:
	    print("\nMultiple transformations resulted in the same RÂ² score.")

<br>

![](./images/1-8.png) 
<br>

## [1-9] íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)
â–£ ì •ì˜ : ëª¨ë¸ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ê±°ë‚˜ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ìƒì„±í•˜ê±°ë‚˜ ë³€í™˜, ì„ íƒ ë“±ì˜ ì‘ì—…<br>
â–£ í•„ìš”ì„± : ê³ í’ˆì§ˆ íŠ¹ì„±ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ, ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ ë°˜ì˜í•˜ì—¬ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€, íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±° ê°€ëŠ¥<br>
â–£ ì£¼ìš” ê¸°ë²• : ìˆ˜í•™ì  ì¡°í•©ê³¼ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ íŒŒìƒ íŠ¹ì„± ìƒì„±, ë¡œê·¸ ë³€í™˜ê³¼ ë‹¤í•­ì‹ ë³€í™˜ìœ¼ë¡œ íŠ¹ì„± ë³€í™˜, ëª¨ë¸ ê¸°ë°˜ ì„ íƒ(Lasso, XGBoost), ì¤‘ìš”ë„ í‰ê°€(LIME, SHAP)* ë° ì°¨ì› ì¶•ì†Œ(PCA, t-SNE)ë¡œ íŠ¹ì„± ì„ íƒ<br>
â–£ ì¥ì  : ëª¨ë¸ ì„±ëŠ¥ì„ í° í­ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥, ë„ë©”ì¸ ì§€ì‹ì„ ë°˜ì˜í•˜ì—¬ ë” ë‚˜ì€ í•´ì„ ê°€ëŠ¥, ì¤‘ìš”í•˜ì§€ ì•Šì€ íŠ¹ì„±ì„ ì œê±°í•´ í•™ìŠµ ì†ë„ í–¥ìƒ<br>
â–£ ë‹¨ì  : ë†’ì€ ë„ë©”ì¸ ì§€ì‹ ìš”êµ¬, ì‹œê°„ê³¼ ìì› ì†Œëª¨, ì˜ëª»ëœ íŠ¹ì„± ìƒì„±ì€ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì„ í˜• ëª¨ë¸, ì˜ì‚¬ê²°ì • ë‚˜ë¬´, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë”¥ëŸ¬ë‹ ë“± ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜<br>
**LIM(Local Interpretable Model-agnostic Explanations)** : ëª¨ë¸ì— ê´€ê³„ì—†ì´ ë¡œì»¬(Local) ë‹¨ìœ„ì—ì„œ íŠ¹ì • ì˜ˆì¸¡ì— ëŒ€í•´ ëª¨ë¸ì´ ì™œ ê·¸ì™€ ê°™ì€ ê²°ì •ì„ ë‚´ë ¸ëŠ”ì§€ë¥¼ ì„¤ëª…(ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ê° íŠ¹ì„±(feature)ì´ ì˜ˆì¸¡ê°’ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ ë‚˜íƒ€ëƒ„)<br>
**SHAP(SHapley Additive exPlanations)** : ê° íŠ¹ì„±ì´ ì˜ˆì¸¡ê°’ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ê³µì •í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ, ëª¨ë“  íŠ¹ì„± ì¡°í•©ì—ì„œì˜ í‰ê·  ê¸°ì—¬ë„ë¥¼ ê³„ì‚°<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-9] íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)
	#############################################################
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score
	from sklearn.preprocessing import PolynomialFeatures
	import numpy as np
	import pandas as pd

	# 1. ë°ì´í„° ë¡œë“œ ë° ë…¸ì´ì¦ˆ ì¶”ê°€
	iris = load_iris()
	X = pd.DataFrame(iris.data, columns=iris.feature_names)
	y = iris.target

	# ë…¸ì´ì¦ˆ ì¶”ê°€
	np.random.seed(42)
	X_noisy = X + np.random.normal(0, 0.5, X.shape)

	# 2. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ ì „ í‰ê°€
	X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_noisy, y, test_size=0.3, random_state=42)
	model_raw = LogisticRegression(max_iter=500)
	model_raw.fit(X_train_raw, y_train)
	y_pred_raw = model_raw.predict(X_test_raw)
	accuracy_raw = accuracy_score(y_test, y_pred_raw)

	# 3. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰: ìƒí˜¸ì‘ìš© íŠ¹ì„± ì¶”ê°€
	poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
	X_fe = pd.DataFrame(poly.fit_transform(X_noisy), columns=poly.get_feature_names_out(X.columns))

	# 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í›„ í‰ê°€
	X_train_fe, X_test_fe, y_train, y_test = train_test_split(X_fe, y, test_size=0.3, random_state=42)
	model_fe = LogisticRegression(max_iter=500)
	model_fe.fit(X_train_fe, y_train)
	y_pred_fe = model_fe.predict(X_test_fe)
	accuracy_fe = accuracy_score(y_test, y_pred_fe)

	# 5. ê²°ê³¼ ì¶œë ¥
	print(f"Accuracy before Feature Engineering: {accuracy_raw:.2f}")
	print(f"Accuracy after Feature Engineering: {accuracy_fe:.2f}")

	if accuracy_fe > accuracy_raw:
	    print("\nFeature Engineering improved the model's performance!")
	elif accuracy_fe == accuracy_raw:
	    print("\nFeature Engineering did not affect the model's performance.")
	else:
	    print("\nFeature Engineering decreased the model's performance.")

<br>

	Accuracy before Feature Engineering: 0.89
	Accuracy after Feature Engineering: 0.91
	Feature Engineering improved the model's performance!

<br>

## [1-10] ì •ë³´ ë³‘í•©(Data Fusion)
â–£ ì •ì˜ : ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ê²°í•©í•˜ì—¬ ë‹¨ì¼í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ë°ì´í„°ì…‹ì„ ìƒì„±<br>
â–£ í•„ìš”ì„± : ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ë” í’ë¶€í•œ ì •ë³´ë¥¼ ì œê³µ, ë°ì´í„° ì¤‘ë³µ ë° ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•˜ì—¬ ë¶„ì„ ê°€ëŠ¥ì„±ì„ ë†’ì„<br>
â–£ ì£¼ìš” ê¸°ë²• : ì„œë¡œ ë‹¤ë¥¸ ìŠ¤í‚¤ë§ˆ ê°„ ë§¤í•‘ ì •ì˜(Data Mapping), ë™ì¼í•œ ë°ì´í„° í¬ì¸íŠ¸ ì¤‘ë³µ ì œê±°(Deduplication), ë‹¤ì–‘í•œ í˜•ì‹ì„ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì •ê·œí™”(Normalization)<br>
â–£ ì¥ì  : ë°ì´í„° í™œìš© ê°€ëŠ¥ì„± ì¦ëŒ€, ë³µí•©ì ì¸ ë¬¸ì œì— ëŒ€í•œ ë‹¤ê°ì  ë¶„ì„ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ë°ì´í„° ì†ŒìŠ¤ ê°„ ì¼ì¹˜ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ, í†µí•© ê·œì¹™ ì„¤ì •ê³¼ ë³€í™˜ ê³¼ì •ì´ ë³µì¡<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë°ì´í„° í†µí•© í›„ ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì— ì ìš© ê°€ëŠ¥<br>

	#############################################################
	# [1] ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
	# [1-10] ì •ë³´ ë³‘í•©(Data Fusion)
	#############################################################
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import r2_score
	from sklearn.datasets import load_iris
	import pandas as pd
	import matplotlib.pyplot as plt
	import matplotlib.font_manager as fm

	# í•œê¸€ í°íŠ¸ ì„¤ì •
	plt.rc('font', family='Malgun Gothic')  # Windowsì˜ ë§‘ì€ ê³ ë”• í°íŠ¸
	plt.rcParams['axes.unicode_minus'] = False  # í•œê¸€ í°íŠ¸ ì„¤ì • ì‹œ ìŒìˆ˜ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€

	# Iris ë°ì´í„°ì…‹ ë¡œë“œ
	iris = load_iris()
	iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
	iris_df['target'] = iris.target

	# ì»¬ëŸ¼ ì´ë¦„ì„ ê°„ë‹¨í•˜ê²Œ ë³€ê²½
	iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'target']

	# 1ë‹¨ê³„: 'sepal_length'ì™€ 'sepal_width'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° ì„ í˜• íšŒê·€ ë¶„ì„ì„ ì§„í–‰
	X_initial = iris_df[['sepal_length', 'sepal_width']]
	y = iris_df['target']

	# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
	X_train_initial, X_test_initial, y_train, y_test = train_test_split(X_initial, y, test_size=0.2, random_state=42)

	# 2ë‹¨ê³„: ì´ˆê¸° ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
	linear_reg = LinearRegression()
	linear_reg.fit(X_train_initial, y_train)

	# 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ê³  R2 ì ìˆ˜ ê³„ì‚°
	y_pred_initial = linear_reg.predict(X_test_initial)
	r2_initial = r2_score(y_test, y_pred_initial)

	# 4ë‹¨ê³„: 'petal_length'ì™€ 'petal_width'ë¥¼ ì¶”ê°€í•˜ì—¬ ë°ì´í„° ë³‘í•©
	X_combined = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]

	# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
	X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

	# 5ë‹¨ê³„: ë‹¤ì¤‘ ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
	multiple_linear_reg = LinearRegression()
	multiple_linear_reg.fit(X_train_combined, y_train)

	# 6ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ê³  R2 ì ìˆ˜ ê³„ì‚°
	y_pred_combined = multiple_linear_reg.predict(X_test_combined)
	r2_combined = r2_score(y_test, y_pred_combined)

	# ê²°ê³¼ ì¶œë ¥
	print("ì´ˆê¸° ëª¨ë¸ì˜ R2 ì ìˆ˜ (sepal_length, sepal_width):", r2_initial)
	print("ë³‘í•© ëª¨ë¸ì˜ R2 ì ìˆ˜ (sepal_length, sepal_width, petal_length, petal_width):", r2_combined)

	# ì‹œê°í™”
	# ì´ˆê¸° ë°ì´í„° ì‹œê°í™” (sepal_lengthì™€ sepal_widthë§Œ ì‚¬ìš©)
	plt.figure(figsize=(12, 6))

	# ì´ˆê¸° ë°ì´í„° ì‚°ì ë„
	plt.subplot(1, 2, 1)
	for target, color, label in zip([0, 1, 2], ['red', 'blue', 'green'], iris.target_names):
	    subset = iris_df[iris_df['target'] == target]
	    plt.scatter(subset['sepal_length'], subset['sepal_width'], c=color, label=label, edgecolor='k')
	plt.title('ì´ˆê¸° ë°ì´í„° (Sepal Length vs. Sepal Width)')
	plt.xlabel('Sepal Length')
	plt.ylabel('Sepal Width')
	plt.legend()
	plt.colorbar(label='Target')

	# ë³‘í•© ë°ì´í„° ì‹œê°í™” (petal_lengthì™€ petal_width ì‚¬ìš©)
	plt.subplot(1, 2, 2)
	for target, color, label in zip([0, 1, 2], ['red', 'blue', 'green'], iris.target_names):
	    subset = iris_df[iris_df['target'] == target]
	    plt.scatter(subset['petal_length'], subset['petal_width'], c=color, label=label, edgecolor='k')
	plt.title('ë³‘í•© ë°ì´í„° (Sepal, Petal Length vs. Sepal, Petal Width)')
	plt.xlabel('Sepal, Petal Length')
	plt.ylabel('Sepal, Petal Width')
	plt.legend()

	plt.tight_layout()
	plt.show()

<br>

![](./images/1-10.png) 
<br>

---
  
# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™” : ê³¼ì í•© ë°©ì§€(Overfitting Prevention)
â–£ ì •ì˜ : ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ì ì‘í•˜ì§€ ì•Šë„ë¡ ì œì–´í•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œë„ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ë‹¤ì–‘í•œ ê¸°ë²•ì˜ ì¡°í•©<br>
â–£ í•„ìš”ì„± : ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë‚˜ ë¶ˆí•„ìš”í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë‚˜ ì‹¤ì „ ë°ì´í„°ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ë³´ì¥<br>
â–£ ì£¼ìš” ê¸°ë²• : ë°ì´í„° ê´€ë ¨(Data Augmentation, Cross Validation), ëª¨ë¸ ê´€ë ¨(Model Simplification, Regularization, Dropout), í›ˆë ¨ ê´€ë ¨(Early Stopping)<br>
â–£ ì¥ì  : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œì˜ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ë³´, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ, ì˜ˆì¸¡ ëª¨ë¸ì˜ ì‹ ë¢°ë„ ì¦ê°€<br>
â–£ ë‹¨ì  : ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì´ ê³¼ë„í•˜ê²Œ ì ìš©ë˜ë©´ ê³¼ì†Œì í•©(Underfitting), ìµœì ì˜ ì„¤ì •ì„ ì°¾ê¸° ìœ„í•œ ì¶”ê°€ì ì¸ ì‹¤í—˜ê³¼ ì¡°ì •ì´ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

<br>

## [2-1] ì •ê·œí™”(L1, L2 Regularization)
â–£ ì •ì˜ : ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜ì— íŒ¨ë„í‹°ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ë¥¼ ì œì–´(L1 ì •ê·œí™”: ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©, L2 ì •ê·œí™”: ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©)<br>
â–£ í•„ìš”ì„± : ëª¨ë¸ì´ ë¶ˆí•„ìš”í•˜ê²Œ í° ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ì—¬ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€<br>
â–£ ì£¼ìš” ê¸°ë²• : L1 ì •ê·œí™”(Lasso Regression), L2 ì •ê·œí™”(Ridge Regression), L1ê³¼ L2 í˜¼í•©(Elastic Net)<br>
â–£ ì¥ì  : L1ì€ í¬ì†Œ ëª¨ë¸(sparse model)ì„ ìƒì„±í•˜ì—¬ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì„ íƒí•˜ëŠ” ë° ìœ ìš©, L2ëŠ” ê³¼ë„í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì—¬ ëª¨ë¸ ì•ˆì •ì„± í–¥ìƒ<br>
â–£ ë‹¨ì  : ê³¼ì†Œì í•© ê°€ëŠ¥ì„±, ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì •í•˜ê¸° ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°(Î») ì„ íƒ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì„ í˜• íšŒê·€(Linear Regression), ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression), ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM), ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬<br>

	#############################################################
	# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™”
	# [2-1] ì •ê·œí™” (L1, L2 Regularization) - iris data
	#############################################################
	from sklearn.model_selection import train_test_split, cross_val_score
	from sklearn.linear_model import LinearRegression, Ridge, Lasso
	from sklearn.metrics import r2_score
	from sklearn.datasets import load_iris
	import numpy as np

	# Iris ë°ì´í„°ì…‹ ë¡œë“œ
	iris = load_iris()
	X = iris.data  # íŠ¹ì„±: sepal_length, sepal_width, petal_length, petal_width
	y = iris.target  # íƒ€ê²Ÿ: í’ˆì¢…

	# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# (1) ê¸°ë³¸ ì„ í˜• íšŒê·€ ë¶„ì„
	linear_reg = LinearRegression()
	linear_reg.fit(X_train, y_train)
	y_pred_basic = linear_reg.predict(X_test)
	r2_basic = r2_score(y_test, y_pred_basic)

	# (2) ì •ê·œí™” (ë¦¿ì§€ íšŒê·€ì™€ ë¼ì˜ íšŒê·€)
	ridge_reg = Ridge(alpha=1.0)  # ë¦¿ì§€ íšŒê·€
	ridge_reg.fit(X_train, y_train)
	y_pred_ridge = ridge_reg.predict(X_test)
	r2_ridge = r2_score(y_test, y_pred_ridge)

	lasso_reg = Lasso(alpha=0.1)  # ë¼ì˜ íšŒê·€
	lasso_reg.fit(X_train, y_train)
	y_pred_lasso = lasso_reg.predict(X_test)
	r2_lasso = r2_score(y_test, y_pred_lasso)

	# (3) ë“œë¡­ì•„ì›ƒ (ë“œë¡­ì•„ì›ƒì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, Scikit-learnì—ì„œëŠ” ì§ì ‘ ì ìš© ë¶ˆê°€)
	# ëŒ€ì‹  ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ íŠ¹ì„± ì œê±°ë‚˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.

	# (4) êµì°¨ ê²€ì¦ (ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì— ëŒ€í•´ ìˆ˜í–‰)
	cv_scores = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')
	r2_cv = np.mean(cv_scores)

	# ê²°ê³¼ ì¶œë ¥
	r2_results = {
	    "ê¸°ë³¸ ì„ í˜• íšŒê·€ (R2)": r2_basic,
	    "ë¦¿ì§€ íšŒê·€ (R2)": r2_ridge,
	    "ë¼ì˜ íšŒê·€ (R2)": r2_lasso,
	    "êµì°¨ ê²€ì¦ í‰ê·  (R2)": r2_cv}

	# ê²°ê³¼ í™•ì¸
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	for method, r2 in r2_results.items():
	    print(f"{method}: {r2}")

<br>

	R2 ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ (R2): 0.9468960016420045
	ë¦¿ì§€ íšŒê·€ (R2): 0.9440579987200237
	ë¼ì˜ íšŒê·€ (R2): 0.9044577045136053
	êµì°¨ ê²€ì¦ í‰ê·  (R2): 0.3225607248900085

<br>

	#############################################################
	# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™”
	# [2-1] ì •ê·œí™” (L1, L2 Regularization) - titanic data
	#############################################################
	import pandas as pd
	from sklearn.model_selection import train_test_split, cross_val_score
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet
	from sklearn.metrics import r2_score

	# 1. ë°ì´í„° ë¡œë“œ
	url = 'https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/titanic_train.csv'
	data = pd.read_csv(url)

	# 2. íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
	X = data.drop(columns=['Survived'])
	y = data['Survived']

	# 3. ìˆ˜ì¹˜í˜• ë° ë²”ì£¼í˜• ë³€ìˆ˜ ì‹ë³„
	numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']
	categorical_features = ['Pclass', 'Sex', 'Embarked']

	# 4. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
	numeric_transformer = Pipeline(steps=[
	    ('imputer', SimpleImputer(strategy='median')),
	    ('scaler', StandardScaler())])

	categorical_transformer = Pipeline(steps=[
	    ('imputer', SimpleImputer(strategy='most_frequent')),
	    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', numeric_transformer, numeric_features),
	        ('cat', categorical_transformer, categorical_features)])

	# 5. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬ì„±
	linear_model = Pipeline(steps=[('preprocessor', preprocessor),
	                               ('classifier', LogisticRegression(max_iter=1000))])

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 6. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	linear_model.fit(X_train, y_train)
	y_pred = linear_model.predict(X_test)
	r2_basic = r2_score(y_test, y_pred)

	# 7. ì •ê·œí™” ì ìš© (ë¦¿ì§€ íšŒê·€)
	ridge_model = Pipeline(steps=[('preprocessor', preprocessor),
	                              ('classifier', Ridge(alpha=1.0))])
	ridge_model.fit(X_train, y_train)
	y_pred_ridge = ridge_model.predict(X_test)
	r2_ridge = r2_score(y_test, y_pred_ridge)

	# 8. ì •ê·œí™” ì ìš© (ë¼ì˜ íšŒê·€)
	lasso_model = Pipeline(steps=[('preprocessor', preprocessor),
	                              ('classifier', Lasso(alpha=0.1))])
	lasso_model.fit(X_train, y_train)
	y_pred_lasso = lasso_model.predict(X_test)
	r2_lasso = r2_score(y_test, y_pred_lasso)

	# 9. ì •ê·œí™” ì ìš© (ì—˜ë¼ìŠ¤í‹±ë„·)
	elastic_model = Pipeline(steps=[('preprocessor', preprocessor),
	                                 ('classifier', ElasticNet(alpha=0.1, l1_ratio=0.5))])
	elastic_model.fit(X_train, y_train)
	y_pred_elastic = elastic_model.predict(X_test)
	r2_elastic = r2_score(y_test, y_pred_elastic)

	# 10. êµì°¨ ê²€ì¦ ì ìš©
	cv_scores = cross_val_score(linear_model, X, y, cv=5, scoring='r2')
	r2_cv = cv_scores.mean()

	# 11. ê²°ê³¼ ì¶œë ¥
	print("RÂ² ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_basic:.4f}")
	print(f"ë¦¿ì§€ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_ridge:.4f}")
	print(f"ë¼ì˜ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_lasso:.4f}")
	print(f"ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_elastic:.4f}")
	print(f"êµì°¨ ê²€ì¦ì„ í†µí•œ í‰ê·  RÂ² ì ìˆ˜: {r2_cv:.4f}")

<br>

	RÂ² ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.1707
	ë¦¿ì§€ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.4334
	ë¼ì˜ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.1085
	ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.2678
	êµì°¨ ê²€ì¦ì„ í†µí•œ í‰ê·  RÂ² ì ìˆ˜: 0.1125

<br>

## [2-2] ì¡°ê¸° ì¢…ë£Œ(Early Stopping)
â–£ ì •ì˜ : í•™ìŠµ ì¤‘ ê²€ì¦ ì„¸íŠ¸ì˜ ì„±ëŠ¥ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•ŠëŠ” ì‹œì ì—ì„œ í•™ìŠµì„ ì¤‘ë‹¨<br>
â–£ í•„ìš”ì„± : í•™ìŠµì„ ë„ˆë¬´ ì˜¤ë˜ ì§„í–‰í•  ê²½ìš° ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©ë  ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•¨<br>
â–£ ì£¼ìš” ê¸°ë²• : ê²€ì¦ ì†ì‹¤ ëª¨ë‹ˆí„°ë§(ê²€ì¦ ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° ì¤‘ë‹¨), Patience ì„¤ì •(íŠ¹ì • ì—í¬í¬ ë™ì•ˆ í–¥ìƒì´ ì—†ì„ ë•Œ ì¢…ë£Œ)<br>
â–£ ì¥ì  : ê³¼ì í•© ë°©ì§€, ë¶ˆí•„ìš”í•œ í•™ìŠµ ì‹œê°„ ì ˆì•½<br>
â–£ ë‹¨ì  : ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ì„ ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ê±°ë‚˜, ìµœì ì˜ ì¢…ë£Œ ì‹œì  ê²°ì • ê³¤ë€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë”¥ëŸ¬ë‹ ëª¨ë¸, ì¼ë¶€ ë¨¸ì‹ ëŸ¬ë‹(Gradient Boosting) ë“± ì—í­(Epoch)*ì´ ê¸´ ê²½ìš°<br>
   * ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ì „ì²´ ë°ì´í„°ì…‹ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ëŠ” íšŸìˆ˜(ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìƒ˜í”Œì´ ëª¨ë¸ì— ì…ë ¥ë˜ì–´ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ë˜ëŠ” ê³¼ì •ì„ í•œë²ˆ ì™„ë£Œí•˜ëŠ” ê²ƒì´ 1 ì—í­)<br>

	#############################################################
	# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™”
	# [2-2] ì¡°ê¸° ì¢…ë£Œ (Early Stopping)
	#############################################################
	import pandas as pd
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.linear_model import SGDRegressor
	from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

	# 1. ë°ì´í„° ë¡œë“œ
	url = 'https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/titanic_train.csv'
	data = pd.read_csv(url)

	# 2. íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
	X = data.drop(columns=['Survived'])
	y = data['Survived']

	# 3. ìˆ˜ì¹˜í˜• ë° ë²”ì£¼í˜• ë³€ìˆ˜ ì‹ë³„
	numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']
	categorical_features = ['Pclass', 'Sex', 'Embarked']

	# 4. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
	numeric_transformer = Pipeline(steps=[
	    ('imputer', SimpleImputer(strategy='median')),
	    ('scaler', StandardScaler())])

	categorical_transformer = Pipeline(steps=[
	    ('imputer', SimpleImputer(strategy='most_frequent')),
	    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

	preprocessor = ColumnTransformer(
	    transformers=[('num', numeric_transformer, numeric_features),
	        ('cat', categorical_transformer, categorical_features)])

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 5. ê¸°ë³¸ ë‹¤í•­ íšŒê·€ ëª¨ë¸ êµ¬ì„± (SGDRegressor ì‚¬ìš©)
	polynomial_model = Pipeline(steps=[
	    ('preprocessor', preprocessor),
	    ('poly_features', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)),
	    ('scaler', StandardScaler()),
	    ('regressor', SGDRegressor(max_iter=1000, tol=1e-3, learning_rate='adaptive',
	                               eta0=0.001, penalty='l2', alpha=0.0001, random_state=42))])

	# 6. ê¸°ë³¸ ë‹¤í•­ íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	polynomial_model.fit(X_train, y_train)
	y_pred_poly = polynomial_model.predict(X_test)
	r2_poly_basic = r2_score(y_test, y_pred_poly)
	mse_poly_basic = mean_squared_error(y_test, y_pred_poly)
	mae_poly_basic = mean_absolute_error(y_test, y_pred_poly)

	# 7. ì¡°ê¸° ì¢…ë£Œ ì ìš© ë‹¤í•­ íšŒê·€ ëª¨ë¸ êµ¬ì„±
	early_stopping_poly_model = Pipeline(steps=[
	    ('preprocessor', preprocessor),
	    ('poly_features', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)),
    	    ('scaler', StandardScaler()),
	    ('regressor', SGDRegressor(max_iter=2000, tol=1e-3, early_stopping=True,
	                               validation_fraction=0.2, n_iter_no_change=5,
	                               learning_rate='adaptive', eta0=0.001, penalty='l2', alpha=0.0001, random_state=42))])

	# 8. ì¡°ê¸° ì¢…ë£Œ ì ìš© ë‹¤í•­ íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
	early_stopping_poly_model.fit(X_train, y_train)
	y_pred_poly_early_stopping = early_stopping_poly_model.predict(X_test)
	r2_poly_early_stopping = r2_score(y_test, y_pred_poly_early_stopping)
	mse_poly_early_stopping = mean_squared_error(y_test, y_pred_poly_early_stopping)
	mae_poly_early_stopping = mean_absolute_error(y_test, y_pred_poly_early_stopping)

	# ê²°ê³¼ ì¶œë ¥
	print("ë‹¤í•­ íšŒê·€ ëª¨ë¸ ê²°ê³¼ (SGDRegressor ì‚¬ìš©):")
	print(f"ê¸°ë³¸ ë‹¤í•­ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_poly_basic:.4f}, MSE: {mse_poly_basic:.4f}, MAE: {mae_poly_basic:.4f}")
	print(f"ì¡°ê¸° ì¢…ë£Œ ì ìš© ë‹¤í•­ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_poly_early_stopping:.4f}, MSE: {mse_poly_early_stopping:.4f}, 	MAE: {mae_poly_early_stopping:.4f}")

<br>

	ë‹¤í•­ íšŒê·€ ëª¨ë¸ ê²°ê³¼ (SGDRegressor ì‚¬ìš©):
	ê¸°ë³¸ ë‹¤í•­ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.4101, MSE: 0.1430, MAE: 0.2685
	ì¡°ê¸° ì¢…ë£Œ ì ìš© ë‹¤í•­ íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.4286, MSE: 0.1386, MAE: 0.2640

<br>

## [2-3] ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)
â–£ ì •ì˜ : ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ê²°í•©(ë°°ê¹…: ê° ëª¨ë¸ì˜ ë…ë¦½ì ì¸ í•™ìŠµ, ë¶€ìŠ¤íŒ…: ê° ëª¨ë¸ì´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ, ìŠ¤íƒœí‚¹: ì„œë¡œë‹¤ë¥¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ê²°ê³¼ ê²°í•©)<br>
â–£ í•„ìš”ì„± : ë‹¨ì¼ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ë°ì´í„°ì˜ ë‹¤ì–‘í•œ íŒ¨í„´ì„ ë” ì˜ í•™ìŠµ<br>
â–£ ì£¼ìš” ê¸°ë²• : ìŠ¤íƒœí‚¹, ë°°ê¹…(Random Forest), ë¶€ìŠ¤íŒ…(AdaBoost, Gradient Boosting, XGBoost, LightGBM)<br>
â–£ ì¥ì  : ë†’ì€ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥, ë‹¤ì–‘í•œ ë°ì´í„° ë° ëª¨ë¸ ìœ í˜•ì— ì ìš© ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš© ì¦ê°€, êµ¬í˜„ ë³µì¡ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜(ë¶„ë¥˜, íšŒê·€ ë“±)<br>

	#############################################################
	# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™”
	# [2-3] ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning)
	#############################################################
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.ensemble import StackingRegressor, BaggingRegressor, GradientBoostingRegressor
	from sklearn.linear_model import LinearRegression
	from sklearn.tree import DecisionTreeRegressor
	from sklearn.svm import SVR
	from sklearn.metrics import r2_score

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. Stacking ì•™ìƒë¸”
	stacking_model = StackingRegressor(
	    estimators=[
	        ('lr', LinearRegression()),
	        ('dt', DecisionTreeRegressor()),
	        ('svr', SVR())],
	    final_estimator=LinearRegression())
	stacking_model.fit(X_train, y_train)
	y_pred_stacking = stacking_model.predict(X_test)
	r2_stacking = r2_score(y_test, y_pred_stacking)

	# 4. Bagging ì•™ìƒë¸”
	bagging_model = BaggingRegressor(
	    estimator=DecisionTreeRegressor(),  # ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„° ì´ë¦„
	    n_estimators=10,
	    random_state=42)
	bagging_model.fit(X_train, y_train)
	y_pred_bagging = bagging_model.predict(X_test)
	r2_bagging = r2_score(y_test, y_pred_bagging)

	# 5. Boosting ì•™ìƒë¸” (Gradient Boosting)
	boosting_model = GradientBoostingRegressor(random_state=42)
	boosting_model.fit(X_train, y_train)
	y_pred_boosting = boosting_model.predict(X_test)
	r2_boosting = r2_score(y_test, y_pred_boosting)

	# ê²°ê³¼ ì¶œë ¥
	print("RÂ² ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_linear:.4f}")
	print(f"Stacking ì•™ìƒë¸” ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_stacking:.4f}")
	print(f"Bagging ì•™ìƒë¸” ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_bagging:.4f}")
	print(f"Boosting ì•™ìƒë¸” ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_boosting:.4f}")
	
<br>

	RÂ² ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.9469
	Stacking ì•™ìƒë¸” ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.9623
	Bagging ì•™ìƒë¸” ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.9990
	Boosting ì•™ìƒë¸” ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.9938

<br>

## [2-4] ëª¨ë¸ í•´ì„ì„± (Model Interpretability)
â–£ ì •ì˜ : ëª¨ë¸ì´ ë‚´ë¦° ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•´ ì„¤ëª… ê°€ëŠ¥í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë¸”ë™ë°•ìŠ¤ ëª¨ë¸(ë”¥ëŸ¬ë‹, ì•™ìƒë¸”)ì˜ íˆ¬ëª…ì„± í™•ë³´, ë¹„ì¦ˆë‹ˆìŠ¤ë‚˜ ì˜ë£Œ ë“± ê³ ìœ„í—˜ ë¶„ì•¼ì—ì„œ ì‹ ë¢° í™•ë³´<br>
â–£ ì£¼ìš” ê¸°ë²• : LIME(íŠ¹ì • ì˜ˆì¸¡ ë¡œì»¬ ë‹¨ìœ„ì—ì„œ ë‹¨ìˆœ ëª¨ë¸ë¡œ ê·¼ì‚¬), SHAP(íŠ¹ì§•ë³„ ê¸°ì—¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ˆì¸¡ì— ëŒ€í•œ ê¸€ë¡œë²Œ ë° ë¡œì»¬ í•´ì„ ì œê³µ)<br>
â–£ ì¥ì  : ì‚¬ìš©ì ì‹ ë¢° í™•ë³´, ëª¨ë¸ ë””ë²„ê¹… ë° ê°œì„  ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ, ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ì—ì„œ ë³µì¡ì„± ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ (e.g., ì‹ ê²½ë§, ì•™ìƒë¸” í•™ìŠµ ëª¨ë¸)<br>

	#############################################################
	# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™”
	# [2-4] ëª¨ë¸ í•´ì„ì„±(Model Interpretability) : LIME
	#############################################################
	import subprocess
	import sys
	# í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ì„¤ì¹˜
	required_packages = ['lime', 'xgboost', 'scikit-learn', 'matplotlib']
	for package in required_packages:
	    try:
	        __import__(package)
	    except ImportError:
	        print(f"Package {package} not found. Installing...")
	        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

	# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
	import lime
	import lime.lime_tabular
	import xgboost as xgb
	from sklearn.datasets import load_breast_cancer
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler
	import matplotlib.pyplot as plt

	# ë°ì´í„° ë¡œë“œ
	data = load_breast_cancer()
	X = data.data
	y = data.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° í‘œì¤€í™”
	scaler = StandardScaler()
	X_train_scaled = scaler.fit_transform(X_train)
	X_test_scaled = scaler.transform(X_test)

	# XGBoost ëª¨ë¸ í•™ìŠµ
	model = xgb.XGBClassifier(eval_metric='logloss')
	model.fit(X_train_scaled, y_train)

	# LIME explainer ìƒì„± (training_sample_weight ì¸ì ì œê±°)
	explainer = lime.lime_tabular.LimeTabularExplainer(
	    training_data=X_train_scaled,
	    training_labels=y_train,
	    mode="classification",
	    feature_names=data.feature_names,
	    class_names=data.target_names,
	    discretize_continuous=True)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ ì˜ˆì¸¡ ì„¤ëª…
	i = 0  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
	explanation = explainer.explain_instance(X_test_scaled[i], model.predict_proba)

	# ê²°ê³¼ ì‹œê°í™”
	explanation.show_in_notebook(show_table=True, show_all=False)

	# íŠ¹ì • ìƒ˜í”Œì— ëŒ€í•œ LIME í•´ì„ ê·¸ë˜í”„ ì¶œë ¥
	explanation.as_pyplot_figure()
	plt.show()

<br>

![](./images/2-4_1.png) 
<br>

	#############################################################
	# [2] ëª¨ë¸ ë³µì¡ë„ ë° ì¼ë°˜í™”
	# [2-4] ëª¨ë¸ í•´ì„ì„±(Model Interpretability) : SHAP
	#############################################################
	import subprocess
	import sys
	# í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ì„¤ì¹˜
	required_packages = ['shap', 'xgboost', 'scikit-learn', 'matplotlib']
	for package in required_packages:
	    try:
 	       __import__(package)
 	   except ImportError:
	        print(f"Package {package} not found. Installing...")
	        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

	# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
	import shap
	import xgboost as xgb
	from sklearn.datasets import load_breast_cancer
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler
	import matplotlib.pyplot as plt

	# ë°ì´í„° ë¡œë“œ
	data = load_breast_cancer()
	X = data.data
	y = data.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° í‘œì¤€í™”
	scaler = StandardScaler()
	X_train_scaled = scaler.fit_transform(X_train)
	X_test_scaled = scaler.transform(X_test)

	# XGBoost ëª¨ë¸ í•™ìŠµ
	model = xgb.XGBClassifier(eval_metric='logloss')
	model.fit(X_train_scaled, y_train)

	# SHAP Explainer ìƒì„±
	explainer = shap.Explainer(model, X_train_scaled)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ SHAP ê°’ ê³„ì‚°
	shap_values = explainer(X_test_scaled)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ SHAP ê°’ ì‹œê°í™”
	i = 0  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
	print(f"SHAP values for test sample {i}:")
	shap.initjs()

	# ë‹¨ì¼ í´ë˜ìŠ¤ ë¶„ë¥˜ì˜ SHAP ê°’ ì‹œê°í™”
	# ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ê°€ ì•„ë‹Œ ê²½ìš°, base_valueì™€ shap_valuesëŠ” 1ì°¨ì›
	shap.force_plot(
	    base_value=shap_values[i].base_values,  # ê¸°ì¤€ê°’ (ìŠ¤ì¹¼ë¼ ê°’)
	    shap_values=shap_values[i].values,     # SHAP ê°’ (íŠ¹ì„±ë³„ ê°’)
	    features=X_test_scaled[i],             # í•´ë‹¹ ìƒ˜í”Œì˜ ì…ë ¥ê°’
	    feature_names=data.feature_names)

	# ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¤‘ìš”ë„ ìš”ì•½ ê·¸ë˜í”„
	shap.summary_plot(shap_values.values, X_test_scaled, feature_names=data.feature_names)

	# íŠ¹ì • ìƒ˜í”Œì— ëŒ€í•œ Bar Chart (SHAP ê°’ì˜ í¬ê¸°)
	shap.plots.bar(shap_values[i])

<br>

![](./images/2-4_2.png) 
<br>

![](./images/2-4_3.png) 
<br>

---

# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
## [3-1] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Hyperparameter Tuning)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©ìê°€ ì‚¬ì „ì— ì„¤ì •í•˜ëŠ” ë³€ìˆ˜ë¡œ, í•™ìŠµë¥ , ì •ê·œí™” ê°•ë„, ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ ìµœëŒ€ ê¹Šì´ ë“± ë“±ì„ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ê³¼ì í•©, ê³¼ì†Œì í•© ë˜ëŠ” í•™ìŠµ ì†ë„ ì €í•˜ê°€ ë°œìƒ<br>
â–£ ì¥ì  : ëª¨ë¸ ì„±ëŠ¥ ê·¹ëŒ€í™” ê°€ëŠ¥, ë‹¤ì–‘í•œ ë°ì´í„°ì™€ ë¬¸ì œ ìœ í˜•ì— ì ìš© ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ê³ , ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë©°, íƒìƒ‰ ê³µê°„ì´ ì»¤ì§ˆìˆ˜ë¡ ë³µì¡ë„ ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

	#############################################################
	# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
	# [3-1] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)
	#############################################################
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	from sklearn.linear_model import LinearRegression
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. Gradient Boosting Regressor í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ë° í•™ìŠµ
	optimized_gbr = GradientBoostingRegressor(
	    n_estimators=150,  # ìµœì ì˜ ë¶€ìŠ¤íŒ… ë‹¨ê³„ ìˆ˜
	    learning_rate=0.1,  # ì ì ˆí•œ í•™ìŠµë¥ 
	    max_depth=4,  # ìµœì ì˜ íŠ¸ë¦¬ ê¹Šì´
	    random_state=42)
	optimized_gbr.fit(X_train, y_train)

	# 4. ìµœì í™”ëœ Gradient Boosting Regressor ëª¨ë¸ í‰ê°€
	y_pred_optimized = optimized_gbr.predict(X_test)
	r2_optimized = r2_score(y_test, y_pred_optimized)

	# ê²°ê³¼ ì¶œë ¥
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: {r2_linear:.4f}")
	print(f"ìµœì í™”ëœ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: {r2_optimized:.4f}")

<br>

	R2 ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: 0.9469
	ìµœì í™”ëœ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: 0.9987

<br>

## [3-2] ê·¸ë¦¬ë“œ ì„œì¹˜(Grid Search)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ëª¨ë“  ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ë°©ë²•<br>
â–£ í•„ìš”ì„± : ì²´ê³„ì ìœ¼ë¡œ ëª¨ë“  ì¡°í•©ì„ íƒìƒ‰í•˜ë¯€ë¡œ ìµœì ì˜ ì„¤ì •ì„ ì°¾ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ<br>
â–£ ì¥ì  : ê°„ë‹¨í•˜ê³  ì§ê´€ì ì´ë©° êµ¬í˜„ì´ ìš©ì´í•˜ë©°, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ì „ ë²”ìœ„ë¥¼ íƒìƒ‰ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : íƒìƒ‰ ê³µê°„ì´ ì»¤ì§ˆìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ê³¼ ì‹œê°„ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ë©°, ë¶ˆí•„ìš”í•œ ì¡°í•©ê¹Œì§€ ê³„ì‚°í•  ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì´ ë¹„êµì  ì‘ì€ ë¬¸ì œì— ì í•©, Scikit-learnì—ì„œ GridSearchCV í•¨ìˆ˜ ì œê³µ<br>

	#############################################################
	# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
	# [3-2] ê·¸ë¦¬ë“œ ì„œì¹˜ (Grid Search)
	#############################################################
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split, GridSearchCV
	from sklearn.linear_model import LinearRegression
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. Hyperparameter Tuning (GridSearchCV with GradientBoostingRegressor)
	param_grid = {
	    'n_estimators': [50, 100, 150],
	    'learning_rate': [0.01, 0.1, 0.2],
	    'max_depth': [3, 4, 5]}

	gbr = GradientBoostingRegressor(random_state=42)
	grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, scoring='r2', cv=5, n_jobs=-1)
	grid_search.fit(X_train, y_train)

	# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ í‰ê°€
	best_model = grid_search.best_estimator_
	y_pred_tuned = best_model.predict(X_test)
	r2_tuned = r2_score(y_test, y_pred_tuned)

	# ê²°ê³¼ ì¶œë ¥
	print("RÂ² ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: {r2_linear:.4f}")
	print(f"Hyperparameter Tuning í›„ Gradient Boosting Regressorì˜ RÂ² ì ìˆ˜: {r2_tuned:.4f}")
	print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")

<br>

	RÂ² ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ RÂ² ì ìˆ˜: 0.9469
	Hyperparameter Tuning í›„ Gradient Boosting Regressorì˜ RÂ² ì ìˆ˜: 1.0000
	ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}

<br>

## [3-3] ëœë¤ ì„œì¹˜(Random Search)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ë“¤ì„ ì„ì˜ë¡œ ì„ íƒí•˜ì—¬ ìµœì ì˜ ì¡°í•©ì„ íƒìƒ‰<br>
â–£ í•„ìš”ì„± : ê·¸ë¦¬ë“œ ì„œì¹˜ë³´ë‹¤ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë†’ì´ë©°, ë” í° íƒìƒ‰ ê³µê°„ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ íƒìƒ‰<br>
â–£ ì¥ì  : ê³„ì‚° íš¨ìœ¨ì„± ì¦ëŒ€, ì ì€ ê³„ì‚°ìœ¼ë¡œë„ ë†’ì€ ì„±ëŠ¥ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ê°€ëŠ¥ì„±<br>
â–£ ë‹¨ì  : ìµœì ì˜ ì¡°í•©ì„ ë°˜ë“œì‹œ ì°¾ì§€ ëª»í•  ê°€ëŠ¥ì„±, íƒìƒ‰ ê²°ê³¼ê°€ ì‹¤í–‰ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì— ì í•©, Scikit-learnì—ì„œ RandomizedSearchCV í•¨ìˆ˜ ì œê³µ<br>

	#############################################################
	# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
	# [3-3] ëœë¤ ì„œì¹˜ (Random Search)
	#############################################################
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split, RandomizedSearchCV
	from sklearn.linear_model import LinearRegression
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score
	from scipy.stats import randint, uniform

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. Random Search (RandomizedSearchCV with GradientBoostingRegressor)
	param_distributions = {
	    'n_estimators': randint(50, 200),  # ë¶€ìŠ¤íŒ… ë‹¨ê³„ ìˆ˜
	    'learning_rate': uniform(0.01, 0.2),  # í•™ìŠµë¥  ë²”ìœ„
	    'max_depth': randint(3, 7)  # íŠ¸ë¦¬ ê¹Šì´ ë²”ìœ„
	}

	gbr = GradientBoostingRegressor(random_state=42)
	random_search = RandomizedSearchCV(estimator=gbr, param_distributions=param_distributions, 
	                                   n_iter=50, scoring='r2', cv=5, random_state=42, n_jobs=-1)
	random_search.fit(X_train, y_train)

	# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ í‰ê°€
	best_model = random_search.best_estimator_
	y_pred_tuned = best_model.predict(X_test)
	r2_tuned = r2_score(y_test, y_pred_tuned)

	# ê²°ê³¼ ì¶œë ¥
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: {r2_linear:.4f}")
	print(f"Random Search í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: {r2_tuned:.4f}")
	print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {random_search.best_params_}")

<br>

	R2 ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: 0.9469
	Random Search í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: 1.0000
	ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'learning_rate': 0.1554543991712842, 'max_depth': 5, 'n_estimators': 89}

<br>

## [3-4] ë² ì´ì¦ˆ ìµœì í™”(Bayesian Optimization)
â–£ ì •ì˜ : ì´ì „ íƒìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìµœì ê°’ì„ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ë¥  ëª¨ë¸(ì˜ˆ: ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤)ì„ í™œìš©í•˜ì—¬ íƒìƒ‰<br>
â–£ í•„ìš”ì„± : ê³„ì‚° ë¹„ìš©ì´ ë†’ì€ ë¬¸ì œì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : íš¨ìœ¨ì ì¸ íƒìƒ‰ìœ¼ë¡œ ê³„ì‚° ìì›ì„ ì ˆì•½, ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´<br>
â–£ ë‹¨ì  : êµ¬í˜„ ë° ì´í•´ê°€ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë©°, íƒìƒ‰ ì´ˆê¸°ì—ëŠ” ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ê³„ì‚° ë¹„ìš©ì´ ë†’ì€ ë¨¸ì‹ ëŸ¬ë‹(ëœë¤ í¬ë ˆìŠ¤íŠ¸) ë° ë”¥ëŸ¬ë‹ ëª¨ë¸<br>

	#############################################################
	# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
	# [3-4] ë² ì´ì¦ˆ ìµœì í™” (Bayesian Optimization)
	#############################################################
	import subprocess
	import sys	
	# í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ëŠ” í•¨ìˆ˜
	def install_package(package):
	    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

	# í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸ ë° ì„¤ì¹˜
	try:
	    from skopt import BayesSearchCV
	except ImportError:
	    install_package("scikit-optimize")
	    from skopt import BayesSearchCV

	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score
	from skopt.space import Real, Integer

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. Bayesian Optimization (BayesSearchCV with GradientBoostingRegressor)
	param_space = {
	    'n_estimators': Integer(50, 200),
	    'learning_rate': Real(0.01, 0.2, prior='log-uniform'),
	    'max_depth': Integer(3, 7)}

	gbr = GradientBoostingRegressor(random_state=42)
	bayes_search = BayesSearchCV(
	    estimator=gbr,
	    search_spaces=param_space,
	    n_iter=50,
	    scoring='r2',
	    cv=5,
	    random_state=42,
	    n_jobs=-1)
	bayes_search.fit(X_train, y_train)

	# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ í‰ê°€
	best_model = bayes_search.best_estimator_
	y_pred_tuned = best_model.predict(X_test)
	r2_tuned = r2_score(y_test, y_pred_tuned)

	# ê²°ê³¼ ì¶œë ¥
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: {r2_linear:.4f}")
	print(f"Bayesian Optimization í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: {r2_tuned:.4f}")
	print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {bayes_search.best_params_}")

<br>

	R2 ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: 0.9469
	Bayesian Optimization í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: 1.0000
	ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: OrderedDict({'learning_rate': 0.10970919052074331, 'max_depth': 5, 'n_estimators': 129})

<br>

## [3-5] í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìë™í™”(Automated Hyperparameter Tuning)
â–£ ì •ì˜ : í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì •ì„ ìë™í™”í•˜ì—¬ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ìˆ˜ë™ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ë° ë“œëŠ” ì‹œê°„ê³¼ ë…¸ë ¥ì„ ì¤„ì´ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : íš¨ìœ¨ì ì´ê³  í¸ë¦¬í•˜ë©° ë°˜ë³µ ê°€ëŠ¥, ì´ˆë³´ìë„ ê³ ì„±ëŠ¥ ëª¨ë¸ êµ¬í˜„ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ë„êµ¬ ë° ì•Œê³ ë¦¬ì¦˜ì˜ ì œí•œ ì‚¬í•­ì— ë”°ë¼ ìµœì  ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìœ¼ë©°, ë„êµ¬ ì‚¬ìš©ì— ë”°ë¥¸ ë¹„ìš© ë°œìƒ ê°€ëŠ¥<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ AutoMLì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸<br>


	#############################################################
	# [3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
	# [3-5] í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìë™í™” (Automated Hyperparameter Tuning)
	#############################################################
	import subprocess
	import sys
	# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•¨ìˆ˜
	def install_package(package):
	    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

	# í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸ ë° ì„¤ì¹˜
	try:
	    import optuna
	except ImportError:
	    install_package("optuna")

	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split, cross_val_score
	from sklearn.linear_model import LinearRegression
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score
	import optuna

	# Optuna ë¡œê·¸ ìµœì†Œí™”
	optuna.logging.set_verbosity(optuna.logging.WARNING)

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
	def objective(trial):
	    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
	    n_estimators = trial.suggest_int("n_estimators", 50, 200)
	    learning_rate = trial.suggest_float("learning_rate", 0.01, 0.2, log=True)
	    max_depth = trial.suggest_int("max_depth", 3, 7)
    
	    model = GradientBoostingRegressor(
	        n_estimators=n_estimators,
	        learning_rate=learning_rate,
	        max_depth=max_depth,
	        random_state=42)
	    # 5-Fold êµì°¨ ê²€ì¦
    	    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
   	    return cv_scores.mean()

	# Optuna ìŠ¤í„°ë”” ìƒì„± ë° ì‹¤í–‰
	study = optuna.create_study(direction="maximize")
	study.optimize(objective, n_trials=50)

	# ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ í•™ìŠµ
	best_params = study.best_params
	optimized_model = GradientBoostingRegressor(
	    n_estimators=best_params["n_estimators"],
	    learning_rate=best_params["learning_rate"],
	    max_depth=best_params["max_depth"],
	    random_state=42)
	optimized_model.fit(X_train, y_train)
	y_pred_tuned = optimized_model.predict(X_test)
	r2_tuned = r2_score(y_test, y_pred_tuned)

	# ê²°ê³¼ ì¶œë ¥
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: {r2_linear:.4f}")
	print(f"Automated Hyperparameter Tuning í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: {r2_tuned:.4f}")
	print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")

<br>

	R2 ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: 0.9469
	Automated Hyperparameter Tuning í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: 0.9986
	ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'n_estimators': 71, 'learning_rate': 0.062404813201012564, 'max_depth': 4}

<br>

## [3-6] AutoML(Automated Machine Learning) í™œìš©
â–£ ì •ì˜ : AutoML (Automated Machine Learning)ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œì˜ ì „ ê³¼ì •ì„ ìë™í™”í•˜ëŠ” ê¸°ìˆ  ë˜ëŠ” ë„êµ¬<br>
â–£ í•„ìš”ì„± : ì „ë¬¸ê°€ ë¶€ì¡± ë¬¸ì œ í•´ê²°, ì‹œê°„ê³¼ ìì› ì ˆì•½, ìµœì ì˜ ëª¨ë¸ íƒìƒ‰, ë°ì´í„° ì¦ê°€ì™€ ë³µì¡ì„± ëŒ€ì‘, ì´ˆê¸° í”„ë¡œí† íƒ€ì´í•‘<br>
â–£ ì£¼ìš”ê¸°ë²•  : ìë™ ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ ì„ íƒ (Algorithm Selection), í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, íŒŒì´í”„ë¼ì¸ ìë™í™” (Pipeline Optimization), ë©”íƒ€ëŸ¬ë‹ (Meta-Learning), ì‹ ê²½ ì•„í‚¤í…ì²˜ ê²€ìƒ‰ (Neural Architecture Search, NAS), ì•™ìƒë¸” ê¸°ë²•<br>
â–£ ì¥ì  : ì‚¬ìš© ìš©ì´ì„±, ì‹œê°„ ì ˆì•½, íš¨ìœ¨ì„±, ë²”ìš©ì„±, ì„±ëŠ¥ ìµœì í™”<br>
â–£ ë‹¨ì  : ì„¤ëª… ê°€ëŠ¥ì„± ë¶€ì¡±, ì œí•œëœ ì»¤ìŠ¤í„°ë§ˆì´ì§•, ê³„ì‚° ë¹„ìš©, ê¸°ìˆ ì  ì œì•½, ë°ì´í„° ì „ì²˜ë¦¬ í•œê³„<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, ì•™ìƒë¸”, ë”¥ëŸ¬ë‹, ì‹ ê²½ ì•„í‚¤í…ì²˜ ê²€ìƒ‰ (NAS), ì‹œê³„ì—´ ì˜ˆì¸¡, ê°•í™”í•™ìŠµ<br>

<br>

---

# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
## [4-1] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Learning Rate Scheduling)
â–£ ì •ì˜ : í•™ìŠµë¥ (Learning Rate)ì„ í•™ìŠµ ê³¼ì • ì¤‘ì— ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ìµœì í™” ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•(Step Decay: ì¼ì • ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥  ê°ì†Œ, Exponential Decay: í•™ìŠµë¥ ì„ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ, Cosine Annealing: í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ë‚®ì¶”ëŠ” ë°©ì‹)<br>
â–£ í•„ìš”ì„± : ê³ ì •ëœ í•™ìŠµë¥ ì€ ì´ˆê¸°ì™€ í›„ë°˜ë¶€ í•™ìŠµì— ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ˆê¸°ì—ëŠ” ë¹ ë¥¸ í•™ìŠµ, í›„ë°˜ë¶€ì—ëŠ” ì•ˆì •ì ì¸ ìˆ˜ë ´ì´ í•„ìš”<br>
â–£ ì¥ì  : í•™ìŠµ ì†ë„ì™€ ìµœì í™” ì•ˆì •ì„±ì„ ëª¨ë‘ í™•ë³´ ê°€ëŠ¥í•˜ë©°, ê³¼ì í•© ë°©ì§€ì— ë„ì›€<br>
â–£ ë‹¨ì  : ì ì ˆí•œ ìŠ¤ì¼€ì¤„ì„ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±, ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì£¼ë¡œ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ SGD, Adamê³¼ ê°™ì€ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-1] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (Learning Rate Scheduling)
	#############################################################
	import numpy as np
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ì„ ì‚¬ìš©í•œ GradientBoostingRegressor
	class GradientBoostingRegressorWithLRScheduling(GradientBoostingRegressor):
	    def __init__(self, initial_learning_rate=0.1, decay_factor=0.95, decay_step=10, **kwargs):
	        super().__init__(**kwargs)
	        self.initial_learning_rate = initial_learning_rate
	        self.decay_factor = decay_factor
	        self.decay_step = decay_step

	    def _update_learning_rate(self, n_iter):
 	       return self.initial_learning_rate * (self.decay_factor ** (n_iter // self.decay_step))

	    def fit(self, X, y, sample_weight=None):
  	      n_iter = 0
	        for iteration in range(self.n_estimators):
            		current_learning_rate = self._update_learning_rate(n_iter)
           		self.learning_rate = current_learning_rate
            		super().fit(X, y, sample_weight=sample_weight)
            		n_iter += 1
        	return self

	# 4. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì„¤ì • ë° ëª¨ë¸ í•™ìŠµ
	learning_rate_scheduled_model = GradientBoostingRegressorWithLRScheduling(
	    initial_learning_rate=0.2,  # ì´ˆê¸° í•™ìŠµë¥  ë†’ê²Œ ì„¤ì •
	    decay_factor=0.95,  # ê°ì†Œ ë¹„ìœ¨ ìœ ì§€
	    decay_step=10,  # ê°ì†Œ ì£¼ê¸°ë¥¼ ê¸¸ê²Œ ì„¤ì •
	    n_estimators=100,
	    random_state=42)
	learning_rate_scheduled_model.fit(X_train, y_train)

	# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ëª¨ë¸ í‰ê°€
	y_pred_lr_scheduled = learning_rate_scheduled_model.predict(X_test)
	r2_lr_scheduled = r2_score(y_test, y_pred_lr_scheduled)

	# ê²°ê³¼ ì¶œë ¥
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: {r2_linear:.4f}")
	print(f"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì‚¬ìš© í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: {r2_lr_scheduled:.4f}")

<br>

	R2 ì ìˆ˜ ê²°ê³¼:
	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: 0.9469
	í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì‚¬ìš© í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: 0.9945

<br>

## [4-2] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Initialization)
â–£ ì •ì˜ : ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ê°’ì„ í•™ìŠµ ì´ˆê¸°ì— ì ì ˆíˆ ì„¤ì •í•˜ì—¬ í•™ìŠµ ê³¼ì •ì„ ì•ˆì •í™”í•˜ëŠ” ê¸°ë²•(Xavier Initialization: ì…ë ¥ ë° ì¶œë ¥ ë…¸ë“œ ìˆ˜ì— ê¸°ë°˜, He Initialization: ReLU ê³„ì—´ í™œì„±í™” í•¨ìˆ˜ì— ì í•©)<br>
â–£ í•„ìš”ì„± : ì˜ëª»ëœ ì´ˆê¸°í™”ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì´ë‚˜ í­ë°œ(Exploding Gradient)ì„ ìœ ë°œí•  ìˆ˜ ìˆìŒ.<br>
â–£ ì¥ì  : í•™ìŠµ ì´ˆê¸° ì•ˆì •ì„± í–¥ìƒ, ë¹ ë¥¸ ìˆ˜ë ´ ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ì¼ë¶€ ì•Œê³ ë¦¬ì¦˜ì—ì„œ íŠ¹ì • ì´ˆê¸°í™” ì „ëµì´ ë” ì í•©í•˜ë¯€ë¡œ ì ì ˆí•œ ì„ íƒ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ëª¨ë¸ (íŠ¹íˆ ì‹¬ì¸µ ì‹ ê²½ë§)<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-2] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Weight Initialization)
	#############################################################
	import matplotlib.pyplot as plt
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.dummy import DummyRegressor
	from sklearn.metrics import r2_score

	# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
	iris = load_iris()
	X = iris.data
	y = iris.target

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# 2. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	linear_model = LinearRegression()
	linear_model.fit(X_train, y_train)
	y_pred_linear = linear_model.predict(X_test)
	r2_linear = r2_score(y_test, y_pred_linear)

	# 3. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì ìš© (Gradient Boosting Regressor)
	# ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ë”ë¯¸ ëª¨ë¸
	dummy_init = DummyRegressor(strategy="mean")
	dummy_init.fit(X_train, y_train)

	# Gradient Boosting Regressorì— ì´ˆê¸° ëª¨ë¸ ì„¤ì •
	initialized_model = GradientBoostingRegressor(
	    init=dummy_init,
	    n_estimators=100,
	    learning_rate=0.1,
	    max_depth=3,
	    random_state=42)
	initialized_model.fit(X_train, y_train)

	# ì´ˆê¸°í™” ì ìš© ëª¨ë¸ í‰ê°€
	y_pred_initialized = initialized_model.predict(X_test)
	r2_initialized = r2_score(y_test, y_pred_initialized)

	# 4. ì‹œê°í™”
	plt.figure(figsize=(12, 6))

	# ê¸°ë³¸ ì„ í˜• íšŒê·€ ê²°ê³¼ ì‹œê°í™”
	plt.subplot(1, 2, 1)
	plt.scatter(range(len(y_test)), y_test, label="True Values", color="blue")
	plt.scatter(range(len(y_pred_linear)), y_pred_linear, label="Linear Regression Predictions", color="red")
	plt.title(f"Linear Regression (R2: {r2_linear:.4f})")
	plt.xlabel("Sample Index")
	plt.ylabel("Target Value")
	plt.legend()

	# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì ìš© ëª¨ë¸ ê²°ê³¼ ì‹œê°í™”
	plt.subplot(1, 2, 2)
	plt.scatter(range(len(y_test)), y_test, label="True Values", color="blue")
	plt.scatter(range(len(y_pred_initialized)), y_pred_initialized, label="Initialized Gradient Boosting Predictions", color="green")
	plt.title(f"Weight Initialization (R2: {r2_initialized:.4f})")
	plt.xlabel("Sample Index")
	plt.ylabel("Target Value")
	plt.legend()
	plt.tight_layout()
	plt.show()

	# ê²°ê³¼ ì¶œë ¥
	print("R2 ì ìˆ˜ ê²°ê³¼:")
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ R2 ì ìˆ˜: {r2_linear:.4f}")
	print(f"ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì ìš© í›„ Gradient Boosting Regressorì˜ R2 ì ìˆ˜: {r2_initialized:.4f}")

<br>

![](./images/4-2.png) 
<br>

## [4-3] í™œì„±í™” í•¨ìˆ˜ ì„ íƒ(Activation Function Selection)
â–£ ì •ì˜ : ë‰´ëŸ°ì˜ ì¶œë ¥ ê°’ì„ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ íŒ¨í„´ì„ ëŠ˜ë¦¬ëŠ” ì—­í• ì„ í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •<br>
(Sigmoid: [0, 1] ì¶œë ¥, ì´ì§„ ë¶„ë¥˜ì—ì„œ ì‚¬ìš©, ReLU: ë¹„ì„ í˜•ì„± ì œê³µ, ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì™„í™”, Leaky ReLU: ReLUì˜ ë³€í˜•, ìŒìˆ˜ êµ¬ê°„ ê¸°ìš¸ê¸° ë³´ì •, Softmax: ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ ì¶œë ¥)<br>
â–£ í•„ìš”ì„± : ì ì ˆí•œ í™œì„±í™” í•¨ìˆ˜ ì„ íƒì€ í•™ìŠµ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹¨<br>
â–£ ì¥ì  : ë¹„ì„ í˜•ì„±ì„ ë„ì…í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²° ê°€ëŠ¥í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ê³¼ ë¬¸ì œì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥<br>
â–£ ë‹¨ì  : ì˜ëª»ëœ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ì‹œ í•™ìŠµ ì†ë„ ì €í•˜ë‚˜ ì„±ëŠ¥ ì•…í™”, íŠ¹ì • í•¨ìˆ˜ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥(Sigmoid, Tanh)<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-3] í™œì„±í™” í•¨ìˆ˜ ì„ íƒ (Activation Function Selection)
	#############################################################
	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import r2_score
	from sklearn.datasets import load_iris

	# ReLU í™œì„±í™” í•¨ìˆ˜ ì •ì˜
	def relu(x):
	    return np.maximum(0, x)

	# Iris ë°ì´í„°ì…‹ ë¡œë“œ
	iris = load_iris()
	X = iris.data  # ì…ë ¥ ë°ì´í„° (íŠ¹ì„±)
	y = iris.target  # ì¶œë ¥ ë°ì´í„° (íƒ€ê²Ÿ)

	# ë°ì´í„° ê°„ì†Œí™”ë¥¼ ìœ„í•´ ë‘ ê°œì˜ í´ë˜ìŠ¤ë§Œ ì‚¬ìš© (í´ë˜ìŠ¤ 0ê³¼ 1)
	binary_indices = y != 2  # í´ë˜ìŠ¤ 2 ì œì™¸
	X = X[binary_indices]
	y = y[binary_indices]

	# ë°ì´í„°ì…‹ì„ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ì„ í˜• íšŒê·€ ëª¨ë¸ ìƒì„±
	model = LinearRegression()

	# í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
	model.fit(X_train, y_train)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ ë° RÂ² ì ìˆ˜ ê³„ì‚°
	y_pred = model.predict(X_test)
	r2_original = r2_score(y_test, y_pred)

	# ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ íŠ¹ì„± ë°ì´í„°ì— ì ìš©
	X_train_relu = relu(X_train)
	X_test_relu = relu(X_test)

	# ReLU ë³€í™˜ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
	model_relu = LinearRegression()
	model_relu.fit(X_train_relu, y_train)

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ ë° RÂ² ì ìˆ˜ ê³„ì‚° (ReLU ì ìš© í›„)
	y_pred_relu = model_relu.predict(X_test_relu)
	r2_relu = r2_score(y_test, y_pred_relu)

	# ì‹œê°í™”
	plt.figure(figsize=(12, 6))

	# ReLU ë¯¸ì ìš© ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
	plt.subplot(1, 2, 1)
	plt.scatter(y_test, y_pred, color="blue", label="ì›ë˜ ì˜ˆì¸¡ ê°’")
	plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="ì´ìƒì  ì í•©ì„ ")
	plt.title("ReLU ë¯¸ì ìš©: R2 = {:.2f}".format(r2_original))
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()

	# ReLU ì ìš© ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
	plt.subplot(1, 2, 2)
	plt.scatter(y_test, y_pred_relu, color="green", label="ReLU ë³€í™˜ ì˜ˆì¸¡ ê°’")
	plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="ì´ìƒì  ì í•©ì„ ")
	plt.title("ReLU ì ìš©: R2 = {:.2f}".format(r2_relu))
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()	
	plt.tight_layout()
	plt.show()

	# í•œê¸€ ê²°ê³¼ ë¶„ì„ ì¶œë ¥
	analysis = f"""
	### ê²°ê³¼ ë¶„ì„
	
	1. **ReLU ë¯¸ì ìš© (RÂ² ì ìˆ˜)**: {r2_original:.2f}
	   - ì›ë˜ ì„ í˜•íšŒê·€ ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì˜ ì›ë³¸ ê°’ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë†’ì€ RÂ² ì ìˆ˜ë¥¼ ë³´ì…ë‹ˆë‹¤.
	   - ì˜ˆì¸¡ ê°’ì´ ì‹¤ì œ ê°’ê³¼ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ë©°, ì‚°ì ë„ê°€ ì´ìƒì ì¸ ì í•©ì„ (ë¹¨ê°„ ì ì„ ) ê·¼ì²˜ì— ë¶„í¬í•©ë‹ˆë‹¤.
	2. **ReLU ì ìš© (RÂ² ì ìˆ˜)**: {r2_relu:.2f}
	   - ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì…ë ¥ ë°ì´í„°ì— ì ìš©í•˜ë©´ ë°ì´í„°ì˜ ìŒìˆ˜ ê°’ì´ 0ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
	   - ì´ëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ë³€ê²½í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥(RÂ² ì ìˆ˜)ì„ ì•½ê°„ ê°œì„ í•˜ê±°ë‚˜ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì‹¤í—˜ì—ì„œëŠ” RÂ² ì ìˆ˜ê°€ ì•½ê°„ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - ReLU í™œì„±í™” í•¨ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì´ ë¹„ì„ í˜• ê´€ê³„ë¥¼ í•™ìŠµí•  ê°€ëŠ¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
	   - ê·¸ëŸ¬ë‚˜ ì„ í˜• íšŒê·€ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ì„ í˜• ëª¨ë¸ì´ë¯€ë¡œ, ReLU ì ìš©ì´ ë°˜ë“œì‹œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¥í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
	   - ReLUëŠ” íŠ¹íˆ ë¹„ì„ í˜• ê´€ê³„ê°€ ê°•í•œ ë°ì´í„°ë‚˜ ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì—ì„œ ë” ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
	"""
	print(analysis)

<br>

![](./images/4-3.png) 
<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-3] í™œì„±í™” í•¨ìˆ˜ ì„ íƒ (Activation Function Selection)
	# "relu", "sigmoid", "tanh
	#############################################################
	import numpy as np
	import tensorflow as tf
	from tensorflow.keras.models import Sequential
	from tensorflow.keras.layers import Dense
	from tensorflow.keras.optimizers import Adam
	from sklearn.datasets import make_classification
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler
	from sklearn.metrics import accuracy_score

	# ë°ì´í„° ìƒì„± (ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ)
	X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
	scaler = StandardScaler()
	X_train = scaler.fit_transform(X_train)
	X_test = scaler.transform(X_test)

	# ëª¨ë¸ ìƒì„± í•¨ìˆ˜
	def create_model(activation_function):
	    model = Sequential([
 	       Dense(64, activation=activation_function, input_shape=(X_train.shape[1],)),
 	       Dense(32, activation=activation_function),
 	       Dense(1, activation="sigmoid")  # ì¶œë ¥ì¸µì€ sigmoidë¡œ ê³ ì • (ì´ì§„ ë¶„ë¥˜)
	    ])
	    model.compile(optimizer=Adam(learning_rate=0.01), loss="binary_crossentropy", metrics=["accuracy"])
	    return model

	# í™œì„±í™” í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
	activations = ["relu", "sigmoid", "tanh"]

	# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
	results = {}

	for activation in activations:
	    print(f"í›ˆë ¨ ì‹œì‘ - í™œì„±í™” í•¨ìˆ˜: {activation}")
	    model = create_model(activation)
	    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2)
    
	    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í‰ê°€
	    y_pred = (model.predict(X_test) > 0.5).astype(int)
	    accuracy = accuracy_score(y_test, y_pred)
	    results[activation] = accuracy
	    print(f"í™œì„±í™” í•¨ìˆ˜: {activation}, í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}")

	# ê²°ê³¼ ë¶„ì„
	print("\n### í™œì„±í™” í•¨ìˆ˜ë³„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ###")
	for activation, accuracy in results.items():
	    print(f"{activation}: {accuracy:.2f}")

	# ì‹œê°í™”
	import matplotlib.pyplot as plt

	plt.bar(results.keys(), results.values(), color=["blue", "green", "orange"])
	plt.title("í™œì„±í™” í•¨ìˆ˜ë³„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ")
	plt.ylabel("ì •í™•ë„")
	plt.xlabel("í™œì„±í™” í•¨ìˆ˜")
	plt.show()

<br>

![](./images/4-3_1.png) 
<br>

## [4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
### [4-4-1] Adam(Adaptive Moment Estimation)
â–£ ì •ì˜: Stochastic Gradient Descent(SGD)ì˜ í™•ì¥ìœ¼ë¡œ ëª¨ë©˜í…€ê³¼ ì ì‘ í•™ìŠµë¥ (Adaptive Learning Rate)ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜. ë”¥ëŸ¬ë‹ì—ì„œ ë„ë¦¬ ì‚¬ìš©(ê³¼ê±°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ê°€ì†í™”í•˜ê³  ì•ˆì •ì„±ì„ ë†’ì„)<br>
â–£ í•„ìš”ì„± : ë³µì¡í•œ ë¹„ì„ í˜• í•¨ìˆ˜ì—ì„œ ê²½ì‚¬ í•˜ê°•ë²•(SGD)ì´ ìˆ˜ë ´í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©<br>
â–£ ì£¼ìš” ê¸°ë²• : ê·¸ë˜ë””ì–¸íŠ¸ì˜ ê³¼ê±° ë°©í–¥(ëˆ„ì )ì„ ì°¸ê³ í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ ê°€ì†í™”(Momentum), ê° ë§¤ê°œë³€ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ì— ë”°ë¼ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ëŠ” ì ì‘ í•™ìŠµë¥  (Adaptive Learning Rate), ì´ë™ í‰ê·  (Exponential Moving Averages)<br>
â–£ ì¥ì  : í•™ìŠµë¥  ì¡°ì •ì´ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§, ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„, ìŠ¤íŒŒìŠ¤ ë°ì´í„° ì²˜ë¦¬ì— íš¨ê³¼ì , ê³¼ê±° ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ í™œìš©í•´ ì§„ë™(oscillation) ê°ì†Œ<br>
â–£ ë‹¨ì  : í•™ìŠµë¥ ì´ ì ì  ì‘ì•„ì ¸, ìˆ˜ë ´ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ, ì¶”ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • í•„ìš”, ê³¼ì í•© ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ì—ì„œ ì£¼ë¡œ ì‚¬ìš©(CNN, RNN, GAN, Transformer ë“±)<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
	# Adam(Adaptive Moment Estimation)
	#############################################################
	import pandas as pd
	import numpy as np
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.metrics import r2_score
	from tensorflow.keras.models import Sequential
	from tensorflow.keras.layers import Dense
	from tensorflow.keras.optimizers import Adam

	# ë°ì´í„° ë¡œë“œ
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì •ì˜
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
	        ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì „ì²˜ë¦¬
	X_train_processed = preprocessor.fit_transform(X_train)
	X_test_processed = preprocessor.transform(X_test)

	# ë°ì´í„° ì •ê·œí™”
	y_train = np.log1p(y_train)  # ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ê°’ì„ ìŠ¤ì¼€ì¼ ì¡°ì •
	y_test = np.log1p(y_test)

	# ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (Adam ì—†ì´ ë‹¨ìˆœ ì„ í˜• íšŒê·€ ëª¨ë¸)
	from sklearn.linear_model import LinearRegression

	lr_model = LinearRegression()
	lr_model.fit(X_train_processed, y_train)
	y_pred_lr = lr_model.predict(X_test_processed)
	r2_lr = r2_score(y_test, y_pred_lr)
	print(f"ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ RÂ² ì ìˆ˜: {r2_lr:.2f}")

	# Adam Optimizer ê¸°ë°˜ ëª¨ë¸ êµ¬ì¶•
	adam_model = Sequential()
	adam_model.add(Dense(128, input_dim=X_train_processed.shape[1], activation='relu'))
	adam_model.add(Dense(64, activation='relu'))
	adam_model.add(Dense(1))  # ì¶œë ¥ì¸µ

	# Adam Optimizer ì„¤ì •
	adam_optimizer = Adam(learning_rate=0.001)
	adam_model.compile(optimizer=adam_optimizer, loss='mean_squared_error', metrics=['mae'])

	# Adam ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ
	adam_model.fit(X_train_processed, y_train, epochs=50, batch_size=32, verbose=1)

	# í‰ê°€ ë° RÂ² ì ìˆ˜ ê³„ì‚°
	y_pred_adam = adam_model.predict(X_test_processed)
	r2_adam = r2_score(y_test, y_pred_adam)
	print(f"Adam Optimizer ê¸°ë°˜ ëª¨ë¸ RÂ² ì ìˆ˜: {r2_adam:.2f}")

<br>

	ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ RÂ² ì ìˆ˜: 0.65
	Adam Optimizer ê¸°ë°˜ ëª¨ë¸ RÂ² ì ìˆ˜: 0.78

<br>

### [4-4-2] SGD(Stochastic Gradient Descent)
â–£ ì •ì˜ : ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì˜ ë³€í˜•ìœ¼ë¡œ, ê° ë°°ì¹˜(batch) ë˜ëŠ” ìƒ˜í”Œì— ëŒ€í•´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„± : ë°ì´í„°ê°€ í´ìˆ˜ë¡ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì†ë„ë¥¼ ê°œì„ <br>
â–£ ì£¼ìš” ê¸°ë²• : ë°°ì¹˜ì— ëŒ€í•´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³  ì¦‰ê°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸, ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ì¼ë¶€ ë°ì´í„°(ë°°ì¹˜)ë¥¼ í™œìš©í•œ í™•ë¥ ì  ì ‘ê·¼<br>
â–£ ì¥ì  : ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° íš¨ìœ¨ì„±, ê°„ë‹¨í•œ êµ¬í˜„ìœ¼ë¡œ ëª¨ë¸ ì¼ë°˜í™”(generalization)ì— ìœ ë¦¬<br>
â–£ ë‹¨ì  : ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì €ì  ì£¼ë³€ì—ì„œ ì§„ë™(oscillation : ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ì—ì„œ ìµœì €ì (ìµœì ê°’, Optimum) ì£¼ë³€ì—ì„œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë°©í–¥ì´ ê³„ì† ë°”ë€ŒëŠ” í˜„ìƒ)ì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°. í•™ìŠµë¥  ì„¤ì •ì´ ë¯¼ê°í•˜ê³ , ëŠë¦° ìˆ˜ë ´ ì†ë„<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹: Logistic Regression, Linear Regression, CNN, RNN<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
	# SGD(Stochastic Gradient Descent)
	#############################################################
	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.metrics import r2_score
	from sklearn.ensemble import RandomForestRegressor
	from sklearn.linear_model import LinearRegression, SGDRegressor
	import warnings

	# matplotlib ê²½ê³  ë¬´ì‹œ
	warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib.font_manager")

	# ê¸°ë³¸ í°íŠ¸ë¥¼ 'DejaVu Sans'ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
	plt.rcParams['font.family'] = 'DejaVu Sans'
	plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

	# ë°ì´í„° ë¡œë“œ
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì •ì˜
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	# ì´ìƒì¹˜ ì œê±° (ìƒìœ„ 99% ê°’ ì œí•œ)
	upper_limit = np.percentile(y, 99)
	y = np.clip(y, None, upper_limit)

	# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
	        ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì „ì²˜ë¦¬
	X_train_processed = preprocessor.fit_transform(X_train)
	X_test_processed = preprocessor.transform(X_test)

	# 1. ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
	lr_model = LinearRegression()
	lr_model.fit(X_train_processed, y_train)
	y_pred_lr = lr_model.predict(X_test_processed)
	r2_lr = r2_score(y_test, y_pred_lr)
	print(f"Linear Regression RÂ² score: {r2_lr:.2f}")

	# 2. Random Forest ëª¨ë¸ í•™ìŠµ
	rf_model = RandomForestRegressor(
	    n_estimators=100,    # íŠ¸ë¦¬ ê°œìˆ˜
	    max_depth=10,        # ìµœëŒ€ ê¹Šì´
	    random_state=42)
	rf_model.fit(X_train_processed, y_train)
	y_pred_rf = rf_model.predict(X_test_processed)
	r2_rf = r2_score(y_test, y_pred_rf)
	print(f"Random Forest RÂ² score: {r2_rf:.2f}")

	# 3. SGD Regressor ëª¨ë¸ í•™ìŠµ
	sgd_model = SGDRegressor(
	    max_iter=3000,
	    tol=1e-4,
	    eta0=0.01,
	    learning_rate='adaptive',
	    shuffle=True,
	    early_stopping=True,
	    validation_fraction=0.1,
	    random_state=42)
	sgd_model.fit(X_train_processed, y_train)
	y_pred_sgd = sgd_model.predict(X_test_processed)
	r2_sgd = r2_score(y_test, y_pred_sgd)
	print(f"SGD Regressor RÂ² score: {r2_sgd:.2f}")

	# ì‹œê°í™”
	plt.figure(figsize=(18, 6))

	# Linear Regression
	plt.subplot(1, 3, 1)
	plt.scatter(y_test, y_pred_lr, alpha=0.6, color='green', label='Linear Regression Predictions')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal Fit Line')
	plt.xlabel("Actual Values")
	plt.ylabel("Predicted Values")
	plt.title("Linear Regression: Predicted vs Actual")
	plt.legend()

	# Random Forest
	plt.subplot(1, 3, 2)
	plt.scatter(y_test, y_pred_rf, alpha=0.6, color='blue', label='Random Forest Predictions')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal Fit Line')
	plt.xlabel("Actual Values")
	plt.ylabel("Predicted Values")
	plt.title("Random Forest: Predicted vs Actual")
	plt.legend()

	# SGD Regressor
	plt.subplot(1, 3, 3)
	plt.scatter(y_test, y_pred_sgd, alpha=0.6, color='purple', label='SGD Predictions')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal Fit Line')
	plt.xlabel("Actual Values")
	plt.ylabel("Predicted Values")
	plt.title("SGD Regressor: Predicted vs Actual")
	plt.legend()
	plt.tight_layout()
	plt.show()

<br>

![](./images/442.png) 
<br>

### [4-4-3] RMSprop
â–£ ì •ì˜ : ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ í•™ìŠµë¥  ê°ì†Œ(Adaptive Learning Rate)ì™€ ë£¨íŠ¸ í‰ê·  ì œê³±(Root Mean Square Propagation) ê°œë…ì„ í™œìš©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜<br>
â–£ í•„ìš”ì„± : í•™ìŠµ ê³¼ì •ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í¬ê¸°ê°€ ì§€ë‚˜ì¹˜ê²Œ í¬ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì£¼ìš” ê¸°ë²• : ê° ë§¤ê°œë³€ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •, ê·¸ë˜ë””ì–¸íŠ¸ì˜ ì œê³± í‰ê· ì„ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•´ í•™ìŠµë¥ ì„ ì—…ë°ì´íŠ¸<br>
â–£ ì¥ì  : ì§„ë™(oscillation) ê°ì†Œ, í•™ìŠµë¥ ì´ ìë™ìœ¼ë¡œ ì¡°ì •ë˜ì–´ ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢ì€ ê³¨ì§œê¸°ë¥¼ ë¹ ë¥´ê²Œ íƒìƒ‰, SGDë³´ë‹¤ ì•ˆì •ì <br>
â–£ ë‹¨ì  : ì¥ê¸°ì ìœ¼ë¡œëŠ” ì ì‘ í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì•„ì ¸ í•™ìŠµì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŒ, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : RNN ë° LSTM ê°™ì€ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ëª¨ë¸, CNN ê¸°ë°˜ ëª¨ë¸<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-4] ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Optimizer Selection) : Adam, SGD, RMSprop
	# RMSprop
	#############################################################
	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.metrics import r2_score
	from sklearn.ensemble import RandomForestRegressor
	from sklearn.linear_model import LinearRegression, SGDRegressor
	import warnings

	# matplotlib ê²½ê³  ë¬´ì‹œ
	warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib.font_manager")

	# ê¸°ë³¸ í°íŠ¸ë¥¼ 'DejaVu Sans'ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
	plt.rcParams['font.family'] = 'DejaVu Sans'
	plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

	# ë°ì´í„° ë¡œë“œ
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì •ì˜
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	# ì´ìƒì¹˜ ì œê±° (ìƒìœ„ 99% ê°’ ì œí•œ)
	upper_limit = np.percentile(y, 99)
	y = np.clip(y, None, upper_limit)

	# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
	        ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì „ì²˜ë¦¬
	X_train_processed = preprocessor.fit_transform(X_train)
	X_test_processed = preprocessor.transform(X_test)

	# 1. ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
	lr_model = LinearRegression()
	lr_model.fit(X_train_processed, y_train)
	y_pred_lr = lr_model.predict(X_test_processed)
	r2_lr = r2_score(y_test, y_pred_lr)
	print(f"Linear Regression RÂ² score: {r2_lr:.2f}")

	# 2. Random Forest ëª¨ë¸ í•™ìŠµ
	rf_model = RandomForestRegressor(
	    n_estimators=100,    # íŠ¸ë¦¬ ê°œìˆ˜
	    max_depth=10,        # ìµœëŒ€ ê¹Šì´
	    random_state=42)
	rf_model.fit(X_train_processed, y_train)
	y_pred_rf = rf_model.predict(X_test_processed)
	r2_rf = r2_score(y_test, y_pred_rf)
	print(f"Random Forest RÂ² score: {r2_rf:.2f}")

	# 3. SGD Regressor ëª¨ë¸ í•™ìŠµ
	sgd_model = SGDRegressor(
	    max_iter=3000,
	    tol=1e-4,
	    eta0=0.01,
	    learning_rate='adaptive',
	    shuffle=True,
	    early_stopping=True,
	    validation_fraction=0.1,
	    random_state=42)
	sgd_model.fit(X_train_processed, y_train)
	y_pred_sgd = sgd_model.predict(X_test_processed)
	r2_sgd = r2_score(y_test, y_pred_sgd)
	print(f"SGD Regressor RÂ² score: {r2_sgd:.2f}")

	# ì‹œê°í™”
	plt.figure(figsize=(18, 6))

	# Linear Regression
	plt.subplot(1, 3, 1)
	plt.scatter(y_test, y_pred_lr, alpha=0.6, color='green', label='Linear Regression Predictions')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal Fit Line')
	plt.xlabel("Actual Values")
	plt.ylabel("Predicted Values")
	plt.title("Linear Regression: Predicted vs Actual")
	plt.legend()

	# Random Forest
	plt.subplot(1, 3, 2)
	plt.scatter(y_test, y_pred_rf, alpha=0.6, color='blue', label='Random Forest Predictions')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal Fit Line')
	plt.xlabel("Actual Values")
	plt.ylabel("Predicted Values")
	plt.title("Random Forest: Predicted vs Actual")
	plt.legend()

	# SGD Regressor
	plt.subplot(1, 3, 3)
	plt.scatter(y_test, y_pred_sgd, alpha=0.6, color='purple', label='SGD Predictions')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal Fit Line')
	plt.xlabel("Actual Values")
	plt.ylabel("Predicted Values")
	plt.title("SGD Regressor: Predicted vs Actual")
	plt.legend()
	plt.tight_layout()
	plt.show()

<br>

![](./images/443.png) 
<br>

## [4-5] ì „ì´ í•™ìŠµ(Transfer Learning)
â–£ ì •ì˜ : ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ìƒˆë¡œìš´ ë¬¸ì œì— ì¬ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•(Pre-trained Model Utilization)<br>
â–£ í•„ìš”ì„± : ë°ì´í„° ë¶€ì¡± ìƒí™©ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì¥, í•™ìŠµ ì‹œê°„ì„ í¬ê²Œ ë‹¨ì¶•<br>
â–£ ì¥ì  : ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ ê°€ëŠ¥, ë¹ ë¥¸ í•™ìŠµê³¼ ë†’ì€ ì´ˆê¸° ì„±ëŠ¥<br>
â–£ ë‹¨ì  : ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë¬¸ì œì— ìµœì í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ì‚¬ì „ í•™ìŠµëœ ë°ì´í„°ì…‹ê³¼ ë„ë©”ì¸ ì°¨ì´ê°€ í´ ê²½ìš° ì„±ëŠ¥ ì €í•˜<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì´ë¯¸ì§€ ì²˜ë¦¬(CNN), ìì—°ì–´ ì²˜ë¦¬(Transformer, GPT)<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-5] ì „ì´ í•™ìŠµ (Transfer Learning)
	#############################################################
	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import r2_score
	from sklearn.datasets import load_iris

	# Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris()
	X = iris.data  # íŠ¹ì„±
	y = iris.target  # íƒ€ê²Ÿ

	# í´ë˜ìŠ¤ 0ê³¼ 1ë§Œ ì‚¬ìš©í•˜ì—¬ ì´ì§„ íšŒê·€ë¡œ ê°„ì†Œí™”
	binary_indices = y != 2
	X = X[binary_indices]
	y = y[binary_indices]

	# ë°ì´í„°ì…‹ì„ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬
	X_source, X_target, y_source, y_target = train_test_split(X, y, test_size=0.5, random_state=42)

	# ì†ŒìŠ¤ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµ
	model_source = LinearRegression()
	model_source.fit(X_source, y_source)
	y_pred_source = model_source.predict(X_target)
	r2_source = r2_score(y_target, y_pred_source)

	# íƒ€ê²Ÿ ë°ì´í„°ì…‹ì—ì„œ ì „ì´ í•™ìŠµ ìˆ˜í–‰
	model_target = LinearRegression()
	model_target.coef_ = model_source.coef_  # ì†ŒìŠ¤ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” ê°’ìœ¼ë¡œ ì‚¬ìš©
	model_target.intercept_ = model_source.intercept_

	# íƒ€ê²Ÿ ë°ì´í„°ì…‹ì—ì„œ ì¬í•™ìŠµ
	model_target.fit(X_target, y_target)
	y_pred_target = model_target.predict(X_target)
	r2_target = r2_score(y_target, y_pred_target)

	# ì‹œê°í™”
	plt.figure(figsize=(12, 6))

	# ì „ì´ í•™ìŠµ ì „
	plt.subplot(1, 2, 1)
	plt.scatter(y_target, y_pred_source, color="blue", label="ì „ì´ í•™ìŠµ ì „")
	plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="ì´ìƒì  ì í•©ì„ ")
	plt.title(f"ì „ì´ í•™ìŠµ ì „ R2: {r2_source:.2f}")
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()

	# ì „ì´ í•™ìŠµ í›„
	plt.subplot(1, 2, 2)
	plt.scatter(y_target, y_pred_target, color="green", label="ì „ì´ í•™ìŠµ í›„")
	plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="ì´ìƒì  ì í•©ì„ ")
	plt.title(f"ì „ì´ í•™ìŠµ í›„ R2: {r2_target:.2f}")
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()
	plt.tight_layout()
	plt.show()

	# ê²°ê³¼ ë¶„ì„ ì¶œë ¥
	print(f"""
	### ê²°ê³¼ ë¶„ì„
	1. **ì „ì´ í•™ìŠµ ì „ RÂ² ì ìˆ˜**: {r2_source:.2f}
	   - ì†ŒìŠ¤ ë°ì´í„°ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ íƒ€ê²Ÿ ë°ì´í„°ì— ì ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	   - ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë°ì´í„°ê°€ ìœ ì‚¬í•˜ë‹¤ë©´, ì´ ì ìˆ˜ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	2. **ì „ì´ í•™ìŠµ í›„ RÂ² ì ìˆ˜**: {r2_target:.2f}
	   - ì†ŒìŠ¤ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ íƒ€ê²Ÿ ë°ì´í„°ì—ì„œ ì¬í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤.
	   - ì¬í•™ìŠµì„ í†µí•´ ëª¨ë¸ì´ íƒ€ê²Ÿ ë°ì´í„°ì— ë” ì˜ ì í•©í•˜ë„ë¡ ì¡°ì •ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - ì „ì´ í•™ìŠµì€ ì†ŒìŠ¤ ë°ì´í„°ì—ì„œ í•™ìŠµëœ ì •ë³´ë¥¼ íƒ€ê²Ÿ ë°ì´í„°ì— í™œìš©í•˜ì—¬, í•™ìŠµ ì‹œê°„ì„ ì¤„ì´ê±°ë‚˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	   - ê·¸ëŸ¬ë‚˜ ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë°ì´í„° ê°„ì˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ê²½ìš°, ì„±ëŠ¥ ê°œì„ ì´ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	""")

<br>

![](./images/4-5.png) 
<br>

## [4-6] ëª¨ë¸êµ¬ì¡° ìµœì í™”(Model Architecture Optimization)
â–£ ì •ì˜ : ëª¨ë¸ì˜ êµ¬ì¡°(ë ˆì´ì–´ ìˆ˜, ë‰´ëŸ° ìˆ˜, ì—°ê²° ë°©ì‹ ë“±)ë¥¼ ìµœì í™”í•˜ì—¬ í•™ìŠµ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : ë³µì¡í•œ ëª¨ë¸ êµ¬ì¡°ëŠ” ê³¼ì í•© ìœ„í—˜ ì¦ê°€, ë‹¨ìˆœí•œ êµ¬ì¡°ëŠ” í‘œí˜„ë ¥ì´ ë¶€ì¡±í•˜ë¯€ë¡œ ì ì ˆí•œ ê· í˜• í•„ìš”<br>
â–£ ì¥ì  : ë°ì´í„°ì™€ ë¬¸ì œì— ì í•©í•œ ëª¨ë¸ ì„¤ê³„ ê°€ëŠ¥, ê³¼ì í•© ìœ„í—˜ ê°ì†Œ<br>
â–£ ë‹¨ì  : ì„¤ê³„ì— ë§ì€ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ ì†Œëª¨, ìë™í™” ë„êµ¬ ì‚¬ìš© ì‹œ ë†’ì€ ê³„ì‚° ë¹„ìš© ë°œìƒ ê°€ëŠ¥<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ëª¨ë¸ (íŠ¹íˆ ì‹ ê²½ë§)<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-6] ëª¨ë¸ êµ¬ì¡° ìµœì í™” (Model Architecture Optimization)
	#############################################################
	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.datasets import load_iris
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import PolynomialFeatures, StandardScaler
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import r2_score

	# Iris ë°ì´í„° ë¡œë“œ
	iris = load_iris()
	X = iris.data  # íŠ¹ì„±
	y = iris.target  # íƒ€ê²Ÿ

	# ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë³€í™˜ (í´ë˜ìŠ¤ 0ê³¼ 1ë§Œ ì‚¬ìš©)
	binary_indices = y != 2
	X = X[binary_indices]
	y = y[binary_indices]

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
	scaler = StandardScaler()
	X_train_scaled = scaler.fit_transform(X_train)
	X_test_scaled = scaler.transform(X_test)

	# ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸
	model = LinearRegression()
	model.fit(X_train_scaled, y_train)
	y_pred_basic = model.predict(X_test_scaled)
	r2_basic = r2_score(y_test, y_pred_basic)

	# ëª¨ë¸ êµ¬ì¡° ìµœì í™”: ë‹¤í•­ì‹ ë³€í™˜
	poly = PolynomialFeatures(degree=2, include_bias=False)
	X_train_poly = poly.fit_transform(X_train_scaled)
	X_test_poly = poly.transform(X_test_scaled)

	# ìµœì í™”ëœ ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
	model_optimized = LinearRegression()
	model_optimized.fit(X_train_poly, y_train)
	y_pred_optimized = model_optimized.predict(X_test_poly)
	r2_optimized = r2_score(y_test, y_pred_optimized)

	# ì‹œê°í™”
	plt.figure(figsize=(12, 6))

	# ê¸°ë³¸ ëª¨ë¸ ì‹œê°í™”
	plt.subplot(1, 2, 1)
	plt.scatter(y_test, y_pred_basic, color="blue", label="ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡")
	plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="Ideal Fit")
	plt.title(f"ê¸°ë³¸ ëª¨ë¸ R2: {r2_basic:.2f}")
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()

	# ìµœì í™” ëª¨ë¸ ì‹œê°í™”
	plt.subplot(1, 2, 2)
	plt.scatter(y_test, y_pred_optimized, color="green", label="ìµœì í™” ëª¨ë¸ ì˜ˆì¸¡")
	plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="Ideal Fit")
	plt.title(f"ìµœì í™” ëª¨ë¸ R2: {r2_optimized:.2f}")
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()	
	plt.tight_layout()
	plt.show()

	# ê²°ê³¼ ë¶„ì„ ì¶œë ¥
	print(f"""
	### ê²°ê³¼ ë¶„ì„
	1. **ê¸°ë³¸ ì„ í˜• íšŒê·€ RÂ² ì ìˆ˜**: {r2_basic:.2f}
	   - ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ì›ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤.
	   - ë°ì´í„°ì˜ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì§€ ëª»í•´ ë‚®ì€ RÂ² ì ìˆ˜ë¥¼ ê¸°ë¡í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
	2. **ìµœì í™”ëœ ëª¨ë¸ RÂ² ì ìˆ˜**: {r2_optimized:.2f}
	   - ë‹¤í•­ì‹ ë³€í™˜ì„ í†µí•´ íŠ¹ì„± ê°„ì˜ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ë°˜ì˜í•œ ëª¨ë¸ì…ë‹ˆë‹¤.
	   - ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - Iris ë°ì´í„°ëŠ” ë¹„ì„ í˜• ê´€ê³„ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ë‹¤í•­ì‹ ë³€í™˜ê³¼ ê°™ì€ ëª¨ë¸ êµ¬ì¡° ìµœì í™” ê¸°ë²•ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	   - ìµœì í™”ê°€ íš¨ê³¼ì ì´ì—ˆë‹¤ë©´, RÂ² ì ìˆ˜ì˜ ìƒìŠ¹ì„ í†µí•´ ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	""")

<br>

![](./images/4-6.png) 
<br>

## [4-7] ì˜¨ë¼ì¸ í•™ìŠµ(Online Learning)
â–£ ì •ì˜ : ì ì§„ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ë©° ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë°ì´í„°ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê±°ë‚˜, ì €ì¥ ê³µê°„ì´ ì œí•œì ì¸ ê²½ìš°<br>
â–£ ì¥ì  : ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ<br>
â–£ ë‹¨ì  : ì˜ëª»ëœ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ëª¨ë¸ì— ì¦‰ì‹œ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµ ê³¼ì • ì¶”ì  ë° ë””ë²„ê¹…ì´ ì–´ë ¤ì›€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ëª¨ë¸ (ì˜ˆ: ì˜¨ë¼ì¸ ì¶”ì²œ ì‹œìŠ¤í…œ, ì‹¤ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸), SGD ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜<br>
Scikit-learnì—ì„œ ì£¼ë¡œ SGD ê¸°ë°˜ ëª¨ë¸(SGDClassifier, SGDRegressor, PassiveAggressiveClassifier, PassiveAggressiveRegressor), ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸, MiniBatchKMeansì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë“± ì œê³µ(partial_fit() ë©”ì„œë“œë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ì€ ì¼ë¶€ì— í•œì •)<br>

	#############################################################
	# [4] í•™ìŠµ ê³¼ì • ìµœì í™”
	# [4-7] ì˜¨ë¼ì¸ í•™ìŠµ (Online Learning)
	# Online Learningì€ ë°ì´í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ í•™ìŠµí•˜ë¯€ë¡œ, 
	# ì •í™•ë„ë³´ë‹¤ëŠ” ì‹¤ì‹œê°„ì„±ê³¼ í™•ì¥ì„±(ë°ì´í„° ìŠ¤íŠ¸ë¦¼, ì ì§„ì  í•™ìŠµ)ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆì–´ì„œ, 
	# Batch Learningë³´ë‹¤ í•­ìƒ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ëŠ” ê²ƒì€ ì œí•œì 
	#############################################################
	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.linear_model import SGDRegressor
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.metrics import r2_score
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.utils import shuffle

	# ë°ì´í„° ë¡œë“œ
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì´ë¦„ í™•ì¸
	print("ë°ì´í„°ì…‹ ì—´ ì´ë¦„:", housing_data.columns)

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	# íƒ€ê²Ÿ ì—´ ì œì™¸
	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ëŒ€ì²´ (ìˆ«ìí˜•: í‰ê· , ë²”ì£¼í˜•: ìµœë¹ˆê°’)
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
	preprocessor_batch = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
	        ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	preprocessor_online = ColumnTransformer(
	    transformers=[
 	       ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
 	       ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# Batch Learning: Gradient Boosting Regressor
	batch_pipeline = Pipeline(steps=[
	    ('preprocessor', preprocessor_batch),
	    ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))])

	# Batch Learning í•™ìŠµ
	batch_pipeline.fit(X_train, y_train)
	y_pred_batch = batch_pipeline.predict(X_test)
	r2_batch = r2_score(y_test, y_pred_batch)

	# Online Learning ëª¨ë¸ ì„¤ì •
	online_model = SGDRegressor(
	    max_iter=1, 
	    eta0=0.0001,  # í•™ìŠµë¥  ê°ì†Œ
	    learning_rate='adaptive', 
	    penalty='l2',  # L2 ì •ê·œí™” ì¶”ê°€
	    random_state=42,
	    tol=1e-3)

	# ë°ì´í„° ì „ì²˜ë¦¬ (Online Learning)
	X_train_processed = preprocessor_online.fit_transform(X_train)
	X_test_processed = preprocessor_online.transform(X_test)

	# Online Learning í•™ìŠµ
	n_epochs = 200  # Epoch ìˆ˜ ì¦ê°€
	batch_size = 100  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
	for epoch in range(n_epochs):
	    X_train_shuffled, y_train_shuffled = shuffle(X_train_processed, y_train, random_state=epoch)
	    for i in range(0, len(X_train_shuffled), batch_size):
	        batch_X = X_train_shuffled[i:i + batch_size]
	        batch_y = y_train_shuffled[i:i + batch_size]
 	       online_model.partial_fit(batch_X, batch_y)

	# Online Learning ì˜ˆì¸¡ ë° í‰ê°€
	y_pred_online = online_model.predict(X_test_processed)
	r2_online = r2_score(y_test, y_pred_online)

	# ì‹œê°í™”
	plt.figure(figsize=(12, 6))

	# Batch Learning ê²°ê³¼
	plt.subplot(1, 2, 1)
	plt.scatter(y_test, y_pred_batch, color="blue", label="Batch Learning ì˜ˆì¸¡")
	plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color="red", linestyle="--", label="Ideal Fit")
	plt.title(f"Batch Learning R2: {r2_batch:.2f}")
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()

	# Online Learning ê²°ê³¼
	plt.subplot(1, 2, 2)
	plt.scatter(y_test, y_pred_online, color="green", label="Online Learning ì˜ˆì¸¡")
	plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color="red", linestyle="--", label="Ideal Fit")
	plt.title(f"Enhanced Online Learning R2: {r2_online:.2f}")
	plt.xlabel("ì‹¤ì œ ê°’")
	plt.ylabel("ì˜ˆì¸¡ ê°’")
	plt.legend()
	plt.tight_layout()
	plt.show()

	# ê²°ê³¼ ë¶„ì„ ì¶œë ¥
	print(f"""
	### ê²°ê³¼ ë¶„ì„
	1. **Batch Learning RÂ² ì ìˆ˜**: {r2_batch:.2f}
	   - Gradient Boosting Regressorë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤.
	2. **ê°•í™”ëœ Online Learning RÂ² ì ìˆ˜**: {r2_online:.2f}
	   - ì ì§„ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ Online Learningì˜ í•™ìŠµì„ ì•ˆì •í™”í–ˆìŠµë‹ˆë‹¤.
	   - Batch Learningì€ ë¹„ì„ í˜• íŠ¹ì„±ì„ ë” ì˜ í•™ìŠµí•˜ì—¬ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
	""")
 
<br>

![](./images/4-7.png) 
<br>

---

# [5] ì„±ëŠ¥ í–¥ìƒ
## [5-1] íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ(Feature Selection)
â–£ ì •ì˜ : ëª¨ë¸ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì‹ë³„í•˜ê³ , ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ìƒê´€ì„±ì´ ë‚®ì€ íŠ¹ì„±ì„ ì œê±°í•˜ëŠ” ê³¼ì •<br>
(Filter Methods: ìƒê´€ê³„ìˆ˜, ì¹´ì´ì œê³± ê²€ì • ë“±, Wrapper Methods: ìˆœì°¨ì „ì§„ì„ íƒ(SFS), ìˆœì°¨í›„ì§„ì œê±°(SBS), Embedded Methods: L1 ì •ê·œí™”, ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê¸°ë°˜ ì¤‘ìš”ë„)<br>
â–£ í•„ìš”ì„± : ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì€ í•™ìŠµ ì‹œê°„ì„ ì¦ê°€ì‹œí‚¤ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, íŠ¹ì„± ì„ íƒì€ ëª¨ë¸ ë‹¨ìˆœí™”ì™€ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬<br>
â–£ ì¥ì  : ê³¼ì í•© ìœ„í—˜ ê°ì†Œ, í•™ìŠµ ì‹œê°„ ë‹¨ì¶•, ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± ì¦ê°€<br>
â–£ ë‹¨ì  : íŠ¹ì„± ì„ íƒ ê³¼ì •ì´ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆìœ¼ë©°, ì¤‘ìš”í•œ íŠ¹ì„±ì„ ë†“ì¹  ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ ê³ ì°¨ì› ë°ì´í„°ì…‹ì´ í¬í•¨ëœ ë¬¸ì œ<br>
Scikit-learnì—ì„œ Variance Threshold(íŠ¹ì„±ì˜ ë¶„ì‚°ì´ ë‚®ì€ íŠ¹ì„± ì œê±°), SelectKBest(ê°€ì¥ ì¤‘ìš”í•œ Kê°œì˜ íŠ¹ì„±ì„ ì„ íƒ), SelectPercentile(ìƒìœ„ n%ì˜ íŠ¹ì„±ì„ ì„ íƒ) í•¨ìˆ˜ ì œê³µ<br>

	#############################################################
	# [5] ì„±ëŠ¥ í–¥ìƒ
	# [5-1] íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ(Feature Importance & Selection)
	#############################################################
	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer

	# ë°ì´í„° ë¡œë“œ
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì´ë¦„ í™•ì¸
	print("ë°ì´í„°ì…‹ ì—´ ì´ë¦„:", housing_data.columns)

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	# íƒ€ê²Ÿ ì—´ ì œì™¸
	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ëŒ€ì²´ (ìˆ«ìí˜•: í‰ê· , ë²”ì£¼í˜•: ìµœë¹ˆê°’)
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
	        ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì „ì²˜ë¦¬
	X_train_processed = preprocessor.fit_transform(X_train)
	X_test_processed = preprocessor.transform(X_test)

	# Gradient Boosting Regressor ëª¨ë¸ í•™ìŠµ
	model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
	model.fit(X_train_processed, y_train)

	# ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜
	y_pred = model.predict(X_test_processed)
	r2_original = r2_score(y_test, y_pred)
	print("ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜: {:.2f}".format(r2_original))

	# íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ
	feature_importances = model.feature_importances_

	# ì¤‘ìš”ë„ ê¸°ì¤€ ì—†ì´ ê·œì œ ì ìš©
	selected_features = feature_importances > 0  # ì¤‘ìš”ë„ê°€ 0 ì´ìƒì¸ ëª¨ë“  íŠ¹ì„±ì„ í¬í•¨
	X_train_selected = X_train_processed[:, selected_features]
	X_test_selected = X_test_processed[:, selected_features]

	# ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
	model_selected = GradientBoostingRegressor(n_estimators=150, learning_rate=0.08, max_depth=4, random_state=42)
	model_selected.fit(X_train_selected, y_train)

	# íŠ¹ì„± ì„ íƒ í›„ ëª¨ë¸ RÂ² ì ìˆ˜
	y_pred_selected = model_selected.predict(X_test_selected)
	r2_selected = r2_score(y_test, y_pred_selected)
	print("íŠ¹ì„± ì„ íƒ í›„ ëª¨ë¸ RÂ² ì ìˆ˜: {:.2f}".format(r2_selected))

	# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
	plt.figure(figsize=(10, 6))
	plt.bar(range(len(feature_importances)), feature_importances, color="blue")
	plt.title("Feature Importances")
	plt.xlabel("Feature Index")
	plt.ylabel("Importance Score")
	plt.axhline(y=0, color='red', linestyle='--', label='Threshold: Include All Features')
	plt.legend()
	plt.show()

	# ê²°ê³¼ ì¶œë ¥
	print("""
	### ê²°ê³¼ ë¶„ì„
	1. **ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜**: {:.2f}
	   - ì „ì²´ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	2. **íŠ¹ì„± ì„ íƒ í›„ ëª¨ë¸ RÂ² ì ìˆ˜**: {:.2f}
	   - ì¤‘ìš”ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  íŠ¹ì„±ì„ ìœ ì§€í•˜ê³  ê·œì œë¥¼ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - íŠ¹ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ëª¨ë¸ êµ¬ì¡° ìµœì í™”ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
	   - ê·œì œì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ íš¨ê³¼ì ì¸ ê°œì„ ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.
	""".format(r2_original, r2_selected))

<br>

![](./images/5-1.png) 
<br>

## [5-2] ì†ì‹¤í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•(Custom Loss Function)
â–£ ì •ì˜ : ë¬¸ì œì˜ íŠ¹ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ì„¤ê³„í•˜ê±°ë‚˜ ê¸°ì¡´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë³€í˜•í•˜ì—¬ ì‚¬ìš©<br>
â–£ í•„ìš”ì„± : ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ê°€ ë¬¸ì œì˜ ëª©í‘œë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í•  ê²½ìš°, ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : ë¬¸ì œì˜ ìš”êµ¬ì‚¬í•­ì— íŠ¹í™”ëœ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥, ì†ì‹¤ í•¨ìˆ˜ ìì²´ê°€ ëª¨ë¸ í•™ìŠµ ë°©í–¥ì„ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— ì„¸ë°€í•œ ì¡°ì • ê°€ëŠ¥<br>
â–£ ë‹¨ì  : êµ¬í˜„ì´ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë©°, ì†ì‹¤ í•¨ìˆ˜ ì„¤ê³„ ì˜¤ë¥˜ëŠ” í•™ìŠµ ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§ˆ ê°€ëŠ¥ì„±<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, íŠ¹íˆ ë¹„ì •í˜• ë°ì´í„°, ë¶ˆê· í˜• ë°ì´í„° ë¬¸ì œì— ì í•©<br>
Scikit-learnì—ì„œëŠ” ì£¼ë¡œ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í†µí•©í•˜ë ¤ë©´ make_scorerë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì§€í‘œë¡œ ì •ì˜<br>

	#############################################################
	# [5] ì„±ëŠ¥ í–¥ìƒ
	# [5-2] ì†ì‹¤ í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§• (Custom Loss Function)
	#############################################################
	import numpy as np
	import pandas as pd
	from sklearn.ensemble import GradientBoostingRegressor
	from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
	from sklearn.model_selection import train_test_split, GridSearchCV
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer

	# ì»¤ìŠ¤í„°ë§ˆì´ì§• ì†ì‹¤ í•¨ìˆ˜ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
	class CustomLossGradientBoostingRegressor(GradientBoostingRegressor):
	    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, alpha=0.5):
	        super().__init__(
 	           n_estimators=n_estimators,
 	           learning_rate=learning_rate,
 	           max_depth=max_depth,
  	          random_state=42)
        	self.alpha = alpha  # MSEì™€ MAEì˜ í˜¼í•© ë¹„ìœ¨

    	def custom_loss(self, y_true, y_pred):
        	# MSEì™€ MAEë¥¼ í˜¼í•©í•œ ì†ì‹¤ í•¨ìˆ˜
        	mse_loss = mean_squared_error(y_true, y_pred)
        	mae_loss = mean_absolute_error(y_true, y_pred)
        	return self.alpha * mse_loss + (1 - self.alpha) * mae_loss

    	def fit(self, X, y):
        	super().fit(X, y)
       	 	return self

	# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì •ì˜
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
	        ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì „ì²˜ë¦¬
	X_train_processed = preprocessor.fit_transform(X_train)
	X_test_processed = preprocessor.transform(X_test)

	# ê¸°ë³¸ ëª¨ë¸ (MSE ê¸°ë°˜)
	base_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
	base_model.fit(X_train_processed, y_train)
	y_pred_base = base_model.predict(X_test_processed)
	r2_base = r2_score(y_test, y_pred_base)
	print("ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜: {:.2f}".format(r2_base))

	# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ GridSearchCV ì„¤ì •
	param_grid = {
	    "n_estimators": [100, 150, 200],
 	    "learning_rate": [0.05, 0.1, 0.2],
  	    "max_depth": [3, 4, 5],}

	custom_model = CustomLossGradientBoostingRegressor(alpha=0.7)

	grid_search = GridSearchCV(
	    estimator=custom_model,
	    param_grid=param_grid,
	    scoring="r2",
 	    cv=3,  # 3-Fold Cross-Validation
	    verbose=2,
	    n_jobs=-1)

	# Grid Search ì‹¤í–‰
	grid_search.fit(X_train_processed, y_train)

	# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
	print("ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°:", grid_search.best_params_)

	# ìµœì ì˜ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
	best_model = grid_search.best_estimator_
	y_pred_best = best_model.predict(X_test_processed)
	r2_best = r2_score(y_test, y_pred_best)
	print("ìµœì í™”ëœ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì†ì‹¤ í•¨ìˆ˜ ëª¨ë¸ RÂ² ì ìˆ˜: {:.2f}".format(r2_best))

	# ê²°ê³¼ ë¹„êµ ì¶œë ¥
	print("""
	### ê²°ê³¼ ë¶„ì„
	1. **ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜**: {:.2f}
	   - ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜(MSE)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	2. **ìµœì í™”ëœ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì†ì‹¤ í•¨ìˆ˜ ëª¨ë¸ RÂ² ì ìˆ˜**: {:.2f}
	   - MSEì™€ MAEì˜ í˜¼í•© ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	   - ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ `n_estimators`, `learning_rate`, `max_depth` ê°’ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
	""".format(r2_base, r2_best))

<br>

	ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜: 0.76
	Fitting 3 folds for each of 27 candidates, totalling 81 fits
	ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}
	ìµœì í™”ëœ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì†ì‹¤ í•¨ìˆ˜ ëª¨ë¸ RÂ² ì ìˆ˜: 0.83

	### ê²°ê³¼ ë¶„ì„
	1. **ê¸°ë³¸ ëª¨ë¸ RÂ² ì ìˆ˜**: 0.76
	   - ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜(MSE)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	2. **ìµœì í™”ëœ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì†ì‹¤ í•¨ìˆ˜ ëª¨ë¸ RÂ² ì ìˆ˜**: 0.83
	   - MSEì™€ MAEì˜ í˜¼í•© ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ í•™ìŠµí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤.
	3. **ê²°ë¡ **:
	   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	   - ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ `n_estimators`, `learning_rate`, `max_depth` ê°’ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
  
<br>

---

# [6] í•˜ë“œì›¨ì–´ ë° ì‹œìŠ¤í…œ ìµœì í™”
## [6-1] í•˜ë“œì›¨ì–´ ìµœì í™”(Hardware Optimization)
â–£ ì •ì˜ : ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ ê³¼ì •ì—ì„œ GPU, TPU ë“± í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë¥¼ í™œìš©í•˜ê±°ë‚˜, ë³‘ë ¬ ì²˜ë¦¬ì™€ ë¶„ì‚° í•™ìŠµì„ í†µí•´ ê³„ì‚°ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê¸°ë²•<br>
â–£ í•„ìš”ì„± : ë”¥ëŸ¬ë‹ ë° ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ëª¨ë¸ì—ì„œ ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì¥ì  : í•™ìŠµ ì†ë„ ë° ì¶”ë¡  ì†ë„ í–¥ìƒ, ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ëª¨ë¸ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™•ì¥ì„± ì œê³µ<br>
â–£ ë‹¨ì  : í•˜ë“œì›¨ì–´ ì¥ë¹„ì˜ ì´ˆê¸° ë¹„ìš©ì´ ë†’ìœ¼ë©°, í•˜ë“œì›¨ì–´ ìµœì í™”ë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ ì„¤ì •ê³¼ ê¸°ìˆ  ì§€ì‹ í•„ìš”<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë”¥ëŸ¬ë‹ ëª¨ë¸ (CNN, RNN, Transformer ë“±), ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ë° ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜<br>

	#############################################################
	# [6] í•˜ë“œì›¨ì–´ ë° ì‹œìŠ¤í…œ ìµœì í™”
	# [6-1] í•˜ë“œì›¨ì–´ ìµœì í™” (Hardware Optimization)
	#############################################################
	from lightgbm import LGBMRegressor
	import pandas as pd
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler, OneHotEncoder
	from sklearn.compose import ColumnTransformer
	from sklearn.pipeline import Pipeline
	from sklearn.impute import SimpleImputer
	from sklearn.metrics import r2_score
	import time

	# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
	url = "https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-15/housing.csv"
	housing_data = pd.read_csv(url)

	# ë°ì´í„° ì—´ ì •ì˜
	categorical_columns = housing_data.select_dtypes(include=['object']).columns.tolist()
	numerical_columns = housing_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

	if 'median_house_value' in numerical_columns:
	    numerical_columns.remove('median_house_value')

	# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
	numeric_imputer = SimpleImputer(strategy='mean')
	categorical_imputer = SimpleImputer(strategy='most_frequent')

	X = housing_data.drop(columns=['median_house_value'], errors='ignore')
	y = housing_data['median_house_value']

	# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
	preprocessor = ColumnTransformer(
	    transformers=[
	        ('num', Pipeline([('imputer', numeric_imputer), ('scaler', StandardScaler())]), numerical_columns),
  	      ('cat', Pipeline([('imputer', categorical_imputer), ('onehot', OneHotEncoder())]), categorical_columns)])

	# ë°ì´í„° ë¶„ë¦¬
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ë°ì´í„° ì „ì²˜ë¦¬
	X_train_processed = preprocessor.fit_transform(X_train)
	X_test_processed = preprocessor.transform(X_test)

	# ê¸°ë³¸ CPU ê¸°ë°˜ LightGBM ëª¨ë¸
	cpu_model = LGBMRegressor(
	    n_estimators=150,
	    learning_rate=0.05,
	    max_depth=7,
	    num_leaves=31,
 	    min_child_samples=10,
	    min_split_gain=0.001,
	    random_state=42,
	    device="cpu")
	    start_time_cpu = time.time()
	    cpu_model.fit(X_train_processed, y_train)
	    cpu_time = time.time() - start_time_cpu

	y_pred_cpu = cpu_model.predict(X_test_processed)
	r2_cpu = r2_score(y_test, y_pred_cpu)
	print(f"ê¸°ë³¸ CPU ê¸°ë°˜ LightGBM ëª¨ë¸ RÂ² ì ìˆ˜: {r2_cpu:.2f}")
	print(f"CPU ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì‹œê°„: {cpu_time:.2f} ì´ˆ")

	# GPU ê¸°ë°˜ LightGBM ëª¨ë¸ (GPUê°€ ìˆëŠ” ê²½ìš°)
	try:
	    gpu_model = LGBMRegressor(
  	        n_estimators=150,
 	        learning_rate=0.05,
	        max_depth=7,
	        num_leaves=31,
	        min_child_samples=10,
	        min_split_gain=0.001,
	        random_state=42,
	        device="gpu",  # GPU ì„¤ì •
	        gpu_platform_id=0,  # OpenCL í”Œë«í¼ ID
	        gpu_device_id=0     # GPU ë””ë°”ì´ìŠ¤ ID
 	   )
	    start_time_gpu = time.time()
	    gpu_model.fit(X_train_processed, y_train)
	    gpu_time = time.time() - start_time_gpu

	    y_pred_gpu = gpu_model.predict(X_test_processed)
	    r2_gpu = r2_score(y_test, y_pred_gpu)
	    print(f"GPU ê¸°ë°˜ LightGBM ëª¨ë¸ RÂ² ì ìˆ˜: {r2_gpu:.2f}")
	    print(f"GPU ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì‹œê°„: {gpu_time:.2f} ì´ˆ")
	except Exception as e:
	    print("GPU ì§€ì›ì´ ê°ì§€ë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
	    print("ì˜¤ë¥˜ ë©”ì‹œì§€:", e)
    
<br>

	[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002612 seconds.
	You can set `force_col_wise=true` to remove the overhead.
	[LightGBM] [Info] Total Bins 1846
	[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 12
	[LightGBM] [Info] Start training from score 207194.693738
	ê¸°ë³¸ CPU ê¸°ë°˜ LightGBM ëª¨ë¸ RÂ² ì ìˆ˜: 0.81
	CPU ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì‹œê°„: 0.76 ì´ˆ
	[LightGBM] [Info] This is the GPU trainer!!
	[LightGBM] [Info] Total Bins 1846
	[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 12
	GPU ì§€ì›ì´ ê°ì§€ë˜ì§€ ì•Šì•„ CPU ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
	ì˜¤ë¥˜ ë©”ì‹œì§€: No OpenCL device found

<br>

---

# [7] ëª¨ë¸ ê²€ì¦ ë° ë¹„êµ
## [7-1] ëª¨ë¸ ê²€ì¦(Model Validation)
â–£ ì •ì˜ : ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ì¼ë°˜í™”ë˜ëŠ”ì§€ í‰ê°€í•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„± : í•™ìŠµ ë°ì´í„°ì—ë§Œ ì˜ ì‘ë™í•˜ëŠ” ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê²ƒì„ë°©ì§€í•˜ê¸° ìœ„í•´ í•„ìš”<br>
â–£ ì£¼ìš”ê¸°ë²• : <br>
Hold-Out Method(ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°:ê²€ì¦ ë°ì´í„° = 80:20 ë˜ëŠ” 70:30 ë¹„ìœ¨ë¡œ êµ¬ë¶„)<br>
K-Fold Cross Validation(ë°ì´í„°ë¥¼ Kê°œì˜ í´ë“œë¡œ ë‚˜ëˆˆ ë’¤, ê° í´ë“œë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë²ˆê°ˆì•„ê°€ë©° ì‚¬ìš©)<br>
Stratified K-Fold Cross Validation(K-Fold Cross Validationì˜ ë³€í˜•ìœ¼ë¡œ, í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ê· ë“±í•˜ë„ë¡ ë°ì´í„°ë¥¼ ë‚˜ëˆ”)<br>
Leave-One-Out Cross Validation (LOOCV): ë°ì´í„°ì˜ ê° ìƒ˜í”Œì„ í•œ ë²ˆì”© ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©<br>
Time Series Validation: ì‹œê³„ì—´ ë°ì´í„°ì— ì í•©í•œ ë°©ë²•ìœ¼ë¡œ, ê³¼ê±° ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê³  ë¯¸ë˜ ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©<br>
Bootstrap Method: ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë³µì› ìƒ˜í”Œë§í•˜ì—¬ ì—¬ëŸ¬ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ê²€ì¦<br>
â–£ ì¥ì  : ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ, ê°ê´€ì  í‰ê°€, ê³¼ì í•©/ê³¼ì†Œì í•© í™•ì¸<br>
â–£ ë‹¨ì  : ì¶”ê°€ ë°ì´í„° í•„ìš”, ì‹œê°„ ë¹„ìš© ì¦ê°€, ê³¼ë„í•œ ìµœì í™” ìœ„í—˜<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì ìš©<br>

<br>

## [7-2] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ(Model Performance Comparison) 
â–£ ì •ì˜ : ì—¬ëŸ¬ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê³¼ì •<br>
â–£ í•„ìš”ì„±: ìµœì ì˜ ëª¨ë¸ ì„ íƒ, íš¨ìœ¨ì ì¸ ìì› í™œìš©<br>
â–£ ì£¼ìš” ê¸°ë²•: í‰ê°€ ì§€í‘œ ì‚¬ìš©, Cross Validation, í†µê³„ì  í…ŒìŠ¤íŠ¸(t-í…ŒìŠ¤íŠ¸ ë˜ëŠ” ANOVA), ì•™ìƒë¸” ë¹„êµ, ì‹œê°„ ë³µì¡ë„ ë° ìì› ì‚¬ìš© í‰ê°€, AutoML í™œìš©<br>
â–£ ì¥ì  : ê°€ì¥ ì í•©í•œ ëª¨ë¸ ì„ íƒ ê°€ëŠ¥, ê°ê´€ì ì¸ ë¹„êµ<br>
â–£ ë‹¨ì  : ì‹œê°„ê³¼ ìì› ì†Œëª¨, ëª¨ë¸ ë³µì¡ë„ ì¦ê°€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜: ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜, ë”¥ëŸ¬ë‹<br> 

<br>

---

# [8] ê¸°ìˆ  ë¶€ì±„ ê´€ë¦¬
## [8-1] ê¸°ìˆ  ë¶€ì±„(Technical Debt) ê´€ë¦¬
â–£ ì •ì˜ :  ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ê³¼ì •ì—ì„œ ë‹¨ê¸°ì ì¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë¹ ë¥´ê³  ë¹„íš¨ìœ¨ì ì¸ í•´ê²°ì±…ì„ ì„ íƒí•¨ìœ¼ë¡œì¨ ë°œìƒí•˜ëŠ” ë¯¸ë˜ì˜ ì¶”ê°€ ì‘ì—…<br>
â–£ í•„ìš”ì„± : ì¥ê¸°ì  ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ê°ì†Œ, ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥, ê¸°ìˆ  ìŠ¤íƒ ê°œì„ , íŒ€ ìƒì‚°ì„± í–¥ìƒ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¯¼ì²©ì„± í–¥ìƒ<br>
â–£ ì£¼ìš”ê¸°ë²• : ì½”ë“œ ë¦¬ë·°(Code Review), ë¦¬íŒ©í† ë§(Refactoring), ìë™í™” í…ŒìŠ¤íŠ¸(Automated Testing), CI/CD (Continuous Integration/Continuous Deployment), ê¸°ìˆ  ë¶€ì±„ ì¸¡ì • ë„êµ¬ ì‚¬ìš©, ë°ë¸Œì˜µìŠ¤(DevOps)ì™€ í˜‘ì—… ê°•í™”, ê¸°ìˆ  ë¶€ì±„ ëª©ë¡í™” (Debt Backlog), ì •ê¸°ì ì¸ ê¸°ìˆ  ìŠ¤íƒ ì—…ë°ì´íŠ¸<br>
â–£ ì¥ì  : ì¥ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ê°ì†Œ, ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ , ê°œë°œ ìƒì‚°ì„± í–¥ìƒ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¯¼ì²©ì„± ì¦ê°€, íŒ€ í˜‘ì—… ê°•í™”<br>
â–£ ë‹¨ì  : ì´ˆê¸° ì‹œê°„ê³¼ ë¹„ìš© ì¦ê°€, ìš°ì„ ìˆœìœ„ ì„¤ì •ì˜ ì–´ë ¤ì›€, ì™„ë²½í•œ ì œê±°ëŠ” ë¶ˆê°€ëŠ¥, ë‹¨ê¸°ì ì¸ ì†ë„ ì €í•˜, ì¸¡ì •ì˜ ì–´ë ¤ì›€<br>
â–£ ì ìš©ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜ : ë¨¸ì‹ ëŸ¬ë‹ ë° ë°ì´í„° íŒŒì´í”„ë¼ì¸, ì›¹ ê°œë°œ í”„ë ˆì„ì›Œí¬, ì•Œê³ ë¦¬ì¦˜ ìµœì í™”<br>

<br>

---
