#  09 : ì§€ë„ í•™ìŠµ (Supervised Learning, SL) : íšŒê·€ (regression)
**ì§€ë„ í•™ìŠµ**ì€ ì£¼ì–´ì§„ ì…ë ¥ê°’($X$)ì— ëŒ€í•˜ì—¬ ì‹ ë¢°ì„± ìˆëŠ” ì¶œë ¥ê°’($y$)ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼<br> 
í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°(í•™ìŠµ ë°ì´í„° $X$, $y$)ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ë‹¤.<br>
ìˆ˜ì‹ì„ ì´ìš©í•˜ì—¬ í‘œí˜„í•˜ë©´, í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” í•™ìŠµë°ì´í„° $(X, y)$ë¡œë¶€í„° $y = f(X)$ë¥¼ ë§Œì¡±í•˜ëŠ”<br> 
ì—¬ëŸ¬ í•¨ìˆ˜ $f$ì¤‘ì—ì„œ ê°€ì¥ ìµœì ì˜(ì£¼ì–´ì§„ Taskì— ë”°ë¼ ë‹¬ë¼ì§) $f$ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.<br>
ì¶œë ¥ ë³€ìˆ˜ $y$ê°€ ìµœì  í•¨ìˆ˜ $f$ë¥¼ ì°¾ë„ë¡ ì§€ë„í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤ê³  í•´ì„œ ì§€ë„ í•™ìŠµì´ë¼ê³  í•œë‹¤.<br>

ì§€ë„ í•™ìŠµì€ **íšŒê·€(Regression)** ì™€ **ë¶„ë¥˜(Classification)** ë¡œ êµ¬ë¶„ëœë‹¤.<br>
íšŒê·€ ëª¨ë¸ì€ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì—°ì†ì ì¸ ê°’ì„ ì¶œë ¥í•˜ê³ , ë¶„ë¥˜ ëª¨ë¸ì€ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì´ì‚°ì ì¸ ê°’ì„ ì¶œë ¥í•œë‹¤.<br> 

ì˜ˆë¥¼ ë“¤ì–´, ë„ë¯¸ì™€ ë¹™ì–´ì˜ ê¸¸ì´ì™€ ë¬´ê²Œ ë°ì´í„°ë¥¼ í†µí•´ ë„ë¯¸ ì—¬ë¶€ë¥¼ ì‹ë³„í•˜ëŠ” ê²ƒì€ ë¶„ë¥˜(ì¶œë ¥ë³€ìˆ˜ : ë²”ì£¼í˜•),<br> 
ë„ë¯¸ì˜ ê¸¸ì´ ë°ì´í„°ë¥¼ í†µí•´ ë„ë¯¸ì˜ ë¬´ê²Œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ íšŒê·€(ì¶œë ¥ë³€ìˆ˜ : ì—°ì†í˜•)ì´ë‹¤.<br>

---

	[1] ì„ í˜• íšŒê·€ (Linear Regression)
  
  	[2] ì¼ë°˜í™” ì„ í˜• íšŒê·€(Generalized Linear Regression, GLM)
   		[2-1] ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression) â†’ ë¶„ë¥˜(10ê°•)
		[2-2] í¬ì•„ì†¡ íšŒê·€ (Poisson Regression)
		[2-3] Coxì˜ ë¹„ë¡€ìœ„í—˜ íšŒê·€(Cox's Proportional Hazard Regression)
     
 	[3] ë‹¤ì¤‘ ì„ í˜• íšŒê·€ (Multiple Linear Regression)
		[3-1] ë‹¨ê³„ì  íšŒê·€ (Stepwise Regression), ìœ„ê³„ì  íšŒê·€ (Hierarchical Regression) 
		[3-2] ë¶„ìœ„ìˆ˜ íšŒê·€ (Quantile Regression)
  
	[4] ë‹¤í•­ ì„ í˜• íšŒê·€ (Polynomial Linear Regression)

   	[5] ì •ê·œí™” (Regularized), ë²Œì ë¶€ì—¬ (Penalized) ì„ í˜• íšŒê·€
		[5-1] ë¦¿ì§€ íšŒê·€ (Ridge Regression)
		[5-2] ë¼ì˜ íšŒê·€ (Lasso Regression)
		[5-3] ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ (Elastic Net Regression)

  	[6] ë¹„ì„ í˜• íšŒê·€ (nonlinear regression)

	[7] ì°¨ì›ì¶•ì†Œ
		[7-1] PCR(Principal Component Regression)
		[7-2] PLS(Partial Least Squares Regression)

---

<br>

# [1] ì„ í˜• íšŒê·€ (Linear Regression)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/linear_model.html#<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html<br>
â–£ ì˜ˆì œ : https://scikit-learn.org/stable/auto_examples/linear_model/index.html<br>
ì¢…ì†ë³€ìˆ˜ y(ì˜ˆìƒê°’)ê³¼ ë…ë¦½ë³€ìˆ˜(ì„¤ëª…ë³€ìˆ˜) Xì™€ì˜ ì„ í˜• ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ë¡œ<br>
íšŒê·€ ê³„ìˆ˜(regression coefficient)ë¥¼ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ëª¨ë¸<br>
'ì„ í˜•'ì€ ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ì˜ ê´€ê³„ê°€ 2ì°¨ì›ì—ì„œëŠ” ì„ í˜•ìœ¼ë¡œ, 3ì°¨ì› ê³µê°„ì—ì„œëŠ” í‰ë©´ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.<br> 

![](./images/RA.PNG)

<br>

![](./images/LinearRegression.gif)


![](./images/mb.png)

ì¶œì²˜ : https://savannahar68.medium.com/getting-started-with-regression-a39aca03b75f
<br>

---
ëª¨ë¸ì´ ë…ë¦½ë³€ìˆ˜ì™€ íšŒê·€ê³„ìˆ˜ì— ëŒ€í•˜ì—¬ ì„ í˜•ì¸ ê²½ìš°<br>
$y = mx + b$ <br>
$y = w_1x + w_0$ <br>
$y_i = Î²_1x_i + Î²_0 + Ïµ_i$<br>
###### $y_i$ : ië²ˆì§¸ ë°˜ì‘ë³€ìˆ˜ ê°’, $x_i$ : ië²ˆì§¸ ì„¤ëª…ë³€ìˆ˜ ê°’, $Î²_0$ : ì ˆí¸ íšŒê·€ê³„ìˆ˜, $Î²_1$ : ê¸°ìš¸ê¸° íšŒê·€ê³„ìˆ˜, $Ïµ_i$ : ië²ˆì§¸ ì¸¡ì •ëœ $y_i$ì˜ ì˜¤ì°¨ ì„±ë¶„<br>
ëª¨ë“  íšŒê·€ê³„ìˆ˜ ê°ê°ì— ëŒ€í•´ í¸ë¯¸ë¶„í•œ ê²°ê³¼ê°€ ë‹¤ë¥¸ íšŒê·€ê³„ìˆ˜ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë„ ì„ í˜•ëª¨í˜•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.<br>
![](./images/LRS.png)
â€‹â€‹
<br>

---
ì„ í˜•íšŒê·€ëŠ” í•™ìŠµì„ í†µí•´ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê´€ì¸¡ê°’ì¸ ì”ì°¨ ì œê³±ë“¤ì˜ í•©ì¸ <ins>**RSS(Residual Sum of Squares)**</ins>ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” íšŒê·€ê³„ìˆ˜($W_0$ê³¼ $W_1$)ë¥¼ ì°¾ëŠ” ê²ƒì´ í•µì‹¬.<br>
![](./images/rss.png)

<ins>**ìµœì†Œì œê³±ë²•(Ordinary Least Squares, OLS)**</ins> : í†µê³„í•™ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì´ê³  ì¤‘ìš”í•œ íšŒê·€ë¶„ì„ ë°©ë²•ìœ¼ë¡œ<br>
â€œë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ì§ì„ ì„ ì°¾ê¸° ìœ„í•´, ì˜¤ì°¨ ì œê³±í•©ì´ ìµœì†Œê°€ ë˜ë„ë¡ ì§ì„ ì˜ ê¸°ìš¸ê¸°ì™€ ì ˆí¸ì„ êµ¬í•˜ëŠ” ë°©ë²•â€<br>
<img width ='500' height = '400' src = 'https://github.com/YangGuiBee/ML/blob/main/TextBook-09/images/LRd.png'>

â€‹<br>

**ê²½ì‚¬í•˜ê°•ë²• (Gradient Decent)**
ë¹„ìš© í•¨ìˆ˜ fì˜ í•¨ìˆ«ê°’ì´ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ í•¨ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ ì¼ì • í¬ê¸°(í•™ìŠµëŸ‰)ë§Œí¼ ë”í•´ë‚˜ê°€ë©° fì˜ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ìµœì í™” ê¸°ë²•ì´ë‹¤.
ê¸°ìš¸ê¸° $Gradient(f)=âˆ‡f(x)=[ âˆ‚f(x_0)/âˆ‚x_0, âˆ‚f(x_1)/âˆ‚x_1,...,âˆ‚f(x_{Nâˆ’1}/âˆ‚x_{Nâˆ’1}]^T$
â€‹ë¯¸ë¶„ ê°€ëŠ¥í•œ Nê°œì˜ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ fë¥¼ ê° ì¶•ì— ëŒ€í•˜ì—¬ í¸ë¯¸ë¶„í•œ ê°’ìœ¼ë¡œ, ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì˜ ëª¨ë“  ì¶•ì— ëŒ€ì‘í•˜ëŠ” ë²¡í„°ì¥ì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•œë‹¤.
ì†ì‹¤ í•¨ìˆ˜ê°€ ì¡°ê¸ˆë§Œ ë³µì¡í•´ì ¸ë„ Global Minimumì„ ë°œê²¬í•˜ì§€ ëª»í•œ ì±„ Local Minimumì— ë¹ ì§€ê¸° ì‰½ê³  í•™ìŠµ ì‹œê°„ì´ ê¸¸ë‹¤.

![](./images/gradient_descent.gif)
<br>
ì˜¤ë¥˜ê°€ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ wê°’ì„ ë³´ì •í•  ìˆ˜ ìˆëŠ” í•´ë²•ì„ êµ¬í•˜ëŠ” ë°©ë²•<br>
(1) $W_1$, $W_0$ì„ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ê³  ì²« ë¹„ìš©í•¨ìˆ˜ì˜ ê°’ì„ ê³„ì‚°í•œë‹¤.<br>
(2) $W_1$, $W_0$ì˜ ê°’ì„ ì£¼ì–´ì§„ íšŸìˆ˜ë§Œí¼ ê³„ì† ì—…ë°ì´íŠ¸í•œë‹¤.<br>
$x_{i+1} = x_i - \alpha \frac{df}{dx}(x_i)$, $x_{i+1} = x_i - \alpha \nabla f(x_i)$<br>

![](./images/w1.svg) , ![](./images/w0.svg)

---

	# ì„ í˜•íšŒê·€ëª¨ë¸(LinearRegression) Scikit-Learn Package ì‚¬ìš©
 	from sklearn.linear_model import LinearRegression
	
 	# ì„ í˜•íšŒê·€ëª¨ë¸(LinearRegression) í´ë˜ìŠ¤ ê°ì²´ ìƒì„±
	lr = LinearRegression()
 
 	# ì„ í˜•íšŒê·€ëª¨ë¸(LinearRegression) í•™ìŠµ
	lr.fit(train_input, train_target)

	# í•™ìŠµê²°ê³¼ë¡œ ë„ì¶œí•œ ê°’ coef_ : ê¸°ìš¸ê¸°(w1), intercept_ : ì ˆí¸(w0)
	print(lr.coef_, lr.intercept_)

 	# ì„ í˜•íšŒê·€ëª¨ë¸(LinearRegression) í•™ìŠµê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ê°’ì— ëŒ€í•œ ì˜ˆì¸¡
	print(lr.predict(([50]))

---
<br>

# [2] ì¼ë°˜í™” ì„ í˜• íšŒê·€(Generalized Linear Regression, GLM)
ì¼ë°˜í™” ì„ í˜• íšŒê·€ì˜ ê²½ìš° ì„ í˜•ì„±, ë…ë¦½ì„±, ë“±ë¶„ì‚°ì„±, ì •ê·œì„±ì˜ ê°€ì •ì„ ê°–ê³  ìˆì§€ë§Œ, ì¢…ì†ë³€ìˆ˜ê°€ ì—°ì†í˜•ì´ ì•„ë‹ˆë¼ë©´ ëŒ€í‘œì ìœ¼ë¡œ ì˜¤ì°¨í•­ì˜ ì •ê·œì„± ê°€ì •ì´ ê¹¨ì§€ê²Œ ë˜ëŠ”ë°, ì¢…ì†ë³€ìˆ˜ë¥¼ ì ì ˆí•œ í•¨ìˆ˜ë¡œ ë³€í™”ì‹œí‚¨ f(y)ë¥¼ ë…ë¦½ë³€ìˆ˜ì™€ íšŒê·€ê³„ìˆ˜ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ëª¨í˜•í™”í•œ ê²ƒì´ë‹¤.<br>

# [2-1] ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression) â†’ ë¶„ë¥˜(10ê°•)

<br>

# [2-2] í¬ì•„ì†¡ íšŒê·€ (Poisson Regression)
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html<br>
ì¢…ì†ë³€ìˆ˜ê°€ í¬ì•„ì†¡ ë¶„í¬(Poisson Distribution)ë¥¼ ë”°ë¥´ëŠ” ê²½ìš°ì— ì‚¬ìš©ë˜ë©°, ì´ì‚°í˜• ì¹´ìš´íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë° ì í•©í•˜ë‹¤.<br> 
í¬ì•„ì†¡ ë¶„í¬ëŠ” ë‹¨ìœ„(í•œì •ëœ) ì‹œê°„ì´ë‚˜ ê³µê°„ì—ì„œ ë°œìƒí•˜ëŠ” í‰ê· ì ì¸ ì‚¬ê±´ì˜ íšŸìˆ˜(Î»)ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • íšŸìˆ˜ì˜ ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì„ ì„¤ëª…í•œë‹¤.<br> 
ì¢…ì†ë³€ìˆ˜ê°€ ë¹ˆë„ë³€ìˆ˜ë¡œ 0ì´ìƒ ì •ìˆ˜ì´ê±°ë‚˜, ì™œë„ê°€ í¬ê±°ë‚˜, ë¶„í¬ìœ í˜•ì´ í¬ì•„ì†¡ ë¡œê·¸ì„ í˜•ì¼ ê²½ìš°ì— ì‹¤ì‹œí•œë‹¤.<br>
ì°¸ê³ ë¡œ í‰ê· ë³´ë‹¤ ë¶„ì‚°ì´ í° ê²½ìš°ì— ì ìš©í•˜ëŠ” **ìŒì´í•­ íšŒê·€(Negative binomial regression)** ëŠ” ë¶„ì‚°ì´ í¬ì•„ì†¡ ëª¨ë¸ì˜ í‰ê· ê³¼ ë™ì¼í•˜ë‹¤ëŠ” ë§¤ìš° ì œí•œì ì¸ ê°€ì •ì„ ì™„í™”í•  ìˆ˜ ìˆë‹¤.
ì „í†µì ì¸ ìŒì´í•­ íšŒê·€ ëª¨ë¸ì€ í¬ì•„ì†¡ê³¼ **ê°ë§ˆ(gamma regression)** í˜¼í•© ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ë„ë¦¬ ì‚¬ìš©ëœë‹¤.<br><br>
**í¬ì•„ì†¡ í™•ë¥ ë³€ìˆ˜ $X$ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜(probability mass function)** : $P(X = k; \lambda) = \frac{e^{-\lambda}\lambda^k}{k!}$<br>
###### X : ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ” íšŸìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥  ë³€ìˆ˜, ğ‘˜ : ë°œìƒí•œ ì‚¬ê±´ì˜ íšŸìˆ˜(0, 1, 2, 3, ...), ğœ† : ë‹¨ìœ„ ì‹œê°„ ë˜ëŠ” ê³µê°„ ë‚´ì—ì„œ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ” í‰ê·  íšŸìˆ˜(í¬ì•„ì†¡ ë¶„í¬ì˜ ëª¨ìˆ˜, í‰ê· ì´ì ë¶„ì‚°ìœ¼ë¡œ Î»ê°€ ì‘ì„ìˆ˜ë¡ ì‚¬ê±´ì´ ë“œë¬¼ê²Œ ë°œìƒí•˜ëŠ” ìƒí™©ì„ ë‚˜íƒ€ë‚´ë©°, Î»ê°€ í´ìˆ˜ë¡ ì‚¬ê±´ì´ ìì£¼ ë°œìƒí•˜ëŠ” ìƒí™©), ğ‘’ : ìì—° ìƒìˆ˜ â‰ˆ2.718, ğ‘˜! : kì˜ íŒ©í† ë¦¬ì–¼ë¡œ, ğ‘˜Ã—(ğ‘˜âˆ’1)Ã—â‹¯Ã—1<br>

**í¬ì•„ì†¡ íšŒê·€ ì ìš© ì‚¬ë¡€ :** ì¼ì • ì£¼ì–´ì§„ ì‹œê°„ ë™ì•ˆì— ë°©ë¬¸í•˜ëŠ” ê³ ê°ì˜ ìˆ˜, ì¼ì • ì£¼ì–´ì§„ ìƒì‚°ì‹œê°„ ë™ì•ˆ ë°œìƒí•˜ëŠ” ë¶ˆëŸ‰ ìˆ˜, í•˜ë£»ë™ì•ˆ ë°œìƒí•˜ëŠ” ì¶œìƒì ìˆ˜, ì–´ë–¤ ì‹œê°„ ë™ì•ˆ í†¨ê²Œì´íŠ¸ë¥¼ í†µê³¼í•˜ëŠ” ì°¨ëŸ‰ì˜ ìˆ˜, ì–´ë–¤ í˜ì´ì§€ì— ìˆëŠ” ì˜¤íƒ€ì˜ ë°œìƒë¥ , ì–´ë–¤ íŠ¹ì • ë©´ì ì˜ ì‚¼ë¦¼ì—ì„œ ìë¼ëŠ” ì†Œë‚˜ë¬´ì˜ ìˆ˜<br>

<br>

	# numpy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ìˆ˜ì¹˜ ê³„ì‚°ì— ìœ ìš©í•œ í•¨ìˆ˜ ì œê³µ)
	import numpy as np                      
	# seaborn ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ë°ì´í„° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬)
	import seaborn as sns                
	# matplotlibì˜ pyplot ëª¨ë“ˆ ì„í¬íŠ¸ (ê·¸ë˜í”„ ê·¸ë¦¬ê¸°ì— ì‚¬ìš©)
	import matplotlib.pyplot as plt         
	# scipy.statsì—ì„œ poisson ëª¨ë“ˆ ì„í¬íŠ¸ (í¬ì•„ì†¡ ë¶„í¬ ê´€ë ¨ í•¨ìˆ˜ ì œê³µ)
	from scipy.stats import poisson          
	# scipy.specialì—ì„œ factorial ëª¨ë“ˆ ì„í¬íŠ¸ (íŒ©í† ë¦¬ì–¼ ê³„ì‚°ì„ ìœ„í•œ í•¨ìˆ˜ ì œê³µ)
	from scipy.special import factorial      
	
	# í‰ê· ì´ 1ì¸ í¬ì•„ì†¡ ë¶„í¬ì—ì„œ 10ê°œì˜ ëœë¤ ìƒ˜í”Œì„ ìƒì„± (ì´ ì½”ë“œëŠ” ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
	poisson.rvs(mu=1, size=10)               

	# seabornì˜ "BrBG" ìƒ‰ìƒ íŒ”ë ˆíŠ¸ì—ì„œ 6ê°œì˜ ìƒ‰ì„ ì„ íƒ (ê·¸ë˜í”„ ìƒ‰ìƒì— ì‚¬ìš©)
	pal_brbg = sns.color_palette("BrBG", 6)  
	
	# 0ë¶€í„° 10ê¹Œì§€ì˜ ì •ìˆ˜ ë°°ì—´ ìƒì„± (xì¶• ê°’, ì¦‰ í¬ì•„ì†¡ ë¶„í¬ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ì‚¬ê±´ì˜ ìˆ˜)
	x = np.arange(0, 11)                     

	# Î» ê°’ì„ 1ë¶€í„° 5ê¹Œì§€ ë°˜ë³µí•˜ì—¬ ê°ê°ì˜ í¬ì•„ì†¡ ë¶„í¬ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¼
	for n_lambda in range(1, 6):             

    		# í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥  ê³„ì‚°: P(x; Î») = (e^(-Î») * Î»^x) / x!
    		y = np.exp(-n_lambda) * np.power(n_lambda, x) / factorial(x)  

    		# ê³„ì‚°ëœ í™•ë¥  yê°’ì„ xê°’ì— ëŒ€í•´ ì„  ê·¸ë˜í”„ë¡œ ê·¸ë¦¼, ê°ê° ë‹¤ë¥¸ ìƒ‰ ì‚¬ìš©
    		plt.plot(x, y, color=pal_brbg[n_lambda - 1], label=f"Î» = {n_lambda}")  

    		# í•´ë‹¹ Î»ì— ëŒ€í•œ í™•ë¥  ê°’ì„ ì ìœ¼ë¡œ í‘œì‹œ
    		plt.scatter(x, y, color=pal_brbg[n_lambda - 1])  

	# yì¶• ë¼ë²¨ ì„¤ì • (í™•ë¥ )
	plt.ylabel("Probability")                
	# ê·¸ë˜í”„ ì œëª© ì„¤ì • (Î» ê°’ì˜ ë²”ìœ„ ëª…ì‹œ)
	plt.title(f"Poisson Distribution (Î» = [1, 5])")  
	# xì¶•ì— 0ë¶€í„° 10ê¹Œì§€ì˜ ê°’ í‘œì‹œ
	plt.xticks(x)                            
	# yì¶•ì— ì ì„  ìŠ¤íƒ€ì¼ì˜ íšŒìƒ‰ ê·¸ë¦¬ë“œ ì¶”ê°€ (ê°€ë…ì„± í–¥ìƒ)
	plt.grid(axis="y", linestyle="--", color="#CCCCCC")  
	# ê·¸ë˜í”„ì˜ ë²”ë¡€ë¥¼ ì˜¤ë¥¸ìª½ ìƒë‹¨ì— í‘œì‹œ
	plt.legend(loc="upper right")            
	# ê·¸ë˜í”„ë¥¼ í™”ë©´ì— ì¶œë ¥
	plt.show()                               


<br>

# [2-3] Coxì˜ ë¹„ë¡€ìœ„í—˜ íšŒê·€(Cox's Proportional Hazard Regression)
Coxì˜ ë¹„ë¡€ìœ„í—˜ íšŒê·€ëŠ” ìƒì¡´ ë¶„ì„(survival analysis)ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” íšŒê·€ ëª¨ë¸ì´ë‹¤. ì–´ë–¤ ì‚¬ê±´(event)ì´ ì¼ì–´ë‚  ë•Œê¹Œì§€ì˜ ì‹œê°„ì„ ëŒ€ìƒìœ¼ë¡œ ë¶„ì„í•˜ëŠ” í†µê³„ë°©ë²•ìœ¼ë¡œ ì‚¬ê±´ê³¼ ì‚¬ê±´ ì‚¬ì´ì˜ ì˜ˆì¸¡ íšŒê·€ ëª¨í˜•ì„ ë¶„ì„í•œë‹¤. ì´ ëª¨ë¸ì€ ì‚¬ê±´(ì˜ˆ: ì‚¬ë§, ì§ˆë³‘ ë°œë³‘, ê¸°ê³„ ê³ ì¥ ë“±)ì´ ë°œìƒí•  ë•Œê¹Œì§€ì˜ ì‹œê°„ê³¼ ê·¸ ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ (ìœ„í—˜ìœ¨) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•œë‹¤. ì£¼ì–´ì§„ ë…ë¦½ë³€ìˆ˜ ê°’ì— ëŒ€í•´ ìœ„í—˜ìœ¨($hazard ratio(log(h(t)/h_0(t)))$)ì´ ì‹œê°„ì— ê±¸ì³ ì¼ì •í•œ ë¹„ìœ¨ë¡œ ìœ ì§€(ë‘ í”¼í—˜ìì— ëŒ€í•´ ìœ„í—˜ìœ¨ì˜ ë¹„ìœ¨ì´ ì‹œê°„ì´ ì§€ë‚˜ë„ ì¼ì •í•˜ê²Œ ìœ ì§€)ëœë‹¤ê³  ê°€ì •í•œë‹¤. ìœ„í—˜ìœ¨(HR)ì´ 1ë³´ë‹¤ í¬ë©´ ìœ„í—˜ì´ ì¦ê°€í•˜ê³ , 1ë³´ë‹¤ ì‘ìœ¼ë©´ ìœ„í—˜ì´ ê°ì†Œí•˜ëŠ” ê²ƒìœ¼ë¡œ í‰ê°€í•œë‹¤. í™˜ìê°€ íŠ¹ì • ì¹˜ë£Œ í›„ ìƒì¡´í•  í™•ë¥ ì„ ì˜ˆì¸¡, ê¸°ê³„ ë¶€í’ˆì´ ê³ ì¥ë‚  ë•Œê¹Œì§€ì˜ ì‹œê°„ì„ ë¶„ì„, ì‚¬íšŒí•™ ì—°êµ¬ì—ì„œ ê²°í˜¼ìƒí™œì´ íŒŒíƒ„ë‚  í™•ë¥ ì„ ì˜ˆì¸¡í•  ë•Œ í™œìš©í•œë‹¤. ë§Œì•½ ë¹„ë¡€ ìœ„í—˜ ê°€ì •ì´ ë§Œì¡±ë˜ì§€ ì•Šìœ¼ë©´ Cox íšŒê·€ ëª¨ë¸ì˜ ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ ê²½ìš°ì—ëŠ” ë¹„ë¡€ ìœ„í—˜ ê°€ì •ì„ ê²€í† í•˜ê±°ë‚˜ ì‹œê°„ì„ ê³ ë ¤í•œ ìƒí˜¸ì‘ìš© ë³€ìˆ˜ë¥¼ ì¶”ê°€í•´ì•¼ í•œë‹¤.<br>
<br>

| êµ¬ë¶„   | í¬ì•„ì†¡ íšŒê·€                                  | Cox íšŒê·€                                                   |
|--------|----------------------------------------------|------------------------------------------------------------|
| ëª©ì    | ì‚¬ê±´ ë°œìƒ íšŸìˆ˜ ì˜ˆì¸¡                          | ì‚¬ê±´ì´ ë°œìƒí•  ë•Œê¹Œì§€ì˜ ì‹œê°„ê³¼ ê·¸ ì‚¬ê±´ì˜ ìœ„í—˜ìœ¨ì„ ë¶„ì„      |
| ë°ì´í„° | ì£¼ë¡œ ì´ì‚°í˜•(ì •ìˆ˜)                            | ìƒì¡´ì‹œê°„ê³¼ ê°™ì€ ì—°ì†í˜•                                     |
| ê°€ì •   | í¬ì•„ì†¡ ë¶„í¬ì™€ ë¡œê·¸ ë§í¬ í•¨ìˆ˜                 | ë¹„ë¡€ìœ„í—˜                                                   |
| ì‚¬ë¡€   | ë²”ì£„ìœ¨, ì§ˆë³‘ ë°œìƒë¥  ë“± ì‚¬ê±´ ë°œìƒ íšŸìˆ˜ì˜ ì˜ˆì¸¡ | í™˜ìì˜ ìƒì¡´ìœ¨, ë¶€í’ˆì˜ ê³ ì¥ ì‹œê°„ ë“± ìƒì¡´ ë¶„ì„ê³¼ ê´€ë ¨ëœ ë¬¸ì œ |

<br>

---
# [3] ë‹¤ì¤‘íšŒê·€ (Multiple Regression)
ë…ë¦½ë³€ìˆ˜ Xê°€ 2ê°œ ì´ìƒì¸ íšŒê·€<br>
$y = w_1x_1 + w_2x_2 + ... + w_nx_n + w_0$ <br>
$y_i = Î²_0 + Î²_1x_{i1} + Î²_2x_{i2} + ... + Î²_kx_{ik} + Ïµ_i$<br>
$y_i$ : ië²ˆì§¸ ê´€ì¸¡ì¹˜, $Ïµ_i$ : ì´ë•Œì˜ ì˜¤ì°¨í•­, $x_{ij}$ : ë…ë¦½ë³€ìˆ˜ë¡œ known value<br>
$Î²_j$ : ì¶”ì •í•˜ê³ ìí•˜ëŠ” ê°’ì¸ íšŒê·€ê³„ìˆ˜ë¡œ $0â‰¤jâ‰¤k$ ì‚¬ì´ì˜ ê°’<br>
Nê°œì˜ ìƒ˜í”Œì— ëŒ€í•˜ì—¬ í™•ì¥í•œ í›„, vector-matrix í˜•íƒœë¡œ í‘œê¸°í•˜ë©´,<br>
<img width ='500' src = 'https://github.com/YangGuiBee/ML/blob/main/TextBook-04/images/vectorMX.png'><br>
$eâˆ¼N(0,Ïƒ^2I_N)$<br>

	import pandas as pd
	import matplotlib.pyplot as plt 
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LinearRegression

 	# ë°ì´í„° ìˆ˜ì§‘
	df = pd.read_csv('https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-04/manhattan.csv')
 	# ë°ì´í„° ì „ì²˜ë¦¬(nullê²‚ì´ ë§ì€ í•­ëª© ì‚­ì œ)
	df = df.drop(['neighborhood','borough','rental_id'], axis=1)
	
	X = df [['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor',
       	'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer',
	'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio','has_gym']]
	y = df [['rent']]       
 
 	# ë°ì´í„° êµ¬ë¶„ (í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° 8:2)
	X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8,test_size=0.2)
 	# ì„ í˜•íšŒê·€ëª¨ë¸ ê°ì²´ ìƒì„±
	mlr = LinearRegression()
 	# í•™ìŠµ
	mlr.fit(X_train, y_train)	
  	# í‰ê°€
 	print(mlr.score(X_train, y_train))
 	# ì˜ˆì¸¡
	y_predict = mlr.predict(X_test)

 	# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
	plt.scatter(y_test,y_predict,alpha=0.4)
	plt.xlabel('Actual Rent')
	plt.ylabel('Predicted Rent')
	plt.title('Multiple Linear Regression')
	plt.show()

<br>

# [3-1] ë‹¨ê³„ì  íšŒê·€ (Stepwise Regression), ìœ„ê³„ì  íšŒê·€ (Hierarchical Regression) 
ì—¬ëŸ¬ ë…ë¦½ë³€ìˆ˜ ì¤‘ì—ì„œ ì¢…ì†ë³€ìˆ˜ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ë³€ìˆ˜ë“¤ì„ ì„ íƒí•˜ëŠ” ë°©ë²•<br>
**ë‹¨ê³„ì  íšŒê·€ (Stepwise Regression)** ëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì„ ìë™ìœ¼ë¡œ ëª¨ë¸ì— ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ íƒìƒ‰(ë³€ìˆ˜ì˜ ì¶”ê°€ë‚˜ ì œê±°ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ ì—¬ë¶€ì— ë”°ë¼ ì´ë£¨ì–´ì§)<br>
ì˜ˆë¥¼ ë“¤ì–´, ë³€ìˆ˜ë¥¼ ì¶”ê°€í•  ë•Œë§ˆë‹¤ F í†µê³„ëŸ‰ì´ìœ ì˜ë¯¸í•˜ê²Œ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜, ì œê±°í•  ë•Œë§ˆë‹¤ ë³€ìˆ˜ì˜ t í†µê³„ëŸ‰ì´ ìœ ì˜ë¯¸í•˜ê²Œ ê°ì†Œí•˜ëŠ”ì§€ í™•ì¸<br> 
ì¥ì : ìë™ìœ¼ë¡œ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ë¯€ë¡œ ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ë§ì„ ê°€ëŠ¥ì„±ì´ ìˆìŒ<br>
**ìœ„ê³„ì  íšŒê·€ (Hierarchical Regression)** ëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì„ ë¯¸ë¦¬ ì •ì˜í•œ ìˆœì„œì— ë”°ë¼ ëª¨ë¸ì— ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ,<br>
ì´ë¡ ì ìœ¼ë¡œ ì¤‘ìš”í•œ ë³€ìˆ˜ë¶€í„° ì‹œì‘í•˜ì—¬ ëœ ì¤‘ìš”í•œ ë³€ìˆ˜ë¥¼ ì°¨ë¡€ë¡œ ì¶”ê°€í•˜ëŠ” ë°©ì‹<br>
ì¥ì : ì´ë¡ ì  ê·¼ê±°ì— ë”°ë¼ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ ê²°ê³¼ í•´ì„ì´ ì´ë¡ ì ìœ¼ë¡œ íƒ€ë‹¹í•¨.<br>

<br>

# [3-2] ë¶„ìœ„ìˆ˜ íšŒê·€ (Quantile Regression)
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html<br>
ë°˜ì‘ ë³€ìˆ˜ì˜ ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ë¥¼ ëª¨ë¸ë§ í• ë•Œ ì‚¬ìš©ë˜ëŠ” ì„ í˜• íšŒê·€ì˜ í™•ì¥ ë²„ì „<br>
1) ì„ í˜• íšŒê·€ ì¡°ê±´ì´ ì¶©ì¡±ë˜ì§€ ì•ŠëŠ” ê²½ìš°<br>
2) ì˜¤ì°¨ì˜ ë¶„ì‚°ì´ í° ê²½ìš°<br>
3) Robustí•œ ê²°ê³¼ë¥¼ ìœ„í•˜ì—¬<br>
4) ë§ì€ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•˜ì—¬<br>
5) ì  ì¶”ì •ì´ ì•„ë‹Œ êµ¬ê°„ì¶”ì •ì„ í†µí•´ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•˜ì—¬<br>
6) ë°˜ì‘ë³€ìˆ˜ì˜ ìŠ¤í”„ë ˆë“œë¥¼ ê°™ì´ ì‚´í´ë³´ê¸° ìœ„í•˜ì—¬<br>
7) íšŒê·€ê³¡ì„ ì— ëŒ€í•œ ì„¤ë“ë ¥ì„ ë†’ì´ê¸° ìœ„í•˜ì—¬<br>

<br>
ë³´í†µ OLS íšŒê·€ëŠ” ì¡°ê±´ë¶€ í‰ê· ê°’ì„ ëª¨ë¸ë§í•˜ëŠ” ë°˜ë©´ ë¶„ìœ„ìˆ˜ íšŒê·€ëŠ” ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ë¥¼ ëª¨ë¸ë§í•˜ê³ <br>
ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ë¥¼ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ Pinball lossë¥¼ ì‚¬ìš©<br>
ê¸°ì¡´ì˜ ì¡°ê±´ë¶€ í‰ê·  ê°’ ì˜ˆì¸¡ì´ ì•„ë‹Œ ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¡œ í’€ì´ ë  ìˆ˜ ìˆë‹¤.<br>

$Q_{\tau}(y_{i}) = \beta_{0}(\tau) + \beta_{1}(\tau)x_{i1} + \cdots + \beta_{p}(\tau)x_{ip}$<br>

ìµœì ì˜ ë¶„ìœ„ìˆ˜ ë°©ì •ì‹ì„ ì°¾ê¸° ìœ„í•œ ê³¼ì •ì€ ì¤‘ìœ„ìˆ˜ì ˆëŒ€í¸ì°¨ì¸ MAD(Median Absolute Deviation) ê°’ì„ ìµœì†Œí™”í•¨ìœ¼ë¡œì¨ ì°¾ì„ ìˆ˜ ìˆë‹¤.<br>
$MAD = \frac{1}{n} \sum_{i=1}^{n} \rho_{\tau}(y_{i} - (\beta_{0}(\tau) + \beta_{1}(\tau)x_{i1} +\cdots +\beta_{p}(\tau)x_{ip}))$<br>
 
Ïí•¨ìˆ˜ëŠ” ì˜¤ì°¨ì˜ ë¶„ìœ„ìˆ˜ì™€ ì „ì²´ì ì¸ ë¶€í˜¸ì— ë”°ë¼ ì˜¤ì°¨ì— ë¹„ëŒ€ì¹­ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì²´í¬ í•¨ìˆ˜<br>
$\rho_{\tau}(u) = \tau\max(u,0) + (1-\tau)\max(-u,0)$<br>
<br>

	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt
	# statsmodelsì˜ formula APIì—ì„œ Quantile Regression í•¨ìˆ˜ ì„í¬íŠ¸
	import statsmodels.formula.api as smf
	# sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ make_regression í•¨ìˆ˜ ì„í¬íŠ¸ (íšŒê·€ìš© ë°ì´í„° ìƒì„±ì— ì‚¬ìš©)
	from sklearn.datasets import make_regression
	from sklearn.model_selection import train_test_split
	# MSE í‰ê°€ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
	from sklearn.metrics import mean_absolute_error

	# ê°€ìƒì˜ íšŒê·€ìš© ë°ì´í„°ë¥¼ ìƒì„± (10000ê°œì˜ ìƒ˜í”Œ, 1ê°œì˜ íŠ¹ì„±, 1ê°œì˜ íƒ€ê²Ÿ ë³€ìˆ˜)
	x, y = make_regression(n_samples=10000, n_features=1, n_informative=1, n_targets=1, random_state=42)

	# ìƒì„±ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
	df = pd.DataFrame([x.reshape(-1), y.reshape(-1)]).T

	# ì»¬ëŸ¼ ì´ë¦„ì„ 'distance'ì™€ 'time'ìœ¼ë¡œ ì„¤ì •
	df.columns = ['distance', 'time']

	# 'distance' ì»¬ëŸ¼ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ë³€í˜•
	df['distance'] = df['distance'].apply(lambda x: 10 + (x + np.random.normal()))

	# 'time' ì»¬ëŸ¼ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ë³€í˜• (ê¸°ìš¸ê¸°ê°€ 0.2ì¸ ì„ í˜• ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨)
	df['time'] = df['time'].apply(lambda x: 40 + 0.2 * (x + np.random.normal()))

	# ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ” (90%ëŠ” í›ˆë ¨, 10%ëŠ” í…ŒìŠ¤íŠ¸)
	train_x, test_x, train_y, test_y = train_test_split(df[['distance']], df[['time']], test_size=0.1, random_state=42)

	# í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í¬ê¸° ì¶œë ¥
	print(train_x.shape)
	print(train_y.shape)
	print(test_x.shape)
	print(test_y.shape)

	# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì™€ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
	model_list = []
	pred_dict = {}

	# 0.1, 0.5, 0.9 ë¶„ìœ„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Quantile Regression ëª¨ë¸ì„ í›ˆë ¨ ë° ì˜ˆì¸¡
	# 0.1 ë¶„ìœ„ìˆ˜ : í•˜ìœ„ 10% ì§€ì , 0.5 ë¶„ìœ„ìˆ˜ëŠ” ì¤‘ì•™ê°’(ì¤‘ìœ„ìˆ˜)ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì˜ ì¤‘ê°„ ì§€ì , 0.9 ë¶„ìœ„ìˆ˜ : ìƒìœ„ 90% ì§€ì ì— í•´ë‹¹í•˜ëŠ” ê°’
	for quantile in [0.1, 0.5, 0.9]:
  		# í›ˆë ¨ ë°ì´í„°(ê±°ë¦¬ì™€ ì‹œê°„)ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©í•˜ì—¬ ì´ˆê¸°í™”
  		df = pd.concat([train_x, train_y], axis=1).reset_index(drop=True)

  		# ë¶„ìœ„ìˆ˜ íšŒê·€(Quantile Regression)ë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ í”¼íŒ…
  		quantile_reg = smf.quantreg('time ~ distance', df).fit(q=quantile)

  		# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
  		pred = quantile_reg.predict(test_x)

  		# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¶„ìœ„ìˆ˜ë³„ë¡œ ì €ì¥
  		pred_dict[quantile] = pred

	# í…ŒìŠ¤íŠ¸ ë°ì´í„°, ì˜ˆì¸¡ ê²°ê³¼, ì‹¤ì œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
	pred_df = pd.concat([test_x.reset_index(drop=True), pd.DataFrame(pred_dict).reset_index(drop=True), test_y.reset_index(drop=True)], axis=1)

	# ì»¬ëŸ¼ëª… ì¶”ê°€: distance, 0.1 ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ê°’, 0.5 ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ê°’, 0.9 ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ê°’, ì‹¤ì œê°’(time)
	pred_df.columns = ['distance', 'pred_0.1', 'pred_0.5', 'pred_0.9', 'actual']

	# í‰ê°€ ê²°ê³¼(MAE)ë¥¼ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ ì¶”ê°€ : í‰ê°€ ê²°ê³¼ëŠ” 0.1, 0.5, 0.9 ë¶„ìœ„ìˆ˜ ê°ê°ì— ëŒ€í•´ ì¶œë ¥ë¨
	for quantile in [0.1, 0.5, 0.9]:
    		mae = mean_absolute_error(pred_df['actual'], pred_df[f'pred_{quantile}'])
    		print(f'Mean Absolute Error (MAE) for quantile {quantile}: {mae:.4f}')


<br>

---
# [4] ë‹¤í•­ íšŒê·€ (Polynomial Regression)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions<br>
ì§ì„ ì´ ì•„ë‹Œ ê³¡ì„  í˜•íƒœì˜ ê´€ê³„ì˜ ê²½ìš°, ë…ë¦½ë³€ìˆ˜ì— ì œê³±ì´ë‚˜ ë¡œê·¸(log) ë“±ì„ ì·¨í•´ ë³´ë©´ì„œ ì‹¤ì‹œí•˜ëŠ” ëª¨ë¸ë§<br>
$y = w_1x_1 + w_2x_2^2 + ... + w_nx_n^n + w_0$ <br>
<br>
![](./images/PolynomialFeatures.png)

<br>
í¸í–¥ì´ ë†’ìœ¼ë©´ ë¶„ì‚°ì€ ë‚®ì•„ì§ : ê³¼ì†Œì í•©(Under fitting), ë¶„ì‚°ì´ ë†’ìœ¼ë©´ í¸í–¥ì´ ë‚®ì•„ì§ : ê³¼ëŒ€ì í•©(Over fitting)<br>
  
![](./images/ddd.PNG)



	import pandas as pd
	import numpy as np
	import matplotlib.pyplot as plt 
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import r2_score
	
	df = pd.read_csv('https://raw.githubusercontent.com/YangGuiBee/ML/main/TextBook-04/housing.data.txt',
                 header=None, sep='\s+')

	df.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']
	df.head()
	
	X = df[['LSTAT']].values
	y = df['MEDV'].values
	
	regr = LinearRegression()

	# ì´ì°¨, ì‚¼ì°¨ ë‹¤í•­ì‹ íŠ¹ì„±ì„ ë§Œë“­ë‹ˆë‹¤
	quadratic = PolynomialFeatures(degree=2)
	cubic = PolynomialFeatures(degree=3)
	X_quad = quadratic.fit_transform(X)
	X_cubic = cubic.fit_transform(X)

	# í•™ìŠµëœ ëª¨ë¸ì„ ê·¸ë¦¬ê¸° ìœ„í•´ íŠ¹ì„± ë²”ìœ„ë¥¼ ë§Œë“­ë‹ˆë‹¤
	X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]
	
	regr = regr.fit(X, y)
	y_lin_fit = regr.predict(X_fit)
	linear_r2 = r2_score(y, regr.predict(X))
	
	regr = regr.fit(X_quad, y)
	y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))
	quadratic_r2 = r2_score(y, regr.predict(X_quad))
	
	regr = regr.fit(X_cubic, y)
	y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))
	cubic_r2 = r2_score(y, regr.predict(X_cubic))
		
	# ê²°ê³¼ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤
	plt.scatter(X,y,label='Training points', color='lightgray')
	plt.plot(X_fit,y_lin_fit,label='Linear(d=1),$R^2=%.2f$' % linear_r2,color='blue',lw=2,linestyle=':')
	plt.plot(X_fit,y_quad_fit,label='Quadratic(d=2),$R^2=%.2f$' % quadratic_r2,color='red',lw=2,linestyle='-')
	plt.plot(X_fit,y_cubic_fit,label='Cubic(d=3),$R^2=%.2f$' % cubic_r2,color='green',lw=2,linestyle='--')
	plt.xlabel('% lower status of the population [LSTAT]')
	plt.ylabel('Price in $1000s [MEDV]')
	plt.legend(loc='upper right')
	plt.show()

<br>


	ì…ë ¥ë°ì´í„°ì˜ Featureë“¤ì´ ë„ˆë¬´ ë§ì€ ê²½ìš°(Featureìˆ˜ì— ë¹„í•´ ê´€ì¸¡ì¹˜ ìˆ˜ê°€ ì ì€ ê²½ìš°) ê³¼ì í•©ì´ ë°œìƒ
	â†’ 
	(í•´ê²°ë°©ì•ˆ1) ë°ì´í„°ë¥¼ ë” ìˆ˜ì§‘í•˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ Featuresë“¤ì„ ì œê±°
	(í•´ê²°ë°©ì•ˆ2) ê°€ì¤‘ì¹˜(íšŒê·€ê³„ìˆ˜)ì— í˜ë„í‹° ê°’ì„ ì ìš©í•˜ëŠ” ê·œì œ(Regularization)ë¥¼ í†µí•´ 
 	            Featureë“¤ì— ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜ê°€ ì»¤ì§€ì§€ ì•Šë„ë¡ ì œí•œ

<br>

---
# [5] ì •ê·œí™” (Regularized), ë²Œì ë¶€ì—¬ (Penalized) ì„ í˜• íšŒê·€
ê·œì œ(Regularization) : ë¹„ìš©í•¨ìˆ˜ì— alphaê°’ìœ¼ë¡œ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•´ì„œ íšŒê·€ê³„ìˆ˜ê°’ì˜ í¬ê¸°ë¥¼ ê°ì†Œì‹œì¼œì„œ ê³¼ì í•©ì„ ê°œì„ <br>
ë¹„ìš©í•¨ìˆ˜ì˜ ëª©í‘œ = $Min(RSS(W) + alpha * ||W||_2^2)$

# [5-1] ë¦¿ì§€ íšŒê·€ (Ridge Regression)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html<br>
ìµœì†Œì œê³±ì¶”ì •ì¹˜(OLS)ê°€ í¸í–¥ë˜ì§€ ì•Šë”ë¼ë„ ë¶„ì‚°ì´ ì»¤ì„œ ê´€ì¸¡ê°’ì´ ì‹¤ì œê°’ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity)ì´ ë°œìƒí•  ê²½ìš°, íšŒê·€ ë¶„ì„ ì¶”ì •ì¹˜ì— ì¹˜ìš°ì¹¨ ì •ë„ë¥¼ ì¶”ê°€í•˜ì—¬ í‘œì¤€ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©<br>
ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì— ê¸°ì—¬í•˜ì§€ ëª»í•˜ëŠ” ë…ë¦½ë³€ìˆ˜ì˜ íšŒê·€ê³„ìˆ˜ í¬ê¸°ë¥¼ 0ì— ê·¼ì ‘í•˜ë„ë¡ ì¶•ì†Œì‹œí‚¤ëŠ” íšŒê·€<br>
L2-norm í˜ë„í‹°í•­ì„ í†µí•´ ì¼ë°˜ ì„ í˜•íšŒê·€ ëª¨ë¸ì— í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ íšŒê·€ê³„ìˆ˜ë¥¼ ì¶•ì†Œ<br>
(L2 norm : ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì˜¤ì°¨ì˜ ì œê³±ì˜ í•©)<br>

	from sklearn.linear_model import Ridge
	from sklearn.metrics import mean_squared_error
	from sklearn.metrics import r2_score

	ridge = Ridge(alpha=1.0)
	ridge.fit(X_train, y_train)
	y_train_pred = ridge.predict(X_train)
	y_test_pred = ridge.predict(X_test)
	print(ridge.coef_)
	
	print('í›ˆë ¨ MSE: %.3f, í…ŒìŠ¤íŠ¸ MSE: %.3f' % (mean_squared_error(y_train, y_train_pred),mean_squared_error(y_test, y_test_pred)))
	print('í›ˆë ¨ R^2: %.3f, í…ŒìŠ¤íŠ¸ R^2: %.3f' % (r2_score(y_train, y_train_pred),r2_score(y_test, y_test_pred)))

<br>

# [5-2] ë¼ì˜ íšŒê·€ (Lasso Regression)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/linear_model.html#lasso<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html<br>
ë¦¿ì§€íšŒê·€ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ ì„¤ëª…ë ¥ì— ê¸°ì—¬í•˜ì§€ ëª»í•˜ëŠ” ë…ë¦½ë³€ìˆ˜ì˜ íšŒê·€ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“œëŠ” íšŒê·€<br>
L1-norm íŒ¨ë„í‹°í•­ìœ¼ë¡œ íšŒê·€ëª¨ë¸ì— íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•¨ìœ¼ë¡œì¨ íšŒê·€ê³„ìˆ˜ë¥¼ ì¶•ì†Œ<br>
(L1 norm : ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì˜¤ì°¨ì˜ ì ˆëŒ€ê°’ì˜ í•©)<br>

	from sklearn.linear_model import Lasso
	from sklearn.metrics import mean_squared_error
	from sklearn.metrics import r2_score
	
	lasso = Lasso(alpha=0.1)
	lasso.fit(X_train, y_train)
	y_train_pred = lasso.predict(X_train)
	y_test_pred = lasso.predict(X_test)
	print(lasso.coef_)
	
	print('í›ˆë ¨ MSE: %.3f, í…ŒìŠ¤íŠ¸ MSE: %.3f' % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))
	print('í›ˆë ¨ R^2: %.3f, í…ŒìŠ¤íŠ¸ R^2: %.3f' % (r2_score(y_train, y_train_pred),r2_score(y_test, y_test_pred)))

<br>

# [5-3] ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ (Elastic Net Regression)
â–£ ê°€ì´ë“œ : https://scikit-learn.org/stable/modules/linear_model.html#elastic-net<br>
â–£ API : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html<br>
ë¦¿ì§€ì™€ ë¼ì˜ì˜ ê²°í•©ìœ¼ë¡œ L1ê·œì œë¡œ Feature ìˆ˜ë¥¼ ì¤„ì„ê³¼ ë™ì‹œì— L2ê·œì œë¡œ ê³„ìˆ˜ê°’ì˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•˜ì—¬ íšŒê·€ëª¨ë¸ì„ ìƒì„±<br>

	from sklearn.linear_model import ElasticNet
	elanet = ElasticNet(alpha=1.0, l1_ratio=0.5)

<br>

![](./images/L1L2_1.PNG)
<br>
ì¶œì²˜ : https://stanford.edu/~shervine/l/ko/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks

<br>

---

# [ì„ í˜•íšŒê·€ëª¨ë¸ê³¼ ê²½ì‚¬í•˜ê°•ë²• ë¹„êµ ì˜ˆì œ]


	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.linear_model import LinearRegression, SGDRegressor
	from sklearn.model_selection import train_test_split

	# ì˜ˆì œ ë°ì´í„° ìƒì„±
	np.random.seed(0)
	X = 2 * np.random.rand(100, 1)  # 0ì—ì„œ 2ê¹Œì§€ì˜ ëœë¤ ìˆ«ì 100ê°œ ìƒì„±
	y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ

	# í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ê¸°
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	# ê¸°ë³¸ LinearRegression ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨
	linear_reg = LinearRegression()
	linear_reg.fit(X_train, y_train)

	# SGDRegressor ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨
	sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3)
	sgd_reg.fit(X_train, y_train.ravel())  # y_trainì€ 1D ë°°ì—´ë¡œ ë³€í™˜

	# ëª¨ë¸ ì˜ˆì¸¡
	y_pred_linear = linear_reg.predict(X_test)
	y_pred_sgd = sgd_reg.predict(X_test)

	# ê²°ê³¼ ì‹œê°í™”
	plt.figure(figsize=(12, 6))

	# Linear Regression ê²°ê³¼
	plt.subplot(1, 2, 1)
	plt.scatter(X_test, y_test, color='blue', label='ì‹¤ì œê°’')
	plt.scatter(X_test, y_pred_linear, color='red', label='LinearRegression ì˜ˆì¸¡ê°’')
	plt.plot(X_test, y_pred_linear, color='red', linewidth=2)
	plt.title('ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸')
	plt.xlabel('X')
	plt.ylabel('y')
	plt.legend()

	# SGDRegressor ê²°ê³¼
	plt.subplot(1, 2, 2)
	plt.scatter(X_test, y_test, color='blue', label='ì‹¤ì œê°’')
	plt.scatter(X_test, y_pred_sgd, color='green', label='SGDRegressor ì˜ˆì¸¡ê°’')
	plt.plot(X_test, y_pred_sgd, color='green', linewidth=2)
	plt.title('SGDRegressor ëª¨ë¸')
	plt.xlabel('X')
	plt.ylabel('y')
	plt.legend()
	plt.tight_layout()
	plt.show()

	# íšŒê·€ ê³„ìˆ˜ ë° ì ˆí¸ ì¶œë ¥
	print("LinearRegression íšŒê·€ ê³„ìˆ˜:", linear_reg.coef_)
	print("LinearRegression ì ˆí¸:", linear_reg.intercept_)
	print("SGDRegressor íšŒê·€ ê³„ìˆ˜:", sgd_reg.coef_)
	print("SGDRegressor ì ˆí¸:", sgd_reg.intercept_)

	# RÂ² ì ìˆ˜ ì¶œë ¥
	score_linear = linear_reg.score(X_test, y_test)
	score_sgd = sgd_reg.score(X_test, y_test)

	print("LinearRegression RÂ² ì ìˆ˜:", score_linear)
	print("SGDRegressor RÂ² ì ìˆ˜:", score_sgd)

<br>

---
# [6] ë¹„ì„ í˜• íšŒê·€ (nonlinear regression)
ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë³€í˜•í•˜ë”ë¼ë„ íŒŒë¼ë¯¸í„°ë¥¼ ì„ í˜• ê²°í•©ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ë¡œ íšŒê·€ëª¨í˜•ì— ì£¼ì–´ì§„ íšŒê·€ì‹ì´ ëª¨ìˆ˜ë“¤ì˜ ë¹„ì„ í˜•í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš° ì„ í˜•íšŒê·€ì—ì„œ íšŒê·€ê³„ìˆ˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì˜ ë³€í™”ëŸ‰ì— ë”°ë¥¸ ë°˜ì‘ë³€ìˆ˜ì˜ í‰ê· ë³€í™”ëŸ‰ìœ¼ë¡œ í•´ì„ë˜ì§€ë§Œ, ë¹„ì„ í˜•íšŒê·€ì—ì„œëŠ” ê° ëª¨ìˆ˜ê°€ íŠ¹ì •í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.<br>
<!--
(1) ë‹¤í•­ íšŒê·€ (Polynomial Regression)
 $y = Î²_0 + Î²_1X + Î²_2X^2 +â‹¯+ Î²_nX^n + Ïµ$

(2) ì§€ìˆ˜ íšŒê·€ (Exponential Regression)
 $y = Î±e^{Î²X} + Ïµ$ 
 $ln(y) = ln(Î±) + Î²X + Ïµ$

(3) ë¡œê·¸ íšŒê·€ (Logarithmic Regression)
 $y = Î± + Î²ln(X) + Ïµ$
 $âˆ‚y/âˆ‚x = Î²/x$

(4) ë‹¤ì¤‘ íšŒê·€ (Multiple Regression)
 $y = Î± + Î²_1X_1 + Î²_2X_2^2 + Î²_3sin(X_3) + Ïµ$

(5) ì‹œê·¸ëª¨ì´ë“œ íšŒê·€ (Sigmoid Regression)
 $y = 1/(1+e^âˆ’{Î²X}) + Ïµ$
 $ln(y/(1âˆ’y)) = Î²X + Ïµ$

(6) ì „ë ¥ íšŒê·€ (Power Regression)
 $y = Î±x^Î² + Ïµ$
 $âˆ‚y/âˆ‚x = Î±â‹…Î²â‹…x^{Î²âˆ’1}$

(7) í¬ì•„ì†¡ íšŒê·€ (Poisson Regression)
 $ln(y) = Î± + Î²X + Ïµ$

(8) ê°ë§ˆ íšŒê·€ (Gamma Regression)
 $y = Î±X^Î² + Ïµ$
 $ln(y) = ln(Î±) + Î²ln(X) + Ïµ$

(9) ë² ì´ì¦ˆ íšŒê·€ (Bayesian Regression)
 $y = Î²_0 + Î²_1X_1 +â‹¯+ Î²_nX_n + Ïµ$

(10) ìŠ¤í”Œë¼ì¸ íšŒê·€ (Spline Regression)
 $y = Î²_iB_i(X)ì˜ í•© + Ïµ$

(11) ë¡œë²„ìŠ¤íŠ¸ íšŒê·€ (Robust Regression)
 $y = Î²_0 + Î²_1X_1 +â‹¯+ Î²_nX_n + Ïµ$

(12) ì»¤ë„ íšŒê·€ (Kernel Regression)
 $y = Î±_iK(X,X_i)ì˜ í•© + Ïµ$

(13) êµ¬í˜• íšŒê·€ (Quadratic Regression)
 $y = Î²_0 + Î²_1x + Î²_2x^2 + Ïµ$
 $âˆ‚y/âˆ‚x = Î²_1 + 2Î²_2x$
-->
<br>

| êµ¬ë¶„ | ìˆ˜ì‹ | ê³¡ì„  í˜•íƒœ ë° ì£¼ìš” ì ìš© ë¶„ì•¼ |
|----|------|------------------------------|
| [6-1] ë¹„ì„ í˜• ìµœì†Œì œê³± íšŒê·€ (*Nonlinear Least Squares Regression, NLS*) | ![eq](https://latex.codecogs.com/png.latex?%5Cmin_%7B%5Ctheta%7D%5Csum_%7Bi%3D1%7D%5En%28y_i%20-%20f%28x_i%3B%5Ctheta%29%29%5E2) | ëª¨ë“  ë¹„ì„ í˜• íšŒê·€ì˜ ê¸°ë³¸ í‹€ â€” ë¬¼ë¦¬Â·ê³µí•™Â·ê²½ì œëª¨í˜• íŒŒë¼ë¯¸í„° ì¶”ì • |
| [6-2] ì§€ìˆ˜ íšŒê·€ (*Exponential Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20a%20e%5E%7Bb%20x%7D) | ì§€ìˆ˜ ì„±ì¥/ê°ì‡ í˜• â€” ì„¸ê·  ì„±ì¥, ë°©ì‚¬ëŠ¥ ë¶•ê´´, ìˆ˜ìµë¥  ê°ì†Œ |
| [6-3] ë¡œê·¸í˜• íšŒê·€ (*Logarithmic Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20a%20%2B%20b%20%5Cln%28x%29) | ì™„ë§Œí•œ ì¦ê°€Â·ê°ì†Œí˜• (Concave/Convex) â€” í•™ìŠµê³¡ì„ , íš¨ìš©í•¨ìˆ˜ |
| [6-4] ì „ë ¥ íšŒê·€ (*Power Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20a%20x%5E%7Bb%7D) | ê±°ë“­ì œê³±í˜• (Scaling law) â€” ë¬¼ë¦¬ëŸ‰ ê´€ê³„, ìƒì‚°í•¨ìˆ˜, íƒ„ì„±ë¶„ì„ |
| [6-5] ì‹œê·¸ëª¨ì´ë“œ íšŒê·€ (*Sigmoid Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cfrac%7BL%7D%7B1%20%2B%20e%5E%7B-k%28x%20-%20x_0%29%7D%7D) | S-curve (ëŒ€ì¹­í˜•) â€” í™•ì‚°, í¬í™”, í•™ìŠµ ì§„ì „ ê³¡ì„  |
| [6-6] ìŠ¤í”Œë¼ì¸ íšŒê·€ (*Spline Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Csum_%7Bj%3D1%7D%5EK%20%5Cbeta_j%20B_j%28x%29) | Piecewise Smooth Curve â€” ë³µì¡í•œ ê³¡ì„  ê·¼ì‚¬, ê²½ì œÂ·ê¸°í•˜ ëª¨ë¸ |
| [6-7] ì»¤ë„ íšŒê·€ (*Kernel Regression*) | ![eq](https://latex.codecogs.com/png.latex?%5Chat%7By%7D%28x%29%20%3D%20%5Cfrac%7B%5Csum_i%20K%28x%20-%20x_i%29%20y_i%7D%7B%5Csum_i%20K%28x%20-%20x_i%29%7D) | ë¶€ë“œëŸ¬ìš´ ë¹„ëª¨ìˆ˜ ì¶”ì„¸ â€” ì‹œê³„ì—´ í‰í™œí™”, ë¹„ì„ í˜• ì˜ˆì¸¡ |
| [6-8] ë‹¤í•­ì‹ íšŒê·€ (*Polynomial Regression, High-order*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1%20x%20%2B%20%5Cbeta_2%20x%5E2%20%2B%20%5Ccdots%20%2B%20%5Cbeta_n%20x%5En) | ê³¡ë¥  ê°€ë³€í˜• â€” ë³µì¡í•œ ì¶”ì„¸ ì í•©, ê³¡ì„  íšŒê·€ |
| [6-9] ë¡œì§€ìŠ¤í‹± ì„±ì¥ íšŒê·€ (*Logistic Growth Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cfrac%7BK%7D%7B1%20%2B%20A%20e%5E%7B-B%20x%7D%7D) | S-curve (í¬í™” ì„±ì¥í˜•) â€” ì¸êµ¬Â·ì‹œì¥Â·ë°”ì´ëŸ¬ìŠ¤ í™•ì‚° ëª¨ë¸ |
| [6-10] ê³°í¼ì¸  íšŒê·€ (*Gompertz Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20a%20e%5E%7B-b%20e%5E%7B-c%20x%7D%7D) | ë¹„ëŒ€ì¹­ S-curve â€” ìƒë¬¼ ì„±ì¥, ì•½ë¬¼ ë°˜ì‘, ê°ì—¼ ì „íŒŒ ê³¡ì„  |
| [6-11] í•˜ì´í¼ë³¼ë¦­ íšŒê·€ (*Hyperbolic Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cfrac%7Ba%7D%7Bx%20%2B%20b%7D%20%2B%20c) | í¬í™”/ì—­ë¹„ë¡€í˜• â€” ë°˜ì‘ ì†ë„, ë†ë„-íš¨ê³¼ ê´€ê³„, ìˆ˜ìœ¨ ë¶„ì„ |
| [6-12] ê°€ìš°ì‹œì•ˆ íšŒê·€ (*Gaussian Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20a%20%5Cexp%5Cleft%28-%5Cfrac%7B%28x-b%29%5E2%7D%7B2%20c%5E2%7D%5Cright%29) | Bell-shape (ëŒ€ì¹­í˜•) â€” ë¶„í¬í˜• ë°˜ì‘, ìµœì ì  íƒìƒ‰, ì•½ë¬¼ ë†ë„ ë°˜ì‘ |
| [6-13] ë³¼ì¸ ë§Œ ì‹œê·¸ëª¨ì´ë“œ íšŒê·€ (*Boltzmann Sigmoidal Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cfrac%7BA_1%20-%20A_2%7D%7B1%20%2B%20e%5E%7B%28x%20-%20x_0%29%2Fd%7D%7D%20%2B%20A_2) | S-curve (ë‹¨ê³„ì  í¬í™”) â€” ë¬¼ì§ˆ ì „ì´, ì˜¨ë„ ë°˜ì‘, ì „ê¸°ì‹ í˜¸ ë³€í™” |
| [6-14] ë˜ì…”ë„ í•¨ìˆ˜ íšŒê·€ (*Rational Function Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cfrac%7Ba_0%20%2B%20a_1%20x%20%2B%20%5Cdots%20%2B%20a_m%20x%5Em%7D%7B1%20%2B%20b_1%20x%20%2B%20%5Cdots%20%2B%20b_n%20x%5En%7D) | ë³µí•© ë¹„ì„ í˜• ê³¡ì„ í˜• â€” ì‹¤í—˜ ë°ì´í„° ê·¼ì‚¬, ì œì–´ëª¨ë¸ |
| [6-15] êµ¬ê°„ë³„ íšŒê·€ (*Piecewise / Segmented Regression*) | ![eq](https://latex.codecogs.com/png.latex?y%20%3D%20%5Cbegin%7Bcases%7D%20a_1%20%2B%20b_1%20x%2C%20%26%20x%20%3C%20c%20%5C%5C%20a_2%20%2B%20b_2%20x%2C%20%26%20x%20%5Cge%20c%20%5Cend%7Bcases%7D) | Break-pointí˜• â€” êµ¬ì¡°ì  ë³€í™” íƒì§€, ì •ì±…íš¨ê³¼ ë¶„ì„ |
| [6-16] ë² ì´ì¦ˆ ë¹„ì„ í˜• íšŒê·€ (*Bayesian Nonlinear Regression*) | ![eq](https://latex.codecogs.com/png.latex?p%28%5Ctheta%20%5Cmid%20D%29%20%5Cpropto%20p%28D%20%5Cmid%20%5Ctheta%29%5C%2C%20p%28%5Ctheta%29) | ë¶ˆí™•ì‹¤ì„± ë°˜ì˜í˜• â€” ì†Œí‘œë³¸ ë°ì´í„°, í™•ë¥ ì  ì˜ˆì¸¡ ëª¨ë¸ |
| [6-17] ì‹ ê²½ë§ íšŒê·€ (*Neural Network Regression, MLP*) | ![eq](https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20%3D%20f%28W_2%20%5C%2C%20%5Csigma%28W_1%20x%20%2B%20b_1%29%20%2B%20b_2%29) | Universal Approximation â€” ë³µì¡í•œ ë¹„ì„ í˜• í•¨ìˆ˜ í•™ìŠµ, ì˜ˆì¸¡Â·ì œì–´ |


---

<!--

| ì•Œê³ ë¦¬ì¦˜                                      | ì£¼ëœ í•™ìŠµ ëª©ì                                          | í•µì‹¬ ì•„ì´ë””ì–´                                                                              | ë¹„ê³                                                    |
| ----------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------------ | ---------------------------------------------------- |
| **[9-1] PLS (Partial Least Squares)**           | ë…ë¦½ë³€ìˆ˜ (X)ì™€ ì¢…ì†ë³€ìˆ˜ (Y) ê°„ ê³µë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ì—¬ ì˜ˆì¸¡ë ¥ ë†’ì€ ì ì¬ìš”ì¸ì„ ì¶”ì¶œ  | (X, Y)ì˜ ê³µí†µëœ ì ì¬ìš”ì¸(latent variable)ì„ ì°¾ì•„ íšŒê·€ê³„ìˆ˜ë¥¼ ì¶”ì •                                       | (Y)ê°€ ì—°ì†í˜•ì¼ ë•Œ ì£¼ë¡œ ì‚¬ìš©í•˜ë©°, ë³€í˜•í˜• **PLS-DA**ëŠ” ë²”ì£¼í˜• (Y)ì—ë„ ì ìš© ê°€ëŠ¥ |
| **[9-2] LDA (Linear Discriminant Analysis)**    | í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ê³  í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ì—¬ íŒë³„ë ¥ì´ ë†’ì€ íˆ¬ì˜ ì¶•ì„ íƒìƒ‰ | Fisherì˜ íŒë³„ê¸°ì¤€ $\max_w \frac{w^T S_B w}{w^T S_W w}$ì„ ì‚¬ìš©í•˜ì—¬ ì„ í˜• íŒë³„ ê²½ê³„ í˜•ì„±                  | ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì°¨ì›ì¶•ì†Œì— ì‚¬ìš©, í´ë˜ìŠ¤ ê²½ê³„ê°€ ëª…í™•í•  ë•Œ ìš°ìˆ˜                 |
| **[9-3] NCA (Neighborhood Component Analysis)** | ìµœê·¼ì ‘ì´ì›ƒ(kNN) ë¶„ë¥˜ ì •í™•ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì„ë² ë”© ê³µê°„ì„ í•™ìŠµ              | ê°™ì€ í´ë˜ìŠ¤ ìƒ˜í”Œ ê°„ ê±°ë¦¬ë¥¼ ì¤„ì´ê³ , ë‹¤ë¥¸ í´ë˜ìŠ¤ ê°„ ê±°ë¦¬ë¥¼ ëŠ˜ë¦¬ëŠ” í™•ë¥ ì  ê±°ë¦¬í•™ìŠµ                                       | ë¹„ì„ í˜• í™•ì¥í˜•(NNCA, MNCA) ì¡´ì¬, ì£¼ë¡œ ë¶„ë¥˜ìš©ìœ¼ë¡œ ì‚¬ìš©                  |
| **[9-4] CCA (Canonical Correlation Analysis)**  | ë‘ ë°ì´í„°ì…‹(ë˜ëŠ” feature set) ê°„ ìƒê´€ê´€ê³„ë¥¼ ìµœëŒ€í™”               | (X, Y) ê°ê°ì— ëŒ€í•œ ì„ í˜• ê²°í•© (w, v)ë¥¼ ì°¾ì•„ $w^T$ Xì™€ $v^T$ Yì˜ ìƒê´€ì„ ê·¹ëŒ€í™”                             | ë‹¤ì¤‘ëª¨ë‹¬(ì´ë¯¸ì§€â†”í…ìŠ¤íŠ¸ ë“±) í‘œí˜„í•™ìŠµì— ì í•©, íšŒê·€Â·ë¶„ë¥˜ ëª¨ë‘ ì‘ìš© ê°€ëŠ¥             |
| **[9-5] Supervised PCA** | ë¼ë²¨ê³¼ ê´€ë ¨ëœ featureì˜ ë¶„ì‚°ì„ ìš°ì„  ë³´ì¡´ | ë¼ë²¨ ì •ë³´ ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œ í›„ PCA ìˆ˜í–‰ $(\tilde{S} = \mathrm{diag}(s(y)) S \mathrm{diag}(s(y)))$ | ì¼ë°˜ PCAë³´ë‹¤ ì˜ˆì¸¡ë³€ìˆ˜ì™€ ëª©í‘œë³€ìˆ˜ì˜ ì—°ê´€ì„± ë°˜ì˜, íšŒê·€Â·ë¶„ë¥˜ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥          |


# [9-1] ë¶€ë¶„ ìµœì†Œì œê³± (Partial Least Squares, PLS)
ì„¤ëª…ë³€ìˆ˜ X ì™€ ëª©í‘œë³€ìˆ˜ Y ë¥¼ ë™ì‹œì— ì˜ ì„¤ëª…í•˜ëŠ” ì ì¬ìš”ì¸(latent components)ì„ ì¶”ì¶œí•˜ê³ , ê·¸ ìš”ì¸ìœ¼ë¡œ íšŒê·€í•˜ëŠ” ë°©ì‹<br>
(= PCAì²˜ëŸ¼ Xì˜ ë¶„ì‚°ë§Œ ë³´ì§€ ì•Šê³ , Yì™€ì˜ ê³µë¶„ì‚°ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì°¨ì›ì„ ì••ì¶•)<br>
PCA : Xì˜ êµ¬ì¡°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì¶•ì„ ì°¾ëŠ”ë‹¤. (ë¹„ì§€ë„)<br>
PLS : Xê°€ Yë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì¶•ì„ ì°¾ëŠ”ë‹¤. (ì§€ë„)<br>


| êµ¬ë¶„                                    | **PCA (ì£¼ì„±ë¶„ë¶„ì„)**                               | **PLS (ë¶€ë¶„ìµœì†Œì œê³±)**                                        | ë¹„ê³                         |
| ------------------------------------- | --------------------------------------------- | ------------------------------------------------------- | ------------------------- |
| **â‘  ëª©ì **       | Xì˜ ë¶„ì‚°(variance)ì„ ìµœëŒ€í™”í•˜ëŠ”<br>ì¶•(ì£¼ì„±ë¶„)ì„ ì°¾ìŒ   | Xì™€ Yì˜ ê³µë¶„ì‚°(covariance)ì„ ìµœëŒ€í™”í•˜ëŠ”<br>ì¶•(ì ì¬ìš”ì¸)ì„ ì°¾ìŒ  | PCAëŠ” X êµ¬ì¡°,<br> PLSëŠ” Xâ†’Y ì˜ˆì¸¡ ì¤‘ì‹¬ |
| **â‘¡ ì‚¬ìš© ë°ì´í„°(Input)**     | ë…ë¦½ë³€ìˆ˜ Xë§Œ ì‚¬ìš©             | ë…ë¦½ë³€ìˆ˜ Xì™€ ì¢…ì†ë³€ìˆ˜ Y ëª¨ë‘ ì‚¬ìš©                                    | PLSëŠ” ì§€ë„í˜• ì°¨ì›ì¶•ì†Œ |
| **â‘¢ ì¶œë ¥(Output)**       | ì£¼ì„±ë¶„ ì ìˆ˜(PC scores), ë¡œë”©(loading)                | ì ì¬ìš”ì¸(scores, loadings, weights), íšŒê·€ê³„ìˆ˜                   | PLSëŠ” íšŒê·€ëª¨ë¸ê¹Œì§€ í¬í•¨            |
| **â‘£ ìˆ˜í•™ì  ê¸°ì¤€** | $\max_{\mathbf{w}} \text{Var}(Xw)$      | $\max_{\mathbf{w,q}} \text{Cov}(Xw, Yq)$       | ë¶„ì‚° vs ê³µë¶„ì‚° ê·¹ëŒ€í™”             |
| **â‘¤ ì°¨ì›ì¶•ì†Œ ë°©ì‹**     | Xì˜ ê³µë¶„ì‚° í–‰ë ¬ ê³ ìœ ë¶„í•´                                | X, Yì˜ êµì°¨ê³µë¶„ì‚° êµ¬ì¡°ë¶„í•´                                        | PLSëŠ” ì˜ˆì¸¡ë ¥ ì¤‘ì‹¬ ì¶• ì„ íƒ          |
| **â‘¥ ê°€ì • ë° ëª©ì  í•¨ìˆ˜ í•´ì„**                   | X ë‚´ë¶€ êµ¬ì¡°ë¥¼ ìš”ì•½(ë°ì´í„° ì••ì¶•)          | Xê°€ Yë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ ë°˜ì˜(ì˜ˆì¸¡ ì„±ëŠ¥â†‘)           | PLSëŠ” Yê°€ ìˆê¸°ì— ì§€ë„í˜• íšŒê·€ì™€ ì—°ê³„    |
| **â‘¦ ê²°ê³¼ í•´ì„**        | ê° ì£¼ì„±ë¶„ì´ Xì˜ ì£¼ìš” ë³€ë™ë°©í–¥ì„ ì„¤ëª…                      | ê° ì ì¬ìš”ì¸ì´ Y ì˜ˆì¸¡ì— ê¸°ì—¬í•œ ë°©í–¥ì„ ì„¤ëª…                                | PLSëŠ” VIP(ë³€ìˆ˜ ì¤‘ìš”ë„) ê³„ì‚° ê°€ëŠ¥    |
| **â‘§ ì‚¬ìš© ì‚¬ë¡€**      | íƒìƒ‰ì  ë°ì´í„° ë¶„ì„, ì‹œê°í™”, ì´ìƒì¹˜ íƒì§€, ë…¸ì´ì¦ˆ ì œê±°               | ê³ ì°¨ì› ë°ì´í„° ì˜ˆì¸¡(í™”í•™ê³„ëŸ‰, ìŠ¤í™íŠ¸ëŸ¼, ìœ ì „ì ë“±)   | PCAëŠ” êµ¬ì¡° íƒìƒ‰,<br> PLSëŠ” ì˜ˆì¸¡/íšŒê·€    |
| **â‘¨ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„**                         | ë¹„ì§€ë„í•™ìŠµ(Unsupervised)                    | ì§€ë„í•™ìŠµ(Supervised)                                       | PLSëŠ” íšŒê·€ê³„ì—´ë¡œ ë¶„ë¥˜ë¨            |
| **â‘© ì¥ë‹¨ì  ìš”ì•½**   | ì¥ì : ë‹¨ìˆœ, ë¹ ë¦„, X êµ¬ì¡° í•´ì„ìš©ì´<br>ë‹¨ì : Yì™€ ë¬´ê´€í•œ ë°©í–¥ í¬í•¨ ê°€ëŠ¥ | ì¥ì : Y ì˜ˆì¸¡ì— íŠ¹í™”, ê³µì„ ì„± í•´ê²°, VIP í•´ì„ê°€ëŠ¥<br>ë‹¨ì : ì»´í¬ë„ŒíŠ¸ ìˆ˜ ì„ íƒ, í•´ì„ ë³µì¡ | PLS âŠƒ PCA<br>(ì˜ˆì¸¡ì§€í–¥ì  í™•ì¥í˜•)     |

-->
